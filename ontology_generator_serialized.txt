Please review this ontology specification and the generating code and provide recommended fixes as tickets
that ensure events are linked to equipment and equipment to lines and sequences properly and that events are linkedt
to other data like shift, material, reasons, etc.

The primary metrics we will use for analysis are the minutes recorded for the different times in the asset effectiveness model:

raw data columns:

AE_MODEL_CATEGORY
ALL_MAINTENANCE
AUTONOMOUS_MAINTENANCE
BUSINESS_EXTERNAL_TIME
DOWNTIME
EFFECTIVE_RUNTIME
GOOD_PRODUCTION_QTY
NOT_ENTERED
PLANNED_MAINTENANCE
PLANT_AVAILABLE_TIME
PLANT_DECISION_TIME
PLANT_EXPERIMENTATION
PRODUCTION_AVAILABLE_TIME
REJECT_PRODUCTION_QTY
TOTAL_TIME
RUN_TIME
WAITING_TIME

===========================================
Ontology Spec (csv)
===========================================

Logical Group,Raw Data Column Name,Proposed OWL Entity,Proposed OWL Property,OWL Property Type,Target/Range (xsd:) / Target Class,OWL Property Characteristics,Inverse Property,Domain,Property Restrictions,ISA-95 Concept,Parent Class,Target Link Context,Notes/Considerations
Performance Metrics,AE_MODEL_CATEGORY,EventRecord,aeModelCategory,DatatypeProperty,xsd:string,-,,EventRecord,,OperationsPerformance Category,EventRecord,"Captures the specific AE/OEE time category assignment for the event (e.g., Runtime, Unplanned, Plant Decision).",
Performance Metrics,ALL_MAINTENANCE,EventRecord,allMaintenanceTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Total duration classified as Maintenance (in minutes). (Assumed Functional per event).,
Performance Metrics,AUTONOMOUS_MAINTENANCE,EventRecord,autonomousMaintenanceTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Autonomous Maintenance (in minutes). Sub-category of All Maintenance. (Assumed Functional).,
Performance Metrics,BUSINESS_EXTERNAL_TIME,EventRecord,businessExternalTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Property of EventRecord per Section 6.3. Represents Business External time in minutes. (Functional per EventRecord instance),
Utilization State/Reason,CO_ORIGINAL_TYPE,OperationalReason,originalChangeoverType,DatatypeProperty,xsd:string,-,,OperationalReason,,OperationsEvent Detail (Original),OperationalReason,Represents the original changeover type code. Allows comparison with changeoverType if needed.,
Utilization State/Reason,CO_TYPE,OperationalReason,changeoverType,DatatypeProperty,xsd:string,-,,OperationalReason,,OperationsEvent Detail,OperationalReason,Property of OperationalReason for changeover events. Categorical.,
Equipment Attributes,COMPLEXITY,Equipment,complexity,DatatypeProperty,xsd:string,-,,Equipment,,Equipment Property,Equipment,Property of Equipment according to Section 5.5.,
Time & Schedule,CREW_ID,PersonnelClass,personnelClassId,DatatypeProperty,xsd:string,Functional,,PersonnelClass,,PersonnelClass ID,owl:Thing,Added to align with Section 5.4 Personnel model.,
Performance Metrics,DOWNTIME,EventRecord,downtimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Downtime (in minutes). (Assumed Functional per event).,
Utilization State/Reason,DOWNTIME_DRIVER,OperationalReason,downtimeDriver,DatatypeProperty,xsd:string,-,,OperationalReason,,OperationsEvent Category,OperationalReason,Property of OperationalReason per Section 5.11.,
Performance Metrics,EFFECTIVE_RUNTIME,EventRecord,effectiveRuntimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Property of EventRecord per Section 6.3. Represents Effective Runtime in minutes. (Functional per EventRecord instance),
Asset Hierarchy,EQUIPMENT_ID,Equipment,equipmentId,DatatypeProperty,xsd:string,Functional,,Equipment,,Equipment ID,ProductionLine,Preferred ID for Equipment Individual. Links via isPartOfProductionLine to ProductionLine. Requires check against EQUIPMENT_TYPE.,
Equipment Attributes,EQUIPMENT_MODEL,Equipment,equipmentModel,DatatypeProperty,xsd:string,Functional,,Equipment,,Equipment Property/Model,Equipment,,Property of Equipment according to Section 5.5.
Asset Hierarchy,EQUIPMENT_NAME,Equipment,equipmentName,DatatypeProperty,xsd:string,-,,Equipment,,Equipment Description,ProductionLine,Secondary ID. Used to determine EquipmentClass. Consider rdfs:label.,
Equipment Class,EQUIPMENT_NAME,EquipmentClass,equipmentClassId,DatatypeProperty,xsd:string,Functional,,EquipmentClass,,EquipmentClass ID,owl:Thing,,"Requires parsing from EQUIPMENT_NAME to extract the class name (e.g., using logic similar to parse_equipment_class)."
Asset Hierarchy,EQUIPMENT_TYPE,EventRecord,involvesResource,ObjectProperty,ProductionLineOrEquipment,,resourceInvolvedIn,EventRecord,min 1,SegmentResponse Resource,EventRecord,,Links Event to the specific Resource (Line OR Equip) it pertains to. Range is the union class ProductionLineOrEquipment.
Asset Hierarchy,GH_AREA,ProcessCell,processCellId,DatatypeProperty,xsd:string,Functional,,ProcessCell,,Area/ProcessCell ID,Area,Used to create/identify ProcessCell (GH_Area) Individual. Links via partOfArea to Area. Source corrected from PHYSICAL_AREA.,
Asset Hierarchy,GH_CATEGORY,Area,areaCategoryCode,DatatypeProperty,xsd:string,Functional,,Area,,Product Category,Area,"Represents the product category code (e.g., ""OC"") directly associated with the Focus Factory (Area). Marked Functional.",
Asset Hierarchy,GH_FOCUSFACTORY,Area,areaId,DatatypeProperty,xsd:string,Functional,,Area,,Area ID,Plant,Used to create/identify Area (Focus Factory) Individual. Links via locatedInPlant to Plant.,
Performance Metrics,GOOD_PRODUCTION_QTY,EventRecord,goodProductionQuantity,DatatypeProperty,xsd:integer,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Quantity of good units produced during the event. (Assumed Functional per event).,
Time & Schedule,JOB_END_TIME_LOC,TimeInterval,endTime,DatatypeProperty,xsd:dateTime,Functional,,TimeInterval,,SegmentResponse EndTime,TimeInterval,Part of the TimeInterval linked by EventRecord. (Functional per TimeInterval instance),
Time & Schedule,JOB_START_TIME_LOC,TimeInterval,startTime,DatatypeProperty,xsd:dateTime,Functional,,TimeInterval,,SegmentResponse StartTime,owl:Thing,EventRecord links via occursDuring to TimeInterval. (Functional per TimeInterval instance),
Asset Hierarchy,LINE_NAME,ProductionLine,lineId,DatatypeProperty,xsd:string,Functional,,ProductionLine,,ProductionLine/ProcessCell ID,ProcessCell,Links via locatedInProcessCell to ProcessCell.,
Material & Prod Order,MATERIAL_ID,Material,materialId,DatatypeProperty,xsd:string,Functional,,Material,,MaterialDefinition ID,owl:Thing,,Used to create/identify Material Individual per Section 5.7.
Material & Prod Order,MATERIAL_UOM,Material,materialUOM,DatatypeProperty,xsd:string,Functional,,Material,,MaterialDefinition BaseUnitOfMeasure,Material,Property of Material per Section 5.7. (Functional per Material instance),
Equipment Attributes,MODEL,Equipment,alternativeModel,DatatypeProperty,xsd:string,-,,Equipment,,Equipment Property,Equipment,Property of Equipment according to Section 5.5.,
Asset Hierarchy,N/A,Area,locatedInPlant,ObjectProperty,Plant,-,hasArea,Area,,Hierarchy Scope,owl:Thing,Plant,Links an Area to the Plant it resides in.
Asset Hierarchy,N/A,Area,hasProcessCell,ObjectProperty,ProcessCell,-,partOfArea,Area,,Hierarchy Scope,owl:Thing,ProcessCell,Inverse of partOfArea. Links Area to its ProcessCells.
Asset Hierarchy,N/A,Equipment,isPartOfProductionLine,ObjectProperty,ProductionLine,-,hasEquipmentPart,Equipment,,Hierarchy Scope,owl:Thing,ProductionLine,Links an Equipment instance to the ProductionLine it is part of.
Equipment Class,N/A,Equipment,memberOfClass,ObjectProperty,EquipmentClass,Functional,hasInstance,Equipment,,EquipmentClass Hierarchy,Equipment,EquipmentClass,Links Equipment instances to their EquipmentClass. Functional assumes equip belongs to one class.
Equipment Sequence,N/A,Equipment,equipmentIsUpstreamOf,ObjectProperty,Equipment,"Transitive, Asymmetric",equipmentIsDownstreamOf,Equipment,,Equipment Hierarchy Instance,Equipment,Equipment,Deferred/Future: Instance sequence data not available/required for V1.
Equipment Sequence,N/A,Equipment,equipmentIsDownstreamOf,ObjectProperty,Equipment,"Transitive, Asymmetric",equipmentIsUpstreamOf,Equipment,,Equipment Hierarchy Instance,Equipment,Equipment,Deferred/Future: Instance sequence data not available/required for V1.
Equipment Sequence,N/A,Equipment,isParallelWith,ObjectProperty,Equipment,"Symmetric, Irreflexive",,Equipment,,Equipment Hierarchy Instance,Equipment,Equipment,Deferred/Future: Not required for V1.
Equipment Sequence,N/A,Equipment,actualSequencePosition,DatatypeProperty,xsd:integer,Functional,,Equipment,,Equipment Hierarchy Instance,Equipment,,Deferred/Future: Instance sequence data not available/required for V1.
Equipment Class,N/A,EquipmentClass,hasInstance,ObjectProperty,Equipment,-,memberOfClass,EquipmentClass,,EquipmentClass Hierarchy,owl:Thing,Equipment,Inverse of memberOfClass. Links EquipmentClass to Equipment instances.
Equipment Sequence,N/A,EquipmentClass,classIsUpstreamOf,ObjectProperty,EquipmentClass,"Transitive, Asymmetric",classIsDownstreamOf,EquipmentClass,,Equipment Hierarchy/Topology,EquipmentClass,EquipmentClass,Defines upstream relationship between equipment classes. Use for default sequence relative order.
Equipment Sequence,N/A,EquipmentClass,classIsDownstreamOf,ObjectProperty,EquipmentClass,"Transitive, Asymmetric",classIsUpstreamOf,EquipmentClass,,Equipment Hierarchy/Topology,EquipmentClass,EquipmentClass,Defines downstream relationship between equipment classes.
Equipment Sequence,N/A,EquipmentClass,isParallelWith,ObjectProperty,EquipmentClass,"Symmetric, Irreflexive",,EquipmentClass,,Equipment Hierarchy/Topology,EquipmentClass,EquipmentClass,Defines equipment classes that operate in parallel. Deferred/Future: Not required for V1 linear sequence focus.
Equipment Sequence,N/A,EquipmentClass,defaultSequencePosition,DatatypeProperty,xsd:integer,Functional,,EquipmentClass,,Equipment Hierarchy/Topology,EquipmentClass,,Required for V1: Numerical position in default sequence (1-based). Functional per class in sequence context.
Operational Context,N/A,EventRecord,isPartOfLineEvent,ObjectProperty,EventRecord,-,hasDetailedEquipmentEvent,EventRecord,,Event Hierarchy,EventRecord,EventRecord,Required V1: Links an equipment-specific event record to the broader line event record it occurred within or contributed to. Domain: EventRecord involving Equipment. Range: EventRecord involving ProductionLine.
Operational Context,N/A,EventRecord,hasDetailedEquipmentEvent,ObjectProperty,EventRecord,-,isPartOfLineEvent,EventRecord,,Event Hierarchy,EventRecord,EventRecord,Required V1: Inverse of isPartOfLineEvent. Links a line-level event to the specific equipment events that comprise or explain it. Domain: EventRecord involving ProductionLine. Range: EventRecord involving Equipment.
Time & Schedule,N/A,EventRecord,occursDuring,ObjectProperty,TimeInterval,Functional,,EventRecord,,SegmentResponse TimeInterval,EventRecord,TimeInterval,Links EventRecord to TimeInterval. This property is essential for data population. Functional assumes event fully within one interval.
Time & Schedule,N/A,PersonnelClass,hasPersonMember,ObjectProperty,Person,-,memberOfPersonnelClass,PersonnelClass,,PersonnelClass Link,owl:Thing,Person,Inverse of memberOfPersonnelClass. Links PersonnelClass to Person members.
Asset Hierarchy,N/A,Plant,hasArea,ObjectProperty,Area,-,locatedInPlant,Plant,,Hierarchy Scope,owl:Thing,Area,Inverse of locatedInPlant. Links Plant to its Areas.
Asset Hierarchy,N/A,ProcessCell,partOfArea,ObjectProperty,Area,-,hasProcessCell,ProcessCell,,Hierarchy Scope,owl:Thing,Area,Links a ProcessCell (GH_Area) to the Area (Focus Factory) it is part of.
Asset Hierarchy,N/A,ProcessCell,containsProductionLine,ObjectProperty,ProductionLine,-,locatedInProcessCell,ProcessCell,,Hierarchy Scope,owl:Thing,ProductionLine,Inverse of locatedInProcessCell. Links ProcessCell to its ProductionLines.
Asset Hierarchy,N/A,ProductionLine,locatedInProcessCell,ObjectProperty,ProcessCell,-,containsProductionLine,ProductionLine,,Hierarchy Scope,owl:Thing,ProcessCell,Links a ProductionLine to the ProcessCell it resides in.
Asset Hierarchy,N/A,ProductionLine,hasEquipmentPart,ObjectProperty,Equipment,-,isPartOfProductionLine,ProductionLine,,Hierarchy Scope,owl:Thing,Equipment,Inverse of isPartOfProductionLine. Links ProductionLine to its Equipment.
Asset Hierarchy,N/A,ProductionLineOrEquipment,,,,,,,,Resource,owl:Thing,Represents a resource that can be either a ProductionLine or an Equipment. Intended as a superclass or union for the range of involvesResource.,
Asset Hierarchy,N/A,,resourceInvolvedIn,ObjectProperty,EventRecord,-,involvesResource,ProductionLineOrEquipment,,,Asset Hierarchy,EventRecord,Inverse of involvesResource. Links a line or equipment back to events it was involved in. Domain corrected.
Performance Metrics,NOT_ENTERED,EventRecord,notEnteredTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration where state was not entered/unknown (in minutes). (Assumed Functional per event).,
Utilization State/Reason,OPERA_TYPE,EventRecord,operationType,DatatypeProperty,xsd:string,-,,EventRecord,,OperationsRecord Type,EventRecord,Property of EventRecord per Section 5.10. Categorical.,
Asset Hierarchy,PHYSICAL_AREA,Area,areaPhysicalCategoryName,DatatypeProperty,xsd:string,-,,Area,,Product Category,Area,,"Represents the descriptive product category name (e.g., ""OralCare"") directly associated with the Focus Factory (Area)."
Performance Metrics,PLANNED_MAINTENANCE,EventRecord,plannedMaintenanceTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Planned Maintenance (in minutes). Sub-category of All Maintenance. (Assumed Functional).,
Asset Hierarchy,PLANT,Plant,plantId,DatatypeProperty,xsd:string,Functional,,Plant,,Enterprise/Site ID,owl:Thing,Used to create/identify Plant Individual - harmonized with B2MML terminology.,
Performance Metrics,PLANT_AVAILABLE_TIME,EventRecord,plantAvailableTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Property of EventRecord per Section 6.3. Represents Plant Available time in minutes. (Functional per EventRecord instance),
Performance Metrics,PLANT_DECISION_TIME,EventRecord,plantDecisionTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Property of EventRecord per Section 6.3. Represents Plant Decision time in minutes. (Functional per EventRecord instance),
Performance Metrics,PLANT_EXPERIMENTATION,EventRecord,plantExperimentationTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Plant Experimentation (in minutes). (Assumed Functional per event).,
Material & Prod Order,PRIMARY_CONV_FACTOR,Material,conversionFactor,DatatypeProperty,xsd:double,Functional,,Material,,Material Property,Material,Property of Material per Section 5.7. (Functional per Material instance context),
Performance Metrics,PRODUCTION_AVAILABLE_TIME,EventRecord,productionAvailableTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Property of EventRecord per Section 6.3. Represents Production Available time in minutes. (Functional per EventRecord instance),
Material & Prod Order,PRODUCTION_ORDER_DESC,ProductionRequest,requestDescription,DatatypeProperty,xsd:string,-,,ProductionRequest,,OperationsRequest Desc,ProductionRequest,Property of ProductionRequest per Section 6.1.,
Material & Prod Order,PRODUCTION_ORDER_ID,ProductionRequest,requestId,DatatypeProperty,xsd:string,Functional,,ProductionRequest,,OperationsRequest ID,owl:Thing,Used to create/identify ProductionRequest per Section 6.1.,
Material & Prod Order,PRODUCTION_ORDER_RATE,ProductionRequest,requestRate,DatatypeProperty,xsd:double,Functional,,ProductionRequest,,OperationsRequest Prop,ProductionRequest,Property of ProductionRequest per Section 6.1. (Functional per Request instance),
Material & Prod Order,PRODUCTION_ORDER_UOM,ProductionRequest,requestRateUOM,DatatypeProperty,xsd:string,Functional,,ProductionRequest,,OperationsRequest Prop,ProductionRequest,,Property of ProductionRequest per Section 6.1. (Functional per Request instance)
Time & Schedule,RAMPUP_FLAG,EventRecord,rampUpFlag,DatatypeProperty,xsd:boolean,Functional,,EventRecord,,OperationsResponse Property,EventRecord,,Property of EventRecord per Section 6.3. (Functional per EventRecord instance)
Performance Metrics,REJECT_PRODUCTION_QTY,EventRecord,rejectProductionQuantity,DatatypeProperty,xsd:integer,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Quantity of reject units produced during the event. (Assumed Functional per event).,
Performance Metrics,RUN_TIME,EventRecord,runTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Runtime (in minutes). (Assumed Functional per event).,
Time & Schedule,SHIFT_DURATION_MIN,Shift,shiftDurationMinutes,DatatypeProperty,xsd:double,Functional,,Shift,,PersonnelSchedule Duration,Shift,Property of Shift per Personnel model. (Functional per Shift instance),
Time & Schedule,SHIFT_END_DATE_LOC,Shift,shiftEndTime,DatatypeProperty,xsd:dateTime,Functional,,Shift,,PersonnelSchedule EndTime,Shift,Property of Shift per Personnel model. (Functional per Shift instance),
Time & Schedule,SHIFT_NAME,Shift,shiftId,DatatypeProperty,xsd:string,Functional,,Shift,,PersonnelSchedule ID,owl:Thing,EventRecord links via duringShift to Shift.,
Time & Schedule,SHIFT_START_DATE_LOC,Shift,shiftStartTime,DatatypeProperty,xsd:dateTime,Functional,,Shift,,PersonnelSchedule StartTime,Shift,Property of Shift per Personnel model. (Functional per Shift instance),
Material & Prod Order,SHORT_MATERIAL_ID,Material,materialDescription,DatatypeProperty,xsd:string,-,,Material,,MaterialDefinition Description,Material,Property of Material per Section 5.7. Consider rdfs:comment.,
Material & Prod Order,SIZE_TYPE,Material,sizeType,DatatypeProperty,xsd:string,-,,Material,,Material Property,Material,Property of Material per Section 5.7.,
Performance Metrics,TOTAL_TIME,EventRecord,reportedDurationMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Duration,EventRecord,,Property of EventRecord per Section 6.3. Represents total event time in minutes. (Functional per EventRecord instance)
Material & Prod Order,TP_UOM,Material,targetProductUOM,DatatypeProperty,xsd:string,Functional,,Material,,Material Property (UoM),Material,Property of Material per Section 5.7. (Functional per Material instance),
Material & Prod Order,UOM_ST,Material,standardUOM,DatatypeProperty,xsd:string,Functional,,Material,,Material Property (UoM),Material,Property of Material per Section 5.7. Maps from UOM_ST. (Functional per Material instance),
Material & Prod Order,UOM_ST_SAP,Material,standardUOM,DatatypeProperty,xsd:string,Functional,,Material,,Material Property (UoM),Material,Property of Material per Section 5.7. Maps from UOM_ST_SAP. (Functional per Material instance),
Utilization State/Reason,UTIL_ALT_LANGUAGE_REASON,OperationalReason,altReasonDescription,DatatypeProperty,xsd:string (with lang tag),-,,OperationalReason,,OperationsEvent Description,OperationalReason,Property of OperationalReason per Section 5.11.,
Utilization State/Reason,UTIL_REASON_DESCRIPTION,OperationalReason,reasonDescription,DatatypeProperty,xsd:string,-,,OperationalReason,,OperationsEvent Reason,owl:Thing,Maps to Ops Event Information (Section 5.11). EventRecord links via eventHasReason.,
Utilization State/Reason,UTIL_STATE_DESCRIPTION,OperationalState,stateDescription,DatatypeProperty,xsd:string,-,,OperationalState,,OperationsRecord State,owl:Thing,,Maps to Ops Record Information (Section 5.10). EventRecord links via eventHasState.
Performance Metrics,WAITING_TIME,EventRecord,waitingTimeMinutes,DatatypeProperty,xsd:double,Functional,,EventRecord,,OperationsPerformance Parameter,EventRecord,Duration classified as Waiting (in minutes). (Assumed Functional per event).,

===========================================
Raw data sample (csv)
===========================================

LINE_NAME,EQUIPMENT_NAME,EQUIPMENT_ID,PLANT,DOWNTIME_DRIVER,OPERA_TYPE,GH_AREA,GH_CATEGORY,GH_FOCUSFACTORY,PHYSICAL_AREA,EQUIPMENT_TYPE,EQUIPMENT_MODEL,COMPLEXITY,MODEL,MATERIAL_ID,SHORT_MATERIAL_ID,SIZE_TYPE,MATERIAL_UOM,UOM_ST,UOM_ST_SAP,TP_UOM,PRODUCTION_ORDER_ID,PRODUCTION_ORDER_DESC,PRODUCTION_ORDER_RATE,PRODUCTION_ORDER_UOM,JOB_START_TIME_LOC,JOB_END_TIME_LOC,UTIL_STATE_DESCRIPTION,UTIL_REASON_DESCRIPTION,UTIL_ALT_LANGUAGE_REASON,CO_TYPE,CO_ORIGINAL_TYPE,SHIFT_NAME,SHIFT_START_DATE_LOC,SHIFT_END_DATE_LOC,SHIFT_DURATION_MIN,CREW_ID,RAMPUP_FLAG,PRODUCTIONDATE_DAY_LOC,PRODUCTIONDATE_MONTH_LOC,PRODUCTIONDATE_QUARTER_LOC,PRODUCTIONDATE_YEAR_LOC,PRIMARY_CONV_FACTOR,PLANT_DESCRIPTION,PLANT_STRATEGIC_LOCATION,PLANT_COUNTRY,PLANT_COUNTRY_DESCRIPTION,PLANT_FACILITY_TYPE,PLANT_POSTAL_CODE,PLANT_PURCHASING_ORGANIZATION,PLANT_STRATEGIC_LOCATION_DESCRIPTION,PLANT_LATITUDE,PLANT_LONGITUDE,PLANT_DIVISION,PLANT_DIVISION_DESCRIPTION,PLANT_SUB_DIVISION,PLANT_SUB_DIVISION_DESCRIPTION,AE_MODEL_CATEGORY,TOTAL_TIME_SECONDS,TOTAL_TIME,BUSINESS_EXTERNAL_TIME,PLANT_AVAILABLE_TIME,EFFECTIVE_RUNTIME,PLANT_DECISION_TIME,PRODUCTION_AVAILABLE_TIME,GOOD_PRODUCTION_QTY,REJECT_PRODUCTION_QTY,DOWNTIME,RUN_TIME,NOT_ENTERED,WAITING_TIME,PLANT_EXPERIMENTATION,ALL_MAINTENANCE,AUTONOMOUS_MAINTENANCE,PLANNED_MAINTENANCE,DAYS_MTD,DAYS_YTD,AVG_THROUGHPUT_MTD,AVG_THROUGHPUT_YTD,CHANGEOVER_COUNT,CHANGEOVER_DURATION,CLEANING_AND_SANITIZATION,LUNCH_AND_BREAK,LUNCH,BREAK,MEETING_AND_TRAINING,NO_DEMAND,SOURCE_DATASET,SOURCE_DATASET_FUNCTIONAL_AREA,SOURCE_DATASET_SUBFUNCTIONAL_AREA
FIPCO006,FIPCO006,224.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Kids,TFS30,000000000061024733,61024733,OFT 38FR_4.6oz_12_Size,CS,12.0,12.0,12.0,108432658.0,108432658.0,12.0,CS,2025-02-05 22:40:21.000 -0500,2025-02-09 00:26:51.000 -0500,DOWNTIME,Ending order,Terminando Orden,Size,,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,7,0.116667,0.0,0.116667,0.0,0.0,0.116667,0,0,0.116667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05316A,MX05316A,OFT 38FR_4.0oz_24_Size,CS,24.0,24.0,24.0,108430841.0,108430841.0,450.0,CS,2025-02-05 09:02:46.000 -0500,2025-02-06 14:17:21.000 -0500,DOWNTIME,Copas,Copas,ALL,ALL,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,747,12.45,0.0,12.45,0.0,0.0,12.45,0,0,12.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO006,FIPCO006_Filler,225.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Kids,TFS30,MX03109A,MX03109A,OFT 38TP_75ml_12_Size,CS,12.0,12.0,12.0,108430287.0,108430287.0,144.0,CS,2025-02-05 20:12:40.000 -0500,2025-02-05 20:13:31.000 -0500,PLANNED,Cleaning and Sanitation,C&S,Size,Size,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,1255,20.916667,0.0,20.916667,0.0,20.916667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,20.916667,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G_CasePacker,193.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,5.829999924,CS,2025-02-05 19:17:20.000 -0500,,DOWNTIME,Entrada plegadiza CCC,Entrada plegadiza CCC,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,153,2.55,0.0,2.55,0.0,0.0,2.55,0,0,2.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,210.0,CS,2025-02-07 02:17:18.000 -0500,2025-02-08 03:28:39.000 -0500,PLANNED,Cleaning and Sanitation,C&S,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,849,14.15,0.0,14.15,0.0,14.15,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,14.15,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_CasePacker,146.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000000151105,151105,OFT 25FZ_2.5oz_24_Size,CS,24.0,24.0,24.0,108433946.0,108433946.0,18.75,CS,2025-02-07 14:04:19.000 -0500,2025-02-09 20:23:49.000 -0500,RUNNING,Running,Corriendo,Product,Product,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,9343,155.716666,0.0,155.716666,0.0,0.0,155.716666,0,0,0.0,155.716666,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_CasePacker,165.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,6.079999924,CS,2025-02-05 00:08:14.000 -0500,2025-02-06 22:02:41.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,114.638159333,0.0,0.0,697,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,36.684210526,8.935897436,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO001,FIPCO001_CasePacker2,309.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX03924A,MX03924A,OFT 25FT_50ml_144_Size,CS,144.0,144.0,144.0,108429891.0,108429891.0,3.130000114,CS,2025-02-04 15:21:14.000 -0500,,BUSINESS EXTERNAL,No Demand,No Demanda,Product,,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,144.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Business External,27042,450.7,450.7,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,450.7,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002,162.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108432977.0,108432977.0,5.42,CS,2025-02-06 22:02:43.000 -0500,2025-02-06 22:59:03.000 -0500,DOWNTIME,Ending order,Terminando Orden,Size,Size,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,262,4.366667,0.0,4.366667,0.0,0.0,4.366667,0,0,4.366667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,210.0,CS,2025-02-07 02:17:18.000 -0500,2025-02-08 03:28:39.000 -0500,DOWNTIME,Cinta de descarga,Cinta de descarga,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,49,0.816667,0.0,0.816667,0.0,0.0,0.816667,0,0,0.816667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Cartoner,164.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108432977.0,108432977.0,390.0,CS,2025-02-06 22:03:12.000 -0500,2025-02-06 22:59:08.000 -0500,WAITING,Upstream,Falta Producto,Size,Size,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,196,3.266666,0.0,3.266666,0.0,0.0,3.266666,0,0,0.0,0.0,0.0,3.266666,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Bundler,177.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000151114,151114,OFT 35FT_4.0oz_24_Size,CS,24.0,24.0,24.0,108432723.0,108432723.0,450.0,CS,2025-02-06 06:52:20.000 -0500,2025-02-06 20:14:53.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,13056,217.600001,0.0,217.600001,0.0,0.0,217.600001,0,0,0.0,217.600001,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Filler,166.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108433722.0,108433722.0,390.0,CS,2025-02-06 23:03:27.000 -0500,,PLANNED,Meeting,Junta,Size,Size,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,711,11.85,0.0,11.85,0.0,11.85,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,11.85,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Cartoner,178.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000156755,156755,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108430289.0,108430289.0,450.0,CS,2025-02-05 00:04:30.000 -0500,2025-02-05 16:37:35.000 -0500,DOWNTIME,Paro operador,Paro operador,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,49,0.816667,0.0,0.816667,0.0,0.0,0.816667,0,0,0.816667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Filler,180.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000151406,151406,OFT 35FT_4.0oz_24_Size,CS,24.0,24.0,24.0,108433622.0,108433622.0,450.0,CS,2025-02-06 20:15:08.000 -0500,2025-02-07 13:06:17.000 -0500,CHANGE OVER,Product,Producto,Product,Product,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,2117,35.283333,0.0,35.283333,0.0,35.283333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,1,35.283333,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Filler,143.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000000151106,151106,OFT 25FZ_2.5oz_24_Size,CS,24.0,24.0,24.0,108433945.0,108433945.0,450.0,CS,2025-02-07 00:07:52.000 -0500,2025-02-07 13:58:36.000 -0500,CHANGE OVER,Product,Producto,Size,Size,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,509,8.483333,0.0,8.483333,0.0,8.483333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G_Cartoner,192.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,210.0,CS,2025-02-05 19:17:20.000 -0500,,WAITING,Downstream,Acumulación,ALL,ALL,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,1362,22.700001,0.0,22.700001,0.0,0.0,22.700001,0,0,0.0,0.0,0.0,22.700001,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00F,FIPCO00F_Cartoner,185.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,FMX03179D,FMX03179D,OFT 28FT_75ml_72_Size,CS,72.0,72.0,72.0,108430290.0,108430290.0,450.0,CS,2025-02-05 04:12:19.000 -0500,2025-02-07 13:20:31.000 -0500,WAITING,Upstream,Falta Producto,Product,Product,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,2907,48.450002,0.0,48.450002,0.0,0.0,48.450002,0,0,0.0,0.0,0.0,48.450002,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Bundler,205.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,210.0,CS,2025-02-07 02:17:18.000 -0500,2025-02-08 03:28:39.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,4807,80.116666,0.0,80.116666,0.0,0.0,80.116666,0,0,0.0,80.116666,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Palletizer,181.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000151114,151114,OFT 35FT_4.0oz_24_Size,CS,24.0,24.0,24.0,108432723.0,108432723.0,18.75,CS,2025-02-06 06:52:21.000 -0500,2025-02-06 20:14:53.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,196.0,0.0,0.0,3675,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,193.421052632,47.115384615,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003_Cartoner,171.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061001979,61001979,OFT 38FR_100ml_72_Size,CS,72.0,72.0,72.0,108427066.0,108427066.0,450.0,CS,2025-02-04 17:47:52.000 -0500,2025-02-05 10:18:42.000 -0500,DOWNTIME,Paro en Empaque,Paro En Empaque,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,1223,20.383334,0.0,20.383334,0.0,0.0,20.383334,0,0,20.383334,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_CasePacker,200.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX05611,FMX05611,OFT 35FT_125ml_48_Size,CS,48.0,48.0,48.0,108430843.0,108430843.0,9.380000114,CS,2025-02-05 03:02:04.000 -0500,2025-02-06 09:33:24.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,52.98507398,0.0,0.0,497,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,26.157894737,6.371794872,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Bundler,205.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,402.0,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,CHANGE OVER,ALL,Todo,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,2269,37.816667,0.0,37.816667,0.0,37.816667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05316A,MX05316A,OFT 38FR_4.0oz_24_Size,CS,24.0,24.0,24.0,108430841.0,108430841.0,450.0,CS,2025-02-05 09:02:46.000 -0500,2025-02-06 14:17:21.000 -0500,COMMON DOWNTIMES,Other,Otros,ALL,ALL,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,629,10.483333,0.0,10.483333,0.0,0.0,10.483333,0,0,10.483333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_CasePacker,151.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,16.75,CS,2025-02-06 14:17:53.000 -0500,2025-02-08 01:08:30.000 -0500,DOWNTIME,Cadena de avance,Cadena de avance,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,88,1.466667,0.0,1.466667,0.0,0.0,1.466667,0,0,1.466667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05316A,MX05316A,OFT 38FR_4.0oz_24_Size,CS,24.0,24.0,24.0,108430841.0,108430841.0,450.0,CS,2025-02-05 09:02:46.000 -0500,2025-02-06 14:17:21.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,4841,80.683335,0.0,80.683335,0.0,0.0,80.683335,0,0,0.0,80.683335,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00J,FIPCO00J_Filler,769.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061012223,61012223,OFT 28FT_60ml_24_3P_Size,CS,72.0,72.0,72.0,108427065.0,108427065.0,450.0,CS,2025-02-05 07:07:27.000 -0500,,DOWNTIME,paro operador,Paro operador,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,108,1.799999,0.0,1.799999,0.0,0.0,1.799999,0,0,1.799999,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,402.0,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,10312,171.866667,0.0,171.866667,0.0,0.0,171.866667,0,0,0.0,171.866667,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Cartoner,178.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000151093,151093,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108431438.0,108431438.0,450.0,CS,2025-02-05 16:38:05.000 -0500,2025-02-06 06:51:16.000 -0500,RUNNING,Running,Corriendo,Product,Product,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,3299,54.983334,0.0,54.983334,0.0,0.0,54.983334,0,0,0.0,54.983334,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_CasePacker,200.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,MX02080D,MX02080D,OFT 35FR_75ml_48_Size,CS,48.0,48.0,48.0,108432978.0,108432978.0,9.380000114,CS,2025-02-06 09:34:17.000 -0500,2025-02-07 13:38:44.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,208.31556249,0.0,0.0,1954,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,102.842105263,25.051282051,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Cartoner,145.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX07159A,MX07159A,OFT 25FT_2.2oz_24_Size,CS,24.0,24.0,24.0,108429893.0,108429893.0,450.0,CS,2025-02-06 06:08:36.000 -0500,2025-02-07 00:07:28.000 -0500,PLANNED,Cleaning and Sanitation,C&S,Product,Product,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,426,7.1,0.0,7.1,0.0,7.1,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,7.1,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Bundler,144.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX06994A,MX06994A,OFT_25FT_2.0oz_24_Size,CS,24.0,24.0,24.0,108432488.0,108432488.0,450.0,CS,2025-02-05 18:14:56.000 -0500,2025-02-06 06:08:02.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,34.631111111,0.0,0.0,15584,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,820.210526316,199.794871795,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_CasePacker,151.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061044255,61044255,OFT 38FR_120ml_24_Size,CS,24.0,24.0,24.0,108430288.0,108430288.0,18.75,CS,2025-02-04 16:06:17.000 -0500,2025-02-05 09:01:41.000 -0500,WAITING,Downstream,Acumulación,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,295,4.916667,0.0,4.916667,0.0,0.0,4.916667,0,0,0.0,0.0,0.0,4.916667,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO001,FIPCO001_Filler,159.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000061039400,61039400,OFT 25FT_50ml_144_Size,CS,144.0,144.0,144.0,108435845.0,108435845.0,450.0,CS,2025-02-07 20:02:50.000 -0500,2025-02-10 07:14:24.000 -0500,COMMON DOWNTIMES,Lack of Cream / Change Feeding,Falta de crema/Cambio alimentación,Product,Product,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,144.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,734,12.233333,0.0,12.233333,0.0,0.0,12.233333,0,0,12.233333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_CasePacker,200.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,MX02080D,MX02080D,OFT 35FR_75ml_48_Size,CS,48.0,48.0,48.0,108432978.0,108432978.0,9.380000114,CS,2025-02-06 09:34:17.000 -0500,2025-02-07 13:38:44.000 -0500,PLANNED,Cleaning and Sanitation,C&S,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,587,9.783333,0.0,9.783333,0.0,9.783333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,9.783333,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Filler,208.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,402.0,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,DOWNTIME,Ending order,Terminando Orden,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,18,0.3,0.0,0.3,0.0,0.0,0.3,0,0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061044255,61044255,OFT 38FR_120ml_24_Size,CS,24.0,24.0,24.0,108430288.0,108430288.0,450.0,CS,2025-02-04 16:06:17.000 -0500,2025-02-05 09:01:41.000 -0500,DOWNTIME,Nivel de tanque A B C,Nivel de tanque A B C,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,258,4.3,0.0,4.3,0.0,0.0,4.3,0,0,4.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003,169.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,MX05532A,MX05532A,OFT 38FR_125ml_48_Size,CS,48.0,48.0,48.0,108433621.0,108433621.0,9.38,CS,2025-02-06 18:16:14.000 -0500,2025-02-08 00:18:07.000 -0500,DOWNTIME,Filler,LLenadora,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,2818,46.966667,0.0,46.966667,0.0,0.0,46.966667,0,0,46.966667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,402.0,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,DOWNTIME,Sistema de cola,Sistema de cola,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,824,13.733334,0.0,13.733334,0.0,0.0,13.733334,0,0,13.733334,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Bundler,149.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,402.0,CS,2025-02-06 14:17:55.000 -0500,2025-02-08 01:08:30.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,18965,316.083336,0.0,316.083336,0.0,0.0,316.083336,0,0,0.0,316.083336,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I,204.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,17.5,CS,2025-02-07 02:16:44.000 -0500,2025-02-08 03:28:26.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,4515,75.249998,0.0,75.249998,0.0,0.0,75.249998,0,0,0.0,75.249998,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00F,FIPCO00F_Filler,187.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,FMX03179D,FMX03179D,OFT 28FT_75ml_72_Size,CS,72.0,72.0,72.0,108430290.0,108430290.0,450.0,CS,2025-02-05 04:12:19.000 -0500,2025-02-07 13:20:32.000 -0500,RUNNING,Running,Corriendo,Product,Product,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,22814,380.233334,0.0,380.233334,0.0,0.0,380.233334,0,0,0.0,380.233334,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I,204.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,16.75,CS,2025-02-05 04:56:34.000 -0500,2025-02-07 02:16:31.000 -0500,DOWNTIME,Bundler,Empaquetadora,Product,Product,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,2228,37.133331,0.0,37.133331,0.0,0.0,37.133331,0,0,37.133331,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_Filler,201.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX05611,FMX05611,OFT 35FT_125ml_48_Size,CS,48.0,48.0,48.0,108430843.0,108430843.0,450.0,CS,2025-02-05 03:02:04.000 -0500,2025-02-06 09:33:24.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,286.726666667,0.0,0.0,129027,121,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,6790.894736842,1654.192307692,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E,176.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000000151093,151093,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108431438.0,108431438.0,18.75,CS,2025-02-05 16:37:43.000 -0500,2025-02-06 06:51:09.000 -0500,CHANGE OVER,Size,Tamaño,Product,Product,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,2972,49.533333,0.0,49.533333,0.0,49.533333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00J,FIPCO00J_Cartoner,767.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061012223,61012223,OFT 28FT_60ml_24_3P_Size,CS,72.0,72.0,72.0,108427065.0,108427065.0,150.0,CS,2025-02-05 07:07:27.000 -0500,,CHANGE OVER,ALL,Todo,ALL,ALL,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,1174,19.566667,0.0,19.566667,0.0,19.566667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,1,19.566667,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G_CasePacker,193.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061029036,61029036,OFT 28FT_66ml_36_2P Size,CS,72.0,72.0,72.0,108423546.0,108423546.0,5.829999924,CS,2025-02-05 02:42:16.000 -0500,2025-02-05 19:16:09.000 -0500,PLANNED,Cleaning and Sanitation,C&S,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,918,15.3,0.0,15.3,0.0,15.3,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,15.3,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Cartoner,164.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,438.0,CS,2025-02-05 00:08:44.000 -0500,2025-02-06 22:02:41.000 -0500,COMMON DOWNTIMES,OutSpec_Tube,FueraEspec_Tubo,Packing,Packing,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,2069,34.483333,0.0,34.483333,0.0,0.0,34.483333,0,0,34.483333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,402.0,CS,2025-02-06 14:17:45.000 -0500,2025-02-08 01:08:30.000 -0500,WAITING,Downstream,Acumulación,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,6868,114.466667,0.0,114.466667,0.0,0.0,114.466667,0,0,0.0,0.0,0.0,114.466667,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G,190.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,5.83,CS,2025-02-05 19:16:10.000 -0500,2025-02-08 21:33:56.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,27170,452.833333,0.0,452.833333,0.0,0.0,452.833333,0,0,0.0,452.833333,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Cartoner,164.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108433722.0,108433722.0,390.0,CS,2025-02-06 23:03:27.000 -0500,,RUNNING,Running,Corriendo,Size,Size,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,15343,255.716665,0.0,255.716665,0.0,0.0,255.716665,0,0,0.0,255.716665,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_CasePacker,207.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,16.75,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,DOWNTIME,Fallo de Pacrob,Fallo de Pacrob,ALL,ALL,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,141,2.35,0.0,2.35,0.0,0.0,2.35,0,0,2.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO001,FIPCO001,155.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Low Complexity,TFS80-6,MX03924A,MX03924A,OFT 25FT_50ml_144_Size,CS,144.0,144.0,144.0,108429891.0,108429891.0,3.13,CS,2025-02-04 15:20:53.000 -0500,2025-02-07 20:02:16.000 -0500,DOWNTIME,Filler,LLenadora,Product,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,144.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,416,6.933333,0.0,6.933333,0.0,0.0,6.933333,0,0,6.933333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E,176.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000000151093,151093,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108431438.0,108431438.0,18.75,CS,2025-02-05 16:37:43.000 -0500,2025-02-06 06:51:09.000 -0500,DOWNTIME,Filler,LLenadora,Product,Product,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,124,2.066666,0.0,2.066666,0.0,0.0,2.066666,0,0,2.066666,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003,169.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,MX06922A,MX06922A,OFT 38FT_160ml_72_Size,CS,72.0,72.0,72.0,108430838.0,108430838.0,5.58,CS,2025-02-05 10:18:41.000 -0500,2025-02-06 02:19:34.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,167.741935484,0.0,0.0,936,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,49.263157895,12.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003_Cartoner,171.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061001979,61001979,OFT 38FR_100ml_72_Size,CS,72.0,72.0,72.0,108427066.0,108427066.0,450.0,CS,2025-02-04 17:47:52.000 -0500,2025-02-05 10:18:42.000 -0500,DOWNTIME,Falla de operación,Falla de operación,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,366,6.1,0.0,6.1,0.0,0.0,6.1,0,0,6.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Palletizer,181.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000156755,156755,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108430289.0,108430289.0,18.75,CS,2025-02-05 00:04:31.000 -0500,2025-02-05 16:37:35.000 -0500,PLANNED,Cleaning and Sanitation,C&S,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,1514,25.233333,0.0,25.233333,0.0,25.233333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,25.233333,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_CasePacker,151.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05316A,MX05316A,OFT 38FR_4.0oz_24_Size,CS,24.0,24.0,24.0,108430841.0,108430841.0,18.75,CS,2025-02-05 09:02:46.000 -0500,2025-02-06 14:17:21.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,203.146666667,0.0,0.0,3809,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,200.473684211,48.833333333,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Cartoner,164.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,438.0,CS,2025-02-05 00:08:44.000 -0500,2025-02-06 22:02:41.000 -0500,NOT ENTERED,Unknown,Desconocido,Packing,Packing,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,641,10.683334,0.0,10.683334,0.0,0.0,10.683334,0,0,10.683334,0.0,10.683334,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_CasePacker,165.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,6.079999924,CS,2025-02-05 00:08:14.000 -0500,2025-02-06 22:02:41.000 -0500,WAITING,Upstream,Falta Producto,Packing,,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,2585,43.083333,0.0,43.083333,0.0,0.0,43.083333,0,0,0.0,0.0,0.0,43.083333,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G,190.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,5.83,CS,2025-02-05 19:16:10.000 -0500,2025-02-08 21:33:56.000 -0500,DOWNTIME,Cartoner,Cartonadora,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,1188,19.8,0.0,19.8,0.0,0.0,19.8,0,0,19.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,210.0,CS,2025-02-07 02:17:18.000 -0500,2025-02-08 03:28:39.000 -0500,DOWNTIME,Sistema de cola,Sistema de cola,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,114,1.9,0.0,1.9,0.0,0.0,1.9,0,0,1.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G_Cartoner,192.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,210.0,CS,2025-02-05 19:17:20.000 -0500,,DOWNTIME,Error de introducción,Error de introducción,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,293,4.883333,0.0,4.883333,0.0,0.0,4.883333,0,0,4.883333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Filler,143.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000000151105,151105,OFT 25FZ_2.5oz_24_Size,CS,24.0,24.0,24.0,108433946.0,108433946.0,450.0,CS,2025-02-07 14:04:36.000 -0500,2025-02-09 20:23:49.000 -0500,DOWNTIME,Tubo Atorado en Dedo,Tubo Atorado en Dedo,Product,Product,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,339,5.65,0.0,5.65,0.0,0.0,5.65,0,0,5.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003_CasePacker,172.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061049090,61049090,OFT 38FR_170ml_72_Size,CS,72.0,72.0,72.0,108432487.0,108432487.0,5.579999924,CS,2025-02-06 02:20:13.000 -0500,2025-02-06 18:16:15.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,116.129033846,0.0,0.0,648,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,34.105263158,8.307692308,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO006,FIPCO006_CasePacker,227.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Kids,TFS30,000000000061024733,61024733,OFT 38FR_4.6oz_12_Size,CS,12.0,12.0,12.0,108432658.0,108432658.0,12.0,CS,2025-02-05 22:40:53.000 -0500,,PLANNED,Cleaning and Sanitation,C&S,Size,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,548,9.133333,0.0,9.133333,0.0,9.133333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,9.133333,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_Cartoner,199.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,MX02355A,MX02355A,OFT 35FR_75ml_48_Size,CS,48.0,48.0,48.0,108435404.0,108435404.0,450.0,CS,2025-02-07 13:39:17.000 -0500,2025-02-08 13:48:17.000 -0500,PLANNED,Cleaning and Sanitation,C&S,Product,Product,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,886,14.766666,0.0,14.766666,0.0,14.766666,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,14.766666,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Cartoner,164.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108432977.0,108432977.0,390.0,CS,2025-02-06 22:03:12.000 -0500,2025-02-06 22:59:08.000 -0500,CHANGE OVER,Size,Tamaño,Size,Size,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,1990,33.166667,0.0,33.166667,0.0,33.166667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,1,33.166667,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00F,FIPCO00F_Bundler,184.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,FMX03179D,FMX03179D,OFT 28FT_75ml_72_Size,CS,72.0,72.0,72.0,108430290.0,108430290.0,450.0,CS,2025-02-05 04:12:19.000 -0500,2025-02-07 13:20:32.000 -0500,BUSINESS EXTERNAL,EMO,EMO,Product,Product,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Business External,16,0.266667,0.266667,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Bundler,149.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,402.0,CS,2025-02-06 14:17:55.000 -0500,2025-02-08 01:08:30.000 -0500,DOWNTIME,Apilador,Apilador,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,42,0.7,0.0,0.7,0.0,0.0,0.7,0,0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Filler,166.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX00035B,FMX00035B,OFT 38FT_150ml_72_Size,CS,72.0,72.0,72.0,108433722.0,108433722.0,390.0,CS,2025-02-06 23:03:27.000 -0500,,WAITING,Downstream,Acumulación,Size,Size,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,7268,121.13333,0.0,121.13333,0.0,0.0,121.13333,0,0,0.0,0.0,0.0,121.13333,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO001,FIPCO001_Palletizer,160.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX03924A,MX03924A,OFT 25FT_50ml_144_Size,CS,144.0,144.0,144.0,108429891.0,108429891.0,3.130000114,CS,2025-02-04 15:21:14.000 -0500,,DOWNTIME,Unknown,Desconocido,,,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,144.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,356.54950773,0.0,0.0,1116,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,58.736842105,14.307692308,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_CasePacker,146.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000000151106,151106,OFT 25FZ_2.5oz_24_Size,CS,24.0,24.0,24.0,108433945.0,108433945.0,18.75,CS,2025-02-07 00:07:52.000 -0500,2025-02-07 13:58:36.000 -0500,DOWNTIME,Extracción de cajas,Extracción de cajas,Size,Size,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,39,0.65,0.0,0.65,0.0,0.0,0.65,0,0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003_Bundler,170.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05532A,MX05532A,OFT 38FR_125ml_48_Size,CS,48.0,48.0,48.0,108433621.0,108433621.0,450.0,CS,2025-02-06 18:16:21.000 -0500,2025-02-08 00:18:15.000 -0500,WAITING,Downstream,Acumulación,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,885,14.75,0.0,14.75,0.0,0.0,14.75,0,0,0.0,0.0,0.0,14.75,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO006,FIPCO006,224.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Kids,TFS30,000000000061024733,61024733,OFT 38FR_4.6oz_12_Size,CS,12.0,12.0,12.0,108432658.0,108432658.0,12.0,CS,2025-02-05 22:40:21.000 -0500,2025-02-09 00:26:51.000 -0500,BUSINESS EXTERNAL,No Demand,No Demanda,Size,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Business External,28621,477.016667,477.016667,0.0,0.0,0.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,477.016667,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002_Bundler,163.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,438.0,CS,2025-02-05 00:08:44.000 -0500,2025-02-06 22:02:41.000 -0500,NOT ENTERED,Unknown,Desconocido,Packing,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,170,2.833334,0.0,2.833334,0.0,0.0,2.833334,0,0,2.833334,0.0,2.833334,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00F,FIPCO00F_Bundler,184.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,MX06103A,MX06103A,OFT 28FT_75ml_72_Size,CS,72.0,72.0,72.0,108433947.0,108433947.0,450.0,CS,2025-02-07 13:21:33.000 -0500,2025-02-08 11:04:16.000 -0500,DOWNTIME,Unknown,Desconocido,,,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,0,0.0,0.0,0.0,15.4,0.0,0.0,6930,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,364.736842105,88.846153846,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I,204.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,17.5,CS,2025-02-07 02:16:44.000 -0500,2025-02-08 03:28:26.000 -0500,DOWNTIME,Filler,LLenadora,ALL,ALL,Shift3,2025-02-06 21:30:00.000 -0500,2025-02-07 05:59:59.000 -0500,510.0,D,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,13,0.216667,0.0,0.216667,0.0,0.0,0.216667,0,0,0.216667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO006,FIPCO006_CasePacker,227.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Kids,TFS30,000000000061024733,61024733,OFT 38FR_4.6oz_12_Size,CS,12.0,12.0,12.0,108432658.0,108432658.0,12.0,CS,2025-02-05 22:40:53.000 -0500,,PLANNED,Lunch/Break,Comida,Size,,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,4218,70.3,0.0,70.3,0.0,70.3,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,70.3,70.3,70.3,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO003,FIPCO003_Bundler,170.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05532A,MX05532A,OFT 38FR_125ml_48_Size,CS,48.0,48.0,48.0,108433621.0,108433621.0,450.0,CS,2025-02-06 18:16:21.000 -0500,2025-02-08 00:18:15.000 -0500,MAINTENANCE,Autonomous Maintenance,Mntto. Autónomo,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,7,0.116667,0.0,0.116667,0.0,0.116667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.116667,0.116667,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_CasePacker,146.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX07159A,MX07159A,OFT 25FT_2.2oz_24_Size,CS,24.0,24.0,24.0,108429893.0,108429893.0,18.75,CS,2025-02-06 06:08:37.000 -0500,2025-02-07 00:07:28.000 -0500,CHANGE OVER,Product,Producto,Product,Product,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,8040,134.0,0.0,134.0,0.0,134.0,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,1,134.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061038725,61038725,OFT 38FR_3.0oz_12_2P_Size,CS,24.0,24.0,24.0,108433948.0,108433948.0,210.0,CS,2025-02-07 02:17:18.000 -0500,2025-02-08 03:28:39.000 -0500,CHANGE OVER,ALL,Todo,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,12.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,13241,220.683333,0.0,220.683333,0.0,220.683333,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Bundler,144.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,000000000000151105,151105,OFT 25FZ_2.5oz_24_Size,CS,24.0,24.0,24.0,108433946.0,108433946.0,450.0,CS,2025-02-07 14:04:34.000 -0500,2025-02-09 20:23:49.000 -0500,DOWNTIME,Transporte de láminas,Transporte de láminas,Product,Product,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,131,2.183333,0.0,2.183333,0.0,0.0,2.183333,0,0,2.183333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00F,FIPCO00F,183.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Packs,TFS80-6,FMX03179D,FMX03179D,OFT 28FT_75ml_72_Size,CS,72.0,72.0,72.0,108430290.0,108430290.0,6.25,CS,2025-02-05 04:11:32.000 -0500,2025-02-07 13:20:13.000 -0500,DOWNTIME,Cartoner,Cartonadora,ALL,ALL,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,1114,18.566667,0.0,18.566667,0.0,0.0,18.566667,0,0,18.566667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009,148.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,16.75,CS,2025-02-06 14:17:29.000 -0500,2025-02-08 01:08:22.000 -0500,CHANGE OVER,ALL,Todo,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Plant Decision,3898,64.966667,0.0,64.966667,0.0,64.966667,0.0,0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_CasePacker,207.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061019476,61019476,OFT 38FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108429895.0,108429895.0,16.75,CS,2025-02-05 04:56:58.000 -0500,2025-02-07 02:16:42.000 -0500,DOWNTIME,Transporte posterior,Transporte posterior,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,98,1.633333,0.0,1.633333,0.0,0.0,1.633333,0,0,1.633333,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00I,FIPCO00I_Cartoner,206.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061032091,61032091,OFT 38FR_4.2oz_24_Size,CS,24.0,24.0,24.0,108436118.0,108436118.0,450.0,CS,2025-02-08 03:28:58.000 -0500,2025-02-10 00:30:39.000 -0500,WAITING,Upstream,Falta Producto,ALL,ALL,Shift3,2025-02-07 21:30:00.000 -0500,2025-02-08 05:59:59.000 -0500,510.0,A,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,783,13.05,0.0,13.05,0.0,0.0,13.05,0,0,0.0,0.0,0.0,13.05,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Filler,152.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061044255,61044255,OFT 38FR_120ml_24_Size,CS,24.0,24.0,24.0,108430288.0,108430288.0,450.0,CS,2025-02-04 16:06:17.000 -0500,2025-02-05 09:01:41.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift1,2025-02-05 06:00:00.000 -0500,2025-02-05 13:59:59.000 -0500,480.0,B,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,1279,21.316666,0.0,21.316666,0.0,0.0,21.316666,0,0,0.0,21.316666,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E_Bundler,177.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000000178331,178331,OFT 35FT_4.6oz_24_Size,CS,24.0,24.0,24.0,108434583.0,108434583.0,450.0,CS,2025-02-07 13:07:08.000 -0500,2025-02-09 13:21:49.000 -0500,WAITING,Upstream,Falta Producto,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,166,2.766666,0.0,2.766666,0.0,0.0,2.766666,0,0,0.0,0.0,0.0,2.766666,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO002,FIPCO002,162.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,Secondary Line,TFS80-6,FMX03180C,FMX03180C,OFT 32FT_100ml_72_Size,CS,72.0,72.0,72.0,108429892.0,108429892.0,6.08,CS,2025-02-05 00:07:43.000 -0500,2025-02-06 22:02:32.000 -0500,DOWNTIME,Bundler,Empaquetadora,Packing,,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,72.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,2017,33.616667,0.0,33.616667,0.0,0.0,33.616667,0,0,33.616667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E,176.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000000178331,178331,OFT 35FT_4.6oz_24_Size,CS,24.0,24.0,24.0,108434583.0,108434583.0,18.75,CS,2025-02-07 13:06:18.000 -0500,2025-02-09 13:21:37.000 -0500,DOWNTIME,Bundler,Empaquetadora,ALL,ALL,Shift2,2025-02-07 14:00:00.000 -0500,2025-02-07 21:29:59.000 -0500,450.0,C,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,3030,50.5,0.0,50.5,0.0,0.0,50.5,0,0,50.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Cartoner,150.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,MX05316A,MX05316A,OFT 38FR_4.0oz_24_Size,CS,24.0,24.0,24.0,108430841.0,108430841.0,450.0,CS,2025-02-05 09:02:46.000 -0500,2025-02-06 14:17:21.000 -0500,DOWNTIME,Ending order,Terminando Orden,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,6,0.1,0.0,0.1,0.0,0.0,0.1,0,0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_Filler,143.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX06994A,MX06994A,OFT_25FT_2.0oz_24_Size,CS,24.0,24.0,24.0,108432488.0,108432488.0,450.0,CS,2025-02-05 18:14:49.000 -0500,2025-02-06 06:08:00.000 -0500,WAITING,Downstream,Acumulación,Product,Product,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,1600,26.666669,0.0,26.666669,0.0,0.0,26.666669,0,0,0.0,0.0,0.0,26.666669,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E,176.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000000151093,151093,OFT 35FT_6.0oz_24_Size,CS,24.0,24.0,24.0,108431438.0,108431438.0,18.75,CS,2025-02-05 16:37:43.000 -0500,2025-02-06 06:51:09.000 -0500,DOWNTIME,Cartoner,Cartonadora,Product,Product,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,1727,28.783335,0.0,28.783335,0.0,0.0,28.783335,0,0,28.783335,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00E,FIPCO00E,176.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Line,,High Complexity,TFS80-6,000000000000151114,151114,OFT 35FT_4.0oz_24_Size,CS,24.0,24.0,24.0,108432723.0,108432723.0,18.75,CS,2025-02-06 06:51:27.000 -0500,2025-02-06 20:14:45.000 -0500,DOWNTIME,Filler,LLenadora,ALL,ALL,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,3719,61.983334,0.0,61.983334,0.0,0.0,61.983334,0,0,61.983334,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00G,FIPCO00G_Palletizer,195.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061028649,61028649,OFT 35FT_115ml_36_2P_Size,CS,72.0,72.0,72.0,108431439.0,108431439.0,5.829999924,CS,2025-02-05 19:17:21.000 -0500,,RUNNING,Running,Corriendo,ALL,ALL,Shift3,2025-02-05 21:30:00.000 -0500,2025-02-06 05:59:59.000 -0500,510.0,D,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,36.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,29829,497.15,0.0,497.15,0.0,0.0,497.15,0,0,0.0,497.15,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO009,FIPCO009_Palletizer,153.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,High Complexity,TFS80-6,000000000061045447,61045447,OFT 38FR_170ml_24 Size,CS,24.0,24.0,24.0,108432764.0,108432764.0,16.75,CS,2025-02-06 14:17:45.000 -0500,2025-02-08 01:08:30.000 -0500,RUNNING,Running,Corriendo,ALL,ALL,Shift2,2025-02-06 14:00:00.000 -0500,2025-02-06 21:29:59.000 -0500,450.0,C,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Runtime,16994,283.233333,0.0,283.233333,0.0,0.0,283.233333,0,0,0.0,283.233333,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO004,FIPCO004_CasePacker,146.0,MX11,,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Low Complexity,TFS80-6,MX06994A,MX06994A,OFT_25FT_2.0oz_24_Size,CS,24.0,24.0,24.0,108432488.0,108432488.0,18.75,CS,2025-02-05 18:14:56.000 -0500,2025-02-06 06:08:02.000 -0500,WAITING,Upstream,Falta Producto,Size,Size,Shift2,2025-02-05 14:00:00.000 -0500,2025-02-05 21:29:59.000 -0500,450.0,C,False,2025-02-05,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Waiting,930,15.5,0.0,15.5,0.0,0.0,15.5,0,0,0.0,0.0,0.0,15.5,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00H,FIPCO00H_Filler,201.0,MX11,Adjustment,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Secondary Line,TFS80-6,MX02080D,MX02080D,OFT 35FR_75ml_48_Size,CS,48.0,48.0,48.0,108432978.0,108432978.0,450.0,CS,2025-02-06 09:33:48.000 -0500,2025-02-07 13:38:44.000 -0500,DOWNTIME,Transporte de cajas,Transporte de cajas,ALL,ALL,Shift1,2025-02-07 06:00:00.000 -0500,2025-02-07 13:59:59.000 -0500,480.0,B,False,2025-02-07,2025-02-01,2025-01-01,2025-01-01,48.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,46,0.766667,0.0,0.766667,0.0,0.0,0.766667,0,0,0.766667,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing
FIPCO00J,FIPCO00J_Cartoner,767.0,MX11,Breakdown,Full OPERA,PACK,OC,TPST,OralCare,Equipment,,Packs,TFS80-6,000000000061012223,61012223,OFT 28FT_60ml_24_3P_Size,CS,72.0,72.0,72.0,108427065.0,108427065.0,150.0,CS,2025-02-05 07:07:27.000 -0500,,DOWNTIME,Error de propulsión general,Error de propulsión general,ALL,ALL,Shift1,2025-02-06 06:00:00.000 -0500,2025-02-06 13:59:59.000 -0500,480.0,B,False,2025-02-06,2025-02-01,2025-01-01,2025-01-01,24.0,"Mexico, Mission Hills",Y,MX,Mexico,MFG,37980,MX03,Strategic,20.9817163,-100.4223264,LA,Latin America,LA,Latin America,Unplanned,177,2.95,0.0,2.95,0.0,0.0,2.95,0,0,2.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,19,78,0.0,0.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,OPERA,Supply Chain,Manufacturing


===========================================
FILE: ontology_generator/__init__.py
===========================================

"""
Ontology Generator

A modular Python application for generating OWL ontologies from CSV specifications and data.
"""

__version__ = '1.0.0'
__author__ = 'Ontology Generator Team'


===========================================
FILE: ontology_generator/analysis/__init__.py
===========================================

from .population import (
    analyze_ontology_population, generate_population_report,
    generate_optimization_recommendations
)
from .reasoning import generate_reasoning_report
from .sequence_analysis import (
    get_equipment_sequence_for_line, generate_equipment_sequence_report,
    analyze_equipment_sequences
)


===========================================
FILE: ontology_generator/analysis/population.py
===========================================

"""
Population analysis module for the ontology generator.

This module provides functions for analyzing the ontology population.
"""
from typing import Dict, List, Set, Tuple, Any, Optional

from owlready2 import Ontology, Thing, ThingClass

from ontology_generator.utils.logging import analysis_logger

def analyze_ontology_population(onto: Ontology, 
                                defined_classes: Dict[str, ThingClass], 
                                specification: List[Dict[str, str]]
                               ) -> Tuple[Dict[str, int], List[str], Dict[str, List[str]], Dict[str, List[str]]]:
    """
    Analyzes the population status of each class in the ontology.
    
    Args:
        onto: The ontology object
        defined_classes: Dictionary mapping class names to class objects
        specification: The original ontology specification
        
    Returns:
        tuple: (population_counts, empty_classes, class_instances, class_usage_info)
            - population_counts: Dict mapping class name to count of individuals
            - empty_classes: List of class names with no individuals
            - class_instances: Dict mapping class name to list of individual names
            - class_usage_info: Dict with additional usage analysis
    """
    analysis_logger.info("Starting analysis of ontology population")
    
    population_counts = {}
    empty_classes = []
    class_instances = {}
    
    # Extract the spec-defined classes
    spec_defined_classes = set()
    for row in specification:
        class_name = row.get('Proposed OWL Entity', '').strip()
        if class_name:
            spec_defined_classes.add(class_name)
    
    # Classes used in domain/range of properties
    property_domain_classes = set()
    property_range_classes = set()
    
    # Analyze property domains and ranges
    for prop in list(onto.object_properties()) + list(onto.data_properties()):
        if hasattr(prop, 'domain') and prop.domain:
            domains = prop.domain if isinstance(prop.domain, list) else [prop.domain]
            for domain in domains:
                if isinstance(domain, ThingClass):
                    property_domain_classes.add(domain.name)
        
        if hasattr(prop, 'range') and prop.range:
            ranges = prop.range if isinstance(prop.range, list) else [prop.range]
            for range_item in ranges:
                if isinstance(range_item, ThingClass):
                    property_range_classes.add(range_item.name)
    
    # Analyze instances
    for class_name, class_obj in defined_classes.items():
        # Skip owl:Thing which will have everything
        if class_obj is Thing:
            continue
            
        # Get all individuals of this class
        instances = list(onto.search(is_a=class_obj))
        count = len(instances)
        population_counts[class_name] = count
        
        if count == 0:
            empty_classes.append(class_name)
        else:
            # Store up to 10 instance names as examples
            sample_instances = [ind.name for ind in instances[:10]]
            class_instances[class_name] = sample_instances
    
    # Create class usage analysis
    class_usage_info = {
        'spec_defined': list(spec_defined_classes),
        'implemented_in_ontology': list(defined_classes.keys()),
        'in_property_domains': list(property_domain_classes),
        'in_property_ranges': list(property_range_classes),
        'populated_classes': list(set(defined_classes.keys()) - set(empty_classes)),
        'empty_classes': empty_classes,
        'extraneous_classes': list(set(defined_classes.keys()) - spec_defined_classes)
    }
    
    analysis_logger.info(f"Analysis complete. Found {len(population_counts)} classes, {len(empty_classes)} empty classes")
    return population_counts, empty_classes, class_instances, class_usage_info

def generate_population_report(population_counts: Dict[str, int], 
                               empty_classes: List[str], 
                               class_instances: Dict[str, List[str]],
                               defined_classes: Dict[str, ThingClass],
                               class_usage_info: Dict[str, List[str]] = None) -> str:
    """
    Generates a formatted report of the ontology population status.
    
    Args:
        population_counts: Dict mapping class name to count of individuals
        empty_classes: List of class names with no individuals
        class_instances: Dict mapping class name to list of individual names
        defined_classes: Dict mapping class names to class objects
        class_usage_info: Dict with additional usage analysis
        
    Returns:
        str: Formatted report text
    """
    report_lines = []
    
    # Add header
    report_lines.append("\n" + "="*80)
    report_lines.append(f"ONTOLOGY POPULATION REPORT")
    report_lines.append("="*80)
    
    # Summary statistics
    total_classes = len(defined_classes)
    populated_classes = total_classes - len(empty_classes)
    total_individuals = sum(population_counts.values())
    
    report_lines.append(f"\nSUMMARY:")
    report_lines.append(f"  • Total Classes: {total_classes}")
    report_lines.append(f"  • Populated Classes: {populated_classes} ({populated_classes/total_classes*100:.1f}%)")
    report_lines.append(f"  • Empty Classes: {len(empty_classes)} ({len(empty_classes)/total_classes*100:.1f}%)")
    report_lines.append(f"  • Total Individuals: {total_individuals}")
    
    # Spec vs. Implementation Analysis
    if class_usage_info:
        spec_defined = set(class_usage_info.get('spec_defined', []))
        implemented = set(class_usage_info.get('implemented_in_ontology', []))
        extraneous = set(class_usage_info.get('extraneous_classes', []))
        not_implemented = spec_defined - implemented
        
        report_lines.append("\nSPECIFICATION ANALYSIS:")
        report_lines.append(f"  • Classes in Specification: {len(spec_defined)}")
        report_lines.append(f"  • Classes Implemented in Ontology: {len(implemented)}")
        if extraneous:
            report_lines.append(f"  • Extraneous Classes (implemented but not in spec): {len(extraneous)}")
            report_lines.append(f"      {', '.join(sorted(list(extraneous)))}")
        if not_implemented:
            report_lines.append(f"  • Classes in Spec but Not Implemented: {len(not_implemented)}")
            report_lines.append(f"      {', '.join(sorted(list(not_implemented)))}")
        
        # Identify unused but defined classes
        used_in_properties = set(class_usage_info.get('in_property_domains', [])) | set(class_usage_info.get('in_property_ranges', []))
        populated = set(class_usage_info.get('populated_classes', []))
        unused_classes = implemented - used_in_properties - populated
        if unused_classes:
            report_lines.append(f"  • Completely Unused Classes (empty and not used in properties): {len(unused_classes)}")
            report_lines.append(f"      {', '.join(sorted(list(unused_classes)))}")
    
    # Populated classes
    report_lines.append("\nPOPULATED CLASSES (Class: Count)")
    populated_items = sorted([(k, v) for k, v in population_counts.items() if v > 0], 
                            key=lambda x: x[1], reverse=True)
    for class_name, count in populated_items:
        report_lines.append(f"  • {class_name}: {count}")
        # Add sample instances for classes with reasonable counts
        if count <= 20:  # Only show examples for classes with fewer instances
            examples = class_instances.get(class_name, [])
            if examples:
                report_lines.append(f"      Examples: {', '.join(examples[:5])}")
                if len(examples) > 5:
                    report_lines.append(f"      ... and {min(count, len(examples) - 5)} more")
    
    # Empty classes 
    if empty_classes:
        report_lines.append("\nEMPTY CLASSES:")
        for class_name in sorted(empty_classes):
            # Get parent class info for context
            class_obj = defined_classes.get(class_name)
            if class_obj and hasattr(class_obj, 'is_a') and class_obj.is_a:
                parent_names = [p.name for p in class_obj.is_a if p is not Thing]
                if parent_names:
                    # Check if used in property domain/range
                    usage = []
                    if class_usage_info and class_name in class_usage_info.get('in_property_domains', []):
                        usage.append("used in property domains")
                    if class_usage_info and class_name in class_usage_info.get('in_property_ranges', []):
                        usage.append("used in property ranges")
                    
                    if usage:
                        report_lines.append(f"  • {class_name} (subclass of: {', '.join(parent_names)}) - {', '.join(usage)}")
                    else:
                        report_lines.append(f"  • {class_name} (subclass of: {', '.join(parent_names)}) - COMPLETELY UNUSED")
                else:
                    report_lines.append(f"  • {class_name} (direct subclass of owl:Thing)")
            else:
                report_lines.append(f"  • {class_name}")
    
    # Add optimization recommendations
    report_lines.append("\nOPTIMIZATION RECOMMENDATIONS:")
    
    if class_usage_info and 'extraneous_classes' in class_usage_info and class_usage_info['extraneous_classes']:
        report_lines.append("  • Consider adding the extraneous classes to your specification for completeness")
    
    if class_usage_info and 'spec_defined' in class_usage_info and implemented - spec_defined:
        report_lines.append("  • Review and consider removing classes that are implemented but not in your spec")
    
    if unused_classes:
        report_lines.append("  • Consider removing completely unused classes that are neither populated nor referenced in properties")
    
    return "\n".join(report_lines)

def generate_optimization_recommendations(class_usage_info: Dict[str, List[str]],
                                      defined_classes: Dict[str, ThingClass]) -> Dict[str, List[str]]:
    """
    Generates specific recommendations for optimizing the ontology structure.
    
    Args:
        class_usage_info: Dict with usage analysis data
        defined_classes: Dictionary mapping class names to class objects
        
    Returns:
        Dict with categorized recommendations
    """
    recommendations = {
        'classes_to_remove': [],
        'extraneous_classes': [],
        'unused_properties': [],
        'configuration_options': []
    }
    
    # Extract necessary data from usage info
    implemented = set(class_usage_info.get('implemented_in_ontology', []))
    spec_defined = set(class_usage_info.get('spec_defined', []))
    extraneous = set(class_usage_info.get('extraneous_classes', []))
    empty_classes = set(class_usage_info.get('empty_classes', []))
    in_domains = set(class_usage_info.get('in_property_domains', []))
    in_ranges = set(class_usage_info.get('in_property_ranges', []))
    
    # Find used defined classes
    populated_classes = implemented - empty_classes
    used_in_properties = in_domains | in_ranges
    
    # Find completely unused classes (not populated and not in domains/ranges)
    completely_unused = implemented - populated_classes - used_in_properties
    
    # Classes that are extraneous AND empty
    unused_extraneous = extraneous & empty_classes & completely_unused
    
    # Generate recommendations
    if unused_extraneous:
        recommendations['classes_to_remove'].extend(list(unused_extraneous))
        recommendations['configuration_options'].append(
            "Add a 'CLASSES_TO_SKIP' list in your configuration to avoid creating these classes"
        )
    
    if extraneous:
        recommendations['extraneous_classes'].extend(list(extraneous))
        if len(extraneous) > 5:
            recommendations['configuration_options'].append(
                "Consider using a 'STRICT_SPEC_ADHERENCE' option to only create classes defined in the spec"
            )
    
    # Check parent-child relationships for optimization
    class_hierarchies = {}
    for class_name, class_obj in defined_classes.items():
        if hasattr(class_obj, 'is_a'):
            parents = [p.name for p in class_obj.is_a if p is not Thing]
            if parents:
                class_hierarchies[class_name] = parents
    
    # Find unused leaf classes (classes that are completely unused and have no children)
    leaf_classes = set()
    for class_name in completely_unused:
        has_children = False
        for _, parents in class_hierarchies.items():
            if class_name in parents:
                has_children = True
                break
        if not has_children:
            leaf_classes.add(class_name)
    
    if leaf_classes:
        recommendations['classes_to_remove'].extend(list(leaf_classes))
        recommendations['configuration_options'].append(
            "Consider adding a 'PRUNE_LEAF_CLASSES' option to automatically remove unused leaf classes"
        )
    
    # Remove duplicates and sort for consistency
    for key in recommendations:
        recommendations[key] = sorted(list(set(recommendations[key])))
    
    return recommendations


===========================================
FILE: ontology_generator/analysis/reasoning.py
===========================================

"""
Reasoning analysis module for the ontology generator.

This module provides functions for generating reasoning reports.
"""
from typing import Dict, List, Tuple, Any, Set

from owlready2 import Ontology, ThingClass

from ontology_generator.utils.logging import analysis_logger

def generate_reasoning_report(onto: Ontology,
                             pre_stats: Dict[str, int],
                             post_stats: Dict[str, int],
                             inconsistent_classes: List[ThingClass],
                             inferred_hierarchy: Dict[str, Dict[str, List[str]]],
                             inferred_properties: Dict[str, List[str]],
                             inferred_individuals: Dict[str, Dict[str, Any]],
                             use_reasoner: bool,
                             max_entities_per_category: int = 10,  # Parameter to limit entities shown
                             verbose: bool = False  # Parameter to control detail level
                            ) -> Tuple[str, bool]:
    """
    Generates a structured report from reasoning results.
    
    Args:
        onto: The ontology object
        pre_stats: Dict with pre-reasoning statistics
        post_stats: Dict with post-reasoning statistics
        inconsistent_classes: List of inconsistent classes
        inferred_hierarchy: Dict of inferred class relationships
        inferred_properties: Dict of inferred property characteristics
        inferred_individuals: Dict of inferred individual relationships
        use_reasoner: Whether the reasoner was used
        max_entities_per_category: Maximum number of entities to show per category
        verbose: Whether to show all details
    
    Returns:
        tuple: (report_str, has_issues)
    """
    report_lines = []
    has_issues = False

    def add_section(title):
        report_lines.extend(["\n" + "="*80, f"{title}", "="*80])

    # 1. Executive Summary
    add_section("REASONING REPORT EXECUTIVE SUMMARY")
    if inconsistent_classes:
        has_issues = True
        report_lines.append("❌ ONTOLOGY STATUS: Inconsistent")
        report_lines.append(f"    Found {len(inconsistent_classes)} inconsistent classes (see details below)")
    else:
        report_lines.append("✅ ONTOLOGY STATUS: Consistent")

    class_diff = post_stats['classes'] - pre_stats['classes']
    prop_diff = (post_stats['object_properties'] - pre_stats['object_properties'] +
                 post_stats['data_properties'] - pre_stats['data_properties'])
    ind_diff = post_stats['individuals'] - pre_stats['individuals']
    report_lines.extend([
        f"\nStructural Changes (Post-Reasoning vs Pre-Reasoning):",
        f"  • Classes: {class_diff:+d}", f"  • Properties (Obj + Data): {prop_diff:+d}", f"  • Individuals: {ind_diff:+d}"
    ])
    inferences_made = bool(inferred_hierarchy or inferred_properties or inferred_individuals)
    report_lines.append(f"\nInferences Made: {'Yes' if inferences_made else 'No'}")

    # 2. Detailed Statistics
    add_section("DETAILED STATISTICS")
    report_lines.extend([
        "\nPre-Reasoning:",
        f"  • Classes: {pre_stats['classes']}", f"  • Object Properties: {pre_stats['object_properties']}",
        f"  • Data Properties: {pre_stats['data_properties']}", f"  • Individuals: {pre_stats['individuals']}",
        "\nPost-Reasoning:",
        f"  • Classes: {post_stats['classes']}", f"  • Object Properties: {post_stats['object_properties']}",
        f"  • Data Properties: {post_stats['data_properties']}", f"  • Individuals: {post_stats['individuals']}"
    ])

    # 3. Consistency Issues
    if inconsistent_classes:
        add_section("CONSISTENCY ISSUES")
        report_lines.append("\nInconsistent Classes:")
        
        # Show all inconsistent classes regardless of verbosity - these are critical
        for cls in inconsistent_classes: 
            report_lines.append(f"  • {cls.name} ({cls.iri})")
        has_issues = True

    # 4. Inferred Knowledge
    add_section("INFERRED KNOWLEDGE")
    if inferred_hierarchy:
        report_lines.append("\nClass Hierarchy Changes:")
        
        # Apply entity limitation based on verbosity
        hierarchy_items = list(inferred_hierarchy.items())
        if not verbose and len(hierarchy_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(hierarchy_items)} classes with hierarchy changes")
            hierarchy_items = hierarchy_items[:max_entities_per_category]
            
        for parent, data in hierarchy_items:
            if data.get('subclasses') or data.get('equivalent'):
                report_lines.append(f"\n  Class: {parent}")
                if data.get('subclasses'):
                    subclass_items = data['subclasses']
                    if not verbose and len(subclass_items) > max_entities_per_category:
                        report_lines.append(f"    ↳ Inferred Subclasses: ({len(subclass_items)} total, showing {max_entities_per_category})")
                        for sub in subclass_items[:max_entities_per_category]:
                            report_lines.append(f"        • {sub}")
                        report_lines.append(f"        • ... and {len(subclass_items) - max_entities_per_category} more")
                    else:
                        report_lines.append("    ↳ Inferred Subclasses:")
                        for sub in subclass_items:
                            report_lines.append(f"        • {sub}")
                
                if data.get('equivalent'):
                    equiv_items = data['equivalent']
                    if not verbose and len(equiv_items) > max_entities_per_category:
                        report_lines.append(f"    ≡ Inferred Equivalent Classes: {', '.join(equiv_items[:max_entities_per_category])} ... and {len(equiv_items) - max_entities_per_category} more")
                    else:
                        report_lines.append(f"    ≡ Inferred Equivalent Classes: {', '.join(equiv_items)}")
    else: 
        report_lines.append("\nNo new class hierarchy relationships inferred.")

    if inferred_properties:
        report_lines.append("\nInferred Property Characteristics:")
        
        # Apply entity limitation based on verbosity
        property_items = list(inferred_properties.items())
        if not verbose and len(property_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(property_items)} properties with inferred characteristics")
            property_items = property_items[:max_entities_per_category]
            
        for prop, chars in property_items:
            report_lines.append(f"\n  Property: {prop}")
            if not verbose and len(chars) > max_entities_per_category:
                for char in chars[:max_entities_per_category]:
                    report_lines.append(f"    • {char}")
                report_lines.append(f"    • ... and {len(chars) - max_entities_per_category} more")
            else:
                for char in chars:
                    report_lines.append(f"    • {char}")
    else: 
        report_lines.append("\nNo new property characteristics inferred.")

    if inferred_individuals:
        report_lines.append("\nIndividual Inferences:")
        
        # Apply entity limitation based on verbosity
        individual_items = list(inferred_individuals.items())
        if not verbose and len(individual_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(individual_items)} individuals with inferences")
            individual_items = individual_items[:max_entities_per_category]
            
        for ind_name, data in individual_items:
            report_lines.append(f"\n  Individual: {ind_name}")
            if data.get('types'):
                types_items = data['types']
                if not verbose and len(types_items) > max_entities_per_category:
                    report_lines.append(f"    Inferred Types: ({len(types_items)} total, showing {max_entities_per_category})")
                    for t in types_items[:max_entities_per_category]:
                        report_lines.append(f"      • {t}")
                    report_lines.append(f"      • ... and {len(types_items) - max_entities_per_category} more")
                else:
                    report_lines.append("    Inferred Types:")
                    for t in types_items:
                        report_lines.append(f"      • {t}")
                        
            if data.get('properties'):
                props_items = list(data['properties'].items())
                if not verbose and len(props_items) > max_entities_per_category:
                    report_lines.append(f"    Inferred Property Values: ({len(props_items)} total, showing {max_entities_per_category})")
                    for p, vals in props_items[:max_entities_per_category]:
                        if not verbose and len(vals) > max_entities_per_category:
                            report_lines.append(f"      • {p}: {', '.join(vals[:max_entities_per_category])} ... and {len(vals) - max_entities_per_category} more")
                        else:
                            report_lines.append(f"      • {p}: {', '.join(vals)}")
                    report_lines.append(f"      • ... and {len(props_items) - max_entities_per_category} more properties")
                else:
                    report_lines.append("    Inferred Property Values:")
                    for p, vals in props_items:
                        if not verbose and len(vals) > max_entities_per_category:
                            report_lines.append(f"      • {p}: {', '.join(vals[:max_entities_per_category])} ... and {len(vals) - max_entities_per_category} more")
                        else:
                            report_lines.append(f"      • {p}: {', '.join(vals)}")
    else: 
        report_lines.append("\nNo new individual types or property values inferred.")

    # 5. Recommendations
    add_section("RECOMMENDATIONS")
    recommendations = []
    if inconsistent_classes:
        recommendations.append("❗ HIGH PRIORITY: Resolve inconsistencies listed above.")
    if not inconsistent_classes and not inferences_made and use_reasoner:
        recommendations.append("⚠️ No inferences made - Ontology is consistent but may lack richness or reasoner configuration issue. Consider adding more specific axioms or reviewing reasoner setup.")
        # Don't flag as issue if reasoner wasn't run
        if use_reasoner: 
            has_issues = True
    if class_diff == 0 and prop_diff == 0 and ind_diff == 0 and use_reasoner:
       recommendations.append("ℹ️ No structural changes after reasoning - verify if this is expected.")
    if recommendations:
        report_lines.extend(["\n" + rec for rec in recommendations])
    else: 
        report_lines.append("\nNo critical issues or major inference gaps found.")

    return "\n".join(report_lines), has_issues


===========================================
FILE: ontology_generator/analysis/sequence_analysis.py
===========================================

"""
Sequence Analysis Module for the Ontology Generator.

This module provides functions for analyzing equipment sequences in the ontology.
"""
from typing import List, Optional, Dict, Any, Tuple
from owlready2 import Thing, Ontology

from ontology_generator.utils.logging import analysis_logger

def _safe_sort_by_attribute(items, attr_name, default_value="Unknown"):
    """
    Safely sorts items by an attribute, handling None values gracefully.
    
    Args:
        items: List of objects to sort
        attr_name: Name of attribute to sort by
        default_value: Default value to use for None attributes
        
    Returns:
        Sorted list of items
    """
    def get_safe_attribute(item):
        value = getattr(item, attr_name, None)
        if value is None:
            value = getattr(item, "name", default_value)
            if value is None:
                analysis_logger.warning(f"Item has neither {attr_name} nor name attribute, using default value for sorting")
                return default_value
        return value
        
    return sorted(items, key=get_safe_attribute)

def get_equipment_sequence_for_line(onto: Ontology, line_individual: Thing) -> List[Thing]:
    """
    Retrieves the equipment sequence for a specific production line.
    
    Args:
        onto: The ontology object
        line_individual: The ProductionLine individual
        
    Returns:
        A list of equipment individuals in sequence order
    """
    # Get required properties
    equipment_is_upstream_of = None
    for prop in onto.object_properties():
        if prop.name == "equipmentIsUpstreamOf":
            equipment_is_upstream_of = prop
            break
    
    if not equipment_is_upstream_of:
        analysis_logger.warning("Property 'equipmentIsUpstreamOf' not found in ontology")
        return []
    
    # Get all equipment on this line
    equipment_on_line = []
    for ind in onto.individuals():
        if hasattr(ind, "isPartOfProductionLine"):
            line_list = ind.isPartOfProductionLine
            if not isinstance(line_list, list):
                line_list = [line_list] if line_list else []
            
            if line_individual in line_list:
                equipment_on_line.append(ind)
    
    if not equipment_on_line:
        analysis_logger.info(f"No equipment found for line {line_individual.name}")
        return []
        
    analysis_logger.info(f"Found {len(equipment_on_line)} equipment instances on line {line_individual.name}")
    
    # Find equipment with no upstream equipment (start of sequence)
    start_equipment = []
    for eq in equipment_on_line:
        has_upstream = False
        for other_eq in equipment_on_line:
            upstream_list = getattr(other_eq, equipment_is_upstream_of.python_name, [])
            if not isinstance(upstream_list, list):
                upstream_list = [upstream_list] if upstream_list else []
                
            if eq in upstream_list:
                has_upstream = True
                break
        if not has_upstream:
            start_equipment.append(eq)
    
    analysis_logger.info(f"Found {len(start_equipment)} starting equipment (no upstream) for line {line_individual.name}")
    
    # Build sequence by following relationships
    sequence = []
    visited = set()
    
    def follow_sequence(eq):
        if eq in visited:
            return
        visited.add(eq)
        sequence.append(eq)
        
        downstream_list = getattr(eq, equipment_is_upstream_of.python_name, [])
        if not isinstance(downstream_list, list):
            downstream_list = [downstream_list] if downstream_list else []
        
        # Filter to equipment on this line only
        downstream_list = [d for d in downstream_list if d in equipment_on_line]
        
        # Sort by equipment ID if multiple downstream (unlikely but possible)
        if len(downstream_list) > 1:
            downstream_list.sort(key=lambda e: getattr(e, "equipmentId", e.name))
        
        for downstream in downstream_list:
            follow_sequence(downstream)
    
    # Start from each entry point
    for eq in start_equipment:
        follow_sequence(eq)
    
    analysis_logger.info(f"Determined sequence with {len(sequence)} equipment for line {line_individual.name}")
    return sequence

def generate_equipment_sequence_report(onto: Ontology) -> str:
    """
    Generates a report of equipment sequences for all lines in the ontology.
    
    Args:
        onto: The ontology object
        
    Returns:
        A string report of all equipment sequences
    """
    analysis_logger.info("Generating equipment sequence report for all lines")
    
    # Find all production lines
    lines = []
    
    # Get the ProductionLine class - search by name
    production_line_class = None
    for cls in onto.classes():
        if cls.name == "ProductionLine":
            production_line_class = cls
            break
    
    if not production_line_class:
        analysis_logger.warning("ProductionLine class not found in ontology - trying to find lines by lineId property")
        # Fallback to property-based detection with warnings about duplicates
        line_ids_seen = set()
        for ind in onto.individuals():
            if hasattr(ind, "lineId"):
                line_id = getattr(ind, "lineId")
                if line_id in line_ids_seen:
                    analysis_logger.warning(f"Duplicate lineId found: {line_id} - possible data quality issue")
                else:
                    line_ids_seen.add(line_id)
                    lines.append(ind)
    else:
        # Use proper class-based detection
        for ind in onto.individuals():
            if isinstance(ind, production_line_class):
                lines.append(ind)
    
    if not lines:
        analysis_logger.warning("No production lines found in ontology")
        return "No production lines found in ontology"
    
    analysis_logger.info(f"Found {len(lines)} production lines")
    
    # Log some line IDs for verification
    sample_size = min(5, len(lines))
    sample_lines = lines[:sample_size]
    sample_ids = [getattr(line, "lineId", line.name) for line in sample_lines]
    analysis_logger.info(f"Sample line IDs: {', '.join(map(str, sample_ids))}")
    
    report_lines = []
    report_lines.append("\n=== EQUIPMENT SEQUENCE REPORT ===")
    
    # Use safe sort for lines to avoid None comparison errors
    try:
        sorted_lines = _safe_sort_by_attribute(lines, "lineId")
    except Exception as e:
        analysis_logger.error(f"Error sorting lines: {e} - using unsorted lines")
        sorted_lines = lines
    
    for line in sorted_lines:
        line_id = getattr(line, "lineId", line.name)
        report_lines.append(f"\nLine: {line_id}")
        
        # Get sequence for this line
        sequence = get_equipment_sequence_for_line(onto, line)
        
        if not sequence:
            report_lines.append("  No equipment sequence found")
            continue
        
        # List equipment in sequence
        for i, eq in enumerate(sequence, 1):
            eq_id = getattr(eq, "equipmentId", "Unknown")
            eq_name = getattr(eq, "equipmentName", eq.name)
            eq_class = "Unknown"
            
            # Get class information
            if hasattr(eq, "memberOfClass") and eq.memberOfClass:
                if hasattr(eq.memberOfClass, "equipmentClassId"):
                    eq_class = eq.memberOfClass.equipmentClassId
            
            report_lines.append(f"  {i}. {eq_id} ({eq_name}) - Class: {eq_class}")
    
    return "\n".join(report_lines)

def analyze_equipment_sequences(onto: Ontology) -> Tuple[Dict[str, List[Thing]], Dict[str, Dict[str, Any]]]:
    """
    Analyzes all equipment sequences in the ontology and returns detailed information.
    
    Args:
        onto: The ontology object
        
    Returns:
        Tuple containing:
        - Dictionary mapping line IDs to equipment sequences
        - Dictionary with sequence statistics
    """
    analysis_logger.info("Analyzing equipment sequences in ontology")
    
    # Find all production lines using class-based detection
    lines = []
    
    # Get the ProductionLine class
    production_line_class = None
    for cls in onto.classes():
        if cls.name == "ProductionLine":
            production_line_class = cls
            break
    
    if production_line_class:
        for ind in onto.individuals():
            if isinstance(ind, production_line_class):
                lines.append(ind)
    else:
        # Fallback to property-based detection
        analysis_logger.warning("ProductionLine class not found in ontology - using lineId property")
        for ind in onto.individuals():
            if hasattr(ind, "lineId"):
                lines.append(ind)
    
    if not lines:
        analysis_logger.warning("No production lines found in ontology")
        return {}, {}
    
    # Generate sequences for each line
    sequences = {}
    stats = {"total_lines": len(lines), "lines_with_sequence": 0, "total_equipment": 0, "class_counts": {}}
    
    # Use safe sort
    try:
        sorted_lines = _safe_sort_by_attribute(lines, "lineId")
    except Exception as e:
        analysis_logger.error(f"Error sorting lines: {e} - using unsorted lines")
        sorted_lines = lines
    
    for line in sorted_lines:
        line_id = getattr(line, "lineId", line.name)
        sequence = get_equipment_sequence_for_line(onto, line)
        
        if sequence:
            sequences[line_id] = sequence
            stats["lines_with_sequence"] += 1
            stats["total_equipment"] += len(sequence)
            
            # Count equipment by class
            for eq in sequence:
                eq_class = "Unknown"
                if hasattr(eq, "memberOfClass") and eq.memberOfClass:
                    if hasattr(eq.memberOfClass, "equipmentClassId"):
                        eq_class = eq.memberOfClass.equipmentClassId
                
                if eq_class not in stats["class_counts"]:
                    stats["class_counts"][eq_class] = 0
                stats["class_counts"][eq_class] += 1
    
    analysis_logger.info(f"Analysis complete. Found sequences for {stats['lines_with_sequence']} of {stats['total_lines']} lines")
    return sequences, stats 

===========================================
FILE: ontology_generator/config.py
===========================================

"""
Ontology Generator Configuration

This module contains constants, mappings, and configuration settings for the ontology generator.
"""
from typing import Dict, Any, Type, Optional
from datetime import datetime, date, time

# --- General Configuration ---
DEFAULT_ONTOLOGY_IRI = "http://example.com/manufacturing_ontology.owl"
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(name)s - %(message)s'
SPEC_PARENT_CLASS_COLUMN = 'Parent Class'  # Assumed column name for hierarchy

# --- Warnings Suppression Configuration ---
# List of warning message substrings that should be suppressed in logs
SUPPRESSED_WARNINGS = [
    "Equipment.actualSequencePosition is missing 'column'",
    "EquipmentClass.defaultSequencePosition is missing 'column'",
    "No equipment instance relationships were created or verified",
    "Context entity 'EquipmentCapability' required for Equipment.hasCapability",
    "Context entity 'EventRecord' required for Material.materialUsedIn",
    "Context entity 'EventRecord' required for OperationalReason.reasonForEvent",
    "Context entity 'EventRecord' required for OperationalState.stateOfEvent",
    "Context entity 'EventRecord' required for ProductionRequest.hasAssociatedEvent",
    "Context entity 'EventRecord' required for Shift.includesEvent",
    "Context entity 'Person' required for EventRecord.performedBy",
    "Created new individual"
]

# --- Language Mapping for Alternative Reason Descriptions ---
# Mapping from country descriptions to BCP 47 language tags
COUNTRY_TO_LANGUAGE: Dict[str, str] = {
    "Mexico": "es",
    "United States": "en",
    "Brazil": "pt",
    "France": "fr",
    "Germany": "de",
    "Italy": "it",
    "Spain": "es",
    "Japan": "ja",
    "China": "zh",
    # Add more mappings as needed based on your data
}
DEFAULT_LANGUAGE = "en"  # Default language if country not found in mapping

# --- Default Equipment Class Sequencing ---
# Defines a default linear sequence for common equipment types
DEFAULT_EQUIPMENT_SEQUENCE: Dict[str, int] = {
    "Filler": 1,
    "Cartoner": 2,
    "Bundler": 3,
    "CaseFormer": 4,
    "CasePacker": 5,
    "CaseSealer": 6,
    "Palletizer": 7,
    # Add any other classes with default positions if needed
}

# --- XSD Type Mapping ---
# This will be initialized when importing the required modules to avoid
# circular imports with owlready2 types
XSD_TYPE_MAP: Dict[str, Type] = {}

def init_xsd_type_map(locstr_type: Any) -> None:
    """
    Initialize the XSD type mapping with the owlready2 locstr type.
    This should be called after owlready2 is imported.
    
    Args:
        locstr_type: The owlready2 locstr type
    """
    global XSD_TYPE_MAP
    
    XSD_TYPE_MAP.update({
        "xsd:string": str,
        "xsd:decimal": float,
        "xsd:double": float,
        "xsd:float": float,
        "xsd:integer": int,
        "xsd:int": int,
        "xsd:long": int,
        "xsd:short": int,
        "xsd:byte": int,
        "xsd:nonNegativeInteger": int,
        "xsd:positiveInteger": int,
        "xsd:negativeInteger": int,
        "xsd:nonPositiveInteger": int,
        "xsd:unsignedLong": int,
        "xsd:unsignedInt": int,
        "xsd:unsignedShort": int,
        "xsd:unsignedByte": int,
        "xsd:dateTime": datetime,
        "xsd:date": date,
        "xsd:time": time,
        "xsd:boolean": bool,
        "xsd:anyURI": str,
        "xsd:string (with lang tag)": locstr_type,
    })


===========================================
FILE: ontology_generator/definition/__init__.py
===========================================

from .parser import (
    parse_specification, parse_property_mappings, validate_property_mappings,
    read_data
)
from .structure import define_ontology_structure, create_selective_classes


===========================================
FILE: ontology_generator/definition/ontology_comp
===========================================



===========================================
FILE: ontology_generator/definition/parser.py
===========================================

"""
Specification parsing module for the ontology generator.

This module provides functions for parsing the ontology specification file.
"""
import csv
from collections import defaultdict
from typing import List, Dict, Any, Optional

from ontology_generator.utils.logging import logger

def parse_specification(spec_file_path: str) -> List[Dict[str, str]]:
    """
    Parses the ontology specification CSV file.
    
    Args:
        spec_file_path: Path to the specification CSV file
        
    Returns:
        A list of dictionaries representing the specification rows
    """
    logger.info(f"Parsing specification file: {spec_file_path}")
    spec_list: List[Dict[str, str]] = []
    try:
        with open(spec_file_path, mode='r', encoding='utf-8-sig') as infile:  # Use utf-8-sig to handle potential BOM
            reader = csv.DictReader(infile)
            # Basic check for expected columns (optional but recommended)
            # expected_cols = {'Proposed OWL Entity', 'Proposed OWL Property', 'Parent Class', ...}
            # if not expected_cols.issubset(reader.fieldnames):
            #     logger.warning(f"Specification file might be missing expected columns. Found: {reader.fieldnames}")
            spec_list = list(reader)
            logger.info(f"Successfully parsed {len(spec_list)} rows from specification.")
            return spec_list
    except FileNotFoundError:
        logger.error(f"Specification file not found: {spec_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error parsing specification file {spec_file_path}: {e}")
        raise
    return []  # Return empty list on error if not raising

def parse_property_mappings(specification: List[Dict[str, str]]) -> Dict[str, Dict[str, Dict[str, Any]]]:
    """
    Parses the ontology specification to extract property-to-column mappings.
    
    Args:
        specification: The parsed specification list of dictionaries
        
    Returns:
        A nested dictionary with the structure:
        {
            'EntityName': {
                'data_properties': {
                    'propertyName': {
                        'column': 'RAW_DATA_COLUMN',
                        'data_type': 'xsd:type',
                        'functional': True/False
                    }
                },
                'object_properties': {
                    'propertyName': {
                        'column': 'RAW_DATA_COLUMN',
                        'target_class': 'TargetClassName',
                        'functional': True/False
                    }
                }
            }
        }
    """
    logger.info("Parsing property mappings from specification")
    mappings = defaultdict(lambda: {'data_properties': {}, 'object_properties': {}})
    
    # Get fieldnames to check for the new optional column
    fieldnames = []
    try:
        with open(specification[0]['_source_file_path_'], mode='r', encoding='utf-8-sig') as infile: # Assuming spec is not empty and comes from a file
             reader = csv.DictReader(infile)
             fieldnames = reader.fieldnames or []
    except Exception:
        # Fallback: Check the first row keys if reading file fails or spec is not from file
        if specification:
             fieldnames = list(specification[0].keys())
             
    has_target_link_context_col = 'Target Link Context' in fieldnames
    if not has_target_link_context_col:
        logger.warning("Specification file does not contain the 'Target Link Context' column. Context-based object property links may not be parsed.")

    for row_num, row in enumerate(specification):
        entity = row.get('Proposed OWL Entity', '').strip()
        property_name = row.get('Proposed OWL Property', '').strip()
        property_type = row.get('OWL Property Type', '').strip()
        raw_data_col = row.get('Raw Data Column Name', '').strip()
        
        # Add source file path if not already present (useful for validation/debugging)
        if '_source_file_path_' not in row and hasattr(specification, '_source_file_path_'): 
             row['_source_file_path_'] = specification._source_file_path_ # Propagate if available
             
        # Skip if any essential fields are missing
        if not entity or not property_name:
            continue
            
        # Skip if property type is missing or invalid
        if property_type not in ['DatatypeProperty', 'ObjectProperty']:
            logger.warning(f"Skipping row {row_num+1}: Invalid or missing OWL Property Type '{property_type}' for {entity}.{property_name}")
            continue
            
        # Raw data col is optional for object properties if target link context is provided
        raw_data_col_is_na = not raw_data_col or raw_data_col.upper() == 'N/A'
        
        # Determine if the property is functional
        is_functional = 'Functional' in row.get('OWL Property Characteristics', '')
        
        # Process data properties
        if property_type == 'DatatypeProperty':
            data_type = row.get('Target/Range (xsd:) / Target Class', '').strip()
            # Create mapping info dictionary
            map_info = {
                'data_type': data_type,
                'functional': is_functional
            }
            # Conditionally add the 'column' key
            if not raw_data_col_is_na:
                map_info['column'] = raw_data_col
                logger.debug(f"Mapped {entity}.{property_name} (DatatypeProperty) to column '{raw_data_col}', type '{data_type}'")
            else:
                # Log definition without mapping
                logger.debug(f"Defined {entity}.{property_name} (DatatypeProperty) type '{data_type}' but no data column mapping.")
            
            # Add to mappings regardless of column presence
            mappings[entity]['data_properties'][property_name] = map_info
            
        # Process object properties
        elif property_type == 'ObjectProperty':
            target_class = row.get('Target/Range (xsd:) / Target Class', '').strip()
            target_link_context = row.get('Target Link Context', '').strip() if has_target_link_context_col else ''
            
            # Initialize mapping info
            map_info = {
                'target_class': target_class,
                'functional': is_functional
            }

            # Check if there's a way to populate/link this property later
            can_populate = False
            if not raw_data_col_is_na:
                # Prefer column mapping if available
                map_info['column'] = raw_data_col
                can_populate = True
                logger.debug(f"Mapped {entity}.{property_name} (ObjectProperty) to column '{raw_data_col}', target '{target_class}'")
                # Warn if context is also provided but will be ignored
                if target_link_context:
                    logger.warning(f"Row {row_num+1}: Both 'Raw Data Column Name' ('{raw_data_col}') and 'Target Link Context' ('{target_link_context}') provided for {entity}.{property_name}. Prioritizing column lookup.")
            elif target_link_context:
                # Use context mapping if column is not available
                map_info['target_link_context'] = target_link_context
                can_populate = True
                logger.debug(f"Mapped {entity}.{property_name} (ObjectProperty) via context '{target_link_context}', target '{target_class}'")
            else:
                 # Defined but cannot be populated from data/context
                 logger.debug(f"Defined {entity}.{property_name} (ObjectProperty) target '{target_class}' but no column or context for mapping.")

            # Add to mappings regardless of populatability, including mapping info if available
            mappings[entity]['object_properties'][property_name] = map_info
    
    # Convert defaultdict to regular dict for return
    return {k: {'data_properties': dict(v['data_properties']), 
                'object_properties': dict(v['object_properties'])} 
            for k, v in mappings.items()}

def validate_property_mappings(property_mappings: Dict[str, Dict[str, Dict[str, Any]]]) -> bool:
    """
    Validates property mappings and logs information for debugging.
    
    Args:
        property_mappings: Property mapping dictionary from parse_property_mappings
        
    Returns:
        bool: True if validation passed, False otherwise
    """
    logger.info("Validating property mappings...")
    
    if not property_mappings:
        logger.error("Property mappings dictionary is empty!")
        return False
    
    validation_passed = True
    entity_count = 0
    data_prop_count = 0
    object_prop_count = 0
    
    # Log summary
    logger.info(f"Found mappings for {len(property_mappings)} entities")
    
    # Check each entity
    for entity_name, entity_props in sorted(property_mappings.items()):
        entity_count += 1
        data_properties = entity_props.get('data_properties', {})
        object_properties = entity_props.get('object_properties', {})
        
        # Count properties
        data_prop_count += len(data_properties)
        object_prop_count += len(object_properties)
        
        # Log entity details
        logger.info(f"Entity: {entity_name} - {len(data_properties)} data properties, {len(object_properties)} object properties")
        
        # Log data properties
        if data_properties:
            logger.debug(f"  Data Properties for {entity_name}:")
            for prop_name, details in sorted(data_properties.items()):
                column = details.get('column', 'MISSING_COLUMN')
                data_type = details.get('data_type', 'MISSING_TYPE')
                functional = details.get('functional', False)
                
                logger.debug(f"    {prop_name}: column='{column}', type='{data_type}', functional={functional}")
                
                # Validate required fields
                if not column or not data_type:
                    logger.warning(f"Missing required field for {entity_name}.{prop_name}: column='{column}', type='{data_type}'")
                    validation_passed = False
        
        # Log object properties
        if object_properties:
            logger.debug(f"  Object Properties for {entity_name}:")
            for prop_name, details in sorted(object_properties.items()):
                column = details.get('column', None) # Changed default to None
                target = details.get('target_class', 'MISSING_TARGET')
                functional = details.get('functional', False)
                link_context = details.get('target_link_context', None) # Added context check

                log_msg = f"    {prop_name}: target='{target}', functional={functional}"
                if column:
                     log_msg += f", column='{column}'"
                if link_context:
                     log_msg += f", context='{link_context}'"
                logger.debug(log_msg)
                
                # Validate required fields
                if not target:
                    logger.warning(f"Missing required field target_class for {entity_name}.{prop_name}")
                    validation_passed = False
                # Must have either column or context
                if not column and not link_context:
                    logger.warning(f"Missing required field: Needs 'column' or 'target_link_context' for {entity_name}.{prop_name}")
                    validation_passed = False
    
    # Check for EventRecord specifically
    if 'EventRecord' not in property_mappings:
        logger.warning("No mappings found for 'EventRecord' entity (the main focus of this change)")
        validation_passed = False
    else:
        # Check for common EventRecord properties
        event_props = property_mappings['EventRecord'].get('data_properties', {})
        expected_props = ['downtimeMinutes', 'runTimeMinutes', 'reportedDurationMinutes']
        missing_props = [p for p in expected_props if p not in event_props]
        
        if missing_props:
            logger.warning(f"Some expected EventRecord properties are missing from mappings: {missing_props}")
            # Don't fail validation for this, but log the warning
    
    # Log summary stats
    logger.info(f"Property mapping validation complete. Found {entity_count} entities, {data_prop_count} data properties, {object_prop_count} object properties.")
    logger.info(f"Validation {'PASSED' if validation_passed else 'FAILED'}")
    
    return validation_passed

def read_data(data_file_path: str) -> List[Dict[str, str]]:
    """
    Reads the operational data CSV file.
    
    Args:
        data_file_path: Path to the data CSV file
        
    Returns:
        A list of dictionaries representing the data rows
    """
    logger.info(f"Reading data file: {data_file_path}")
    data_rows: List[Dict[str, str]] = []
    try:
        with open(data_file_path, mode='r', encoding='utf-8-sig') as infile:
            reader = csv.DictReader(infile)
            data_rows = list(reader)
            logger.info(f"Successfully read {len(data_rows)} data rows.")
            return data_rows
    except FileNotFoundError:
        logger.error(f"Data file not found: {data_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading data file {data_file_path}: {e}")
        raise
    return []  # Return empty list on error if not raising


===========================================
FILE: ontology_generator/definition/structure.py
===========================================

"""
Ontology structure definition module for the ontology generator.

This module provides functions for defining the ontology structure.
"""
import re
import types
from typing import Dict, List, Tuple, Set, Optional, Any

from owlready2 import (
    Ontology, Thing, Nothing, ThingClass, PropertyClass,
    FunctionalProperty, InverseFunctionalProperty, TransitiveProperty,
    SymmetricProperty, AsymmetricProperty, ReflexiveProperty, IrreflexiveProperty,
    ObjectProperty, DataProperty
)

from ontology_generator.utils.logging import logger
from ontology_generator.config import SPEC_PARENT_CLASS_COLUMN, XSD_TYPE_MAP

def define_ontology_structure(onto: Ontology, specification: List[Dict[str, str]]) -> Tuple[Dict[str, ThingClass], Dict[str, PropertyClass], Dict[str, bool]]:
    """
    Defines OWL classes and properties based on the parsed specification.

    Args:
        onto: The ontology to define the structure in
        specification: The parsed specification
        
    Returns:
        tuple: (defined_classes, defined_properties, property_is_functional)
            - defined_classes: Dict mapping class name to owlready2 class object.
            - defined_properties: Dict mapping property name to owlready2 property object.
            - property_is_functional: Dict mapping property name to boolean indicating functionality.
    """
    logger.info(f"Defining ontology structure in: {onto.base_iri}")
    defined_classes: Dict[str, ThingClass] = {}
    defined_properties: Dict[str, PropertyClass] = {}
    property_is_functional: Dict[str, bool] = {}  # Track which properties are functional based on spec
    class_metadata: Dict[str, Dict[str, Any]] = {} # Store metadata like notes per class

    # --- Pre-process Spec for Class Metadata and Hierarchy ---
    logger.debug("--- Pre-processing specification for class details ---")
    all_class_names: Set[str] = set()
    class_parents: Dict[str, str] = {} # {child_name: parent_name}
    for i, row in enumerate(specification):
        class_name = row.get('Proposed OWL Entity', '').strip()
        if class_name:
            all_class_names.add(class_name)
            # Store metadata (using first encountered row for simplicity, could collect all)
            if class_name not in class_metadata:
                    class_metadata[class_name] = {
                        'notes': row.get('Notes/Considerations', ''),
                        'isa95': row.get('ISA-95 Concept', ''),
                        'row_index': i # For reference if needed
                    }
            # Store parent class info if column exists
            parent_name = row.get(SPEC_PARENT_CLASS_COLUMN, '').strip()
            if parent_name and parent_name != class_name: # Avoid self-parenting
                class_parents[class_name] = parent_name
                all_class_names.add(parent_name) # Ensure parent is also considered a class


    # --- Pass 1: Define Classes with Hierarchy ---
    logger.debug("--- Defining Classes ---")
    with onto:
        # Ensure Thing is available if not explicitly listed
        if "Thing" not in all_class_names and "owl:Thing" not in all_class_names:
            pass # Thing is implicitly available via owlready2

        defined_order: List[str] = [] # Track definition order for hierarchy
        definition_attempts = 0
        max_attempts = len(all_class_names) + 5 # Allow some leeway for complex hierarchies

        classes_to_define: Set[str] = set(cn for cn in all_class_names if cn.lower() != "owl:thing") # Exclude Thing variants

        while classes_to_define and definition_attempts < max_attempts:
            defined_in_pass: Set[str] = set()
            for class_name in sorted(list(classes_to_define)): # Sort for somewhat deterministic order
                parent_name = class_parents.get(class_name)
                parent_class_obj: ThingClass = Thing # Default parent is Thing

                if parent_name:
                    if parent_name == "Thing" or parent_name.lower() == "owl:thing": # Handle case variation
                        parent_class_obj = Thing
                    elif parent_name in defined_classes:
                        parent_class_obj = defined_classes[parent_name]
                    else:
                        # Parent not defined yet, skip this class for now
                        logger.debug(f"Deferring class '{class_name}', parent '{parent_name}' not defined yet.")
                        continue

                # Define the class
                try:
                    if class_name not in defined_classes:
                        logger.debug(f"Attempting to define Class: {class_name} with Parent: {parent_class_obj.name}")
                        # Ensure class name is valid Python identifier if needed by backend
                        safe_class_name = re.sub(r'\W|^(?=\d)', '_', class_name)
                        if safe_class_name != class_name:
                            logger.warning(f"Class name '{class_name}' sanitized to '{safe_class_name}' for internal use. Using original name for IRI.")
                            # Sticking with original name as owlready2 often handles non-standard chars in IRIs

                        # Revert to types.new_class
                        new_class: ThingClass = types.new_class(class_name, (parent_class_obj,))

                        defined_classes[class_name] = new_class
                        defined_order.append(class_name)
                        defined_in_pass.add(class_name)
                        logger.debug(f"Defined Class: {new_class.iri} (Parent: {parent_class_obj.iri})") # Removed the extra type check log

                        # Add annotations like comments/labels from pre-processed metadata
                        meta = class_metadata.get(class_name)
                        if meta:
                            comments = []
                            if meta['notes']: comments.append(f"Notes: {meta['notes']}")
                            if meta['isa95']: comments.append(f"ISA-95 Concept: {meta['isa95']}")
                            if comments:
                                new_class.comment = comments
                                logger.debug(f"Added comments to class {class_name}")

                except Exception as e:
                    logger.error(f"Error defining class '{class_name}' with parent '{getattr(parent_class_obj,'name','N/A')}': {e}")
                    # Let it retry, might be a transient issue or solvable in later pass

            classes_to_define -= defined_in_pass
            definition_attempts += 1
            if not defined_in_pass and classes_to_define:
                logger.error(f"Could not define remaining classes (possible circular dependency or missing parents): {classes_to_define}")
                break # Avoid infinite loop

        if classes_to_define:
            logger.warning(f"Failed to define the following classes: {classes_to_define}")

    # --- Pass 2: Define Properties ---
    logger.debug("--- Defining Properties ---")
    properties_to_process = [row for row in specification if row.get('Proposed OWL Property')]
    temp_inverse_map: Dict[str, str] = {} # Stores {prop_name: inverse_name}

    with onto:
        # Define properties first without inverse, handle inverse in a second pass
        for row in properties_to_process:
            prop_name = row.get('Proposed OWL Property','').strip()
            if not prop_name or prop_name in defined_properties:
                continue # Skip empty or already defined properties

            prop_type_str = row.get('OWL Property Type', '').strip()
            domain_str = row.get('Domain', '').strip()
            range_str = row.get('Target/Range (xsd:) / Target Class', '').strip()
            characteristics_str = row.get('OWL Property Characteristics', '').strip().lower() # Normalize
            inverse_prop_name = row.get('Inverse Property', '').strip()

            if not prop_type_str or not domain_str or not range_str:
                logger.warning(f"Skipping property '{prop_name}' due to missing type, domain, or range in spec.")
                continue

            # Determine parent classes for the property
            parent_classes: List[type] = []
            base_prop_type: Optional[type] = None
            if prop_type_str == 'ObjectProperty':
                base_prop_type = ObjectProperty
            elif prop_type_str == 'DatatypeProperty':
                base_prop_type = DataProperty
            else:
                logger.warning(f"Unknown property type '{prop_type_str}' for property '{prop_name}'. Skipping.")
                continue

            parent_classes.append(base_prop_type)

            # Add characteristics
            is_functional = 'functional' in characteristics_str
            property_is_functional[prop_name] = is_functional # Track functionality status
            if is_functional: parent_classes.append(FunctionalProperty)
            if 'inversefunctional' in characteristics_str: parent_classes.append(InverseFunctionalProperty)
            if 'transitive' in characteristics_str: parent_classes.append(TransitiveProperty)
            if 'symmetric' in characteristics_str: parent_classes.append(SymmetricProperty)
            if 'asymmetric' in characteristics_str: parent_classes.append(AsymmetricProperty)
            if 'reflexive' in characteristics_str: parent_classes.append(ReflexiveProperty)
            if 'irreflexive' in characteristics_str: parent_classes.append(IrreflexiveProperty)

            try:
                # Define the property
                new_prop: PropertyClass = types.new_class(prop_name, tuple(parent_classes))

                # Set Domain
                domain_class_names = [dc.strip() for dc in domain_str.split('|')]
                prop_domain: List[ThingClass] = []
                valid_domain_found = False
                for dc_name in domain_class_names:
                    domain_class = defined_classes.get(dc_name)
                    if domain_class:
                        prop_domain.append(domain_class)
                        valid_domain_found = True
                    elif dc_name == "Thing" or dc_name.lower() == "owl:thing": # Allow Thing as domain
                        prop_domain.append(Thing)
                        valid_domain_found = True
                    else:
                        logger.warning(f"Domain class '{dc_name}' not found for property '{prop_name}'.")

                if prop_domain:
                    new_prop.domain = prop_domain # Assign list directly for union domain
                    logger.debug(f"Set domain for {prop_name} to {[dc.name for dc in prop_domain]}")
                elif not valid_domain_found:
                    logger.warning(f"No valid domain classes found for property '{prop_name}'. Skipping domain assignment.")

                # Set Range
                if base_prop_type is ObjectProperty:
                    range_class_names = [rc.strip() for rc in range_str.split('|')]
                    prop_range: List[ThingClass] = []
                    valid_range_found = False
                    for rc_name in range_class_names:
                        range_class = defined_classes.get(rc_name)
                        if range_class:
                            prop_range.append(range_class)
                            valid_range_found = True
                        elif rc_name == "Thing" or rc_name.lower() == "owl:thing": # Allow Thing as range
                            prop_range.append(Thing)
                            valid_range_found = True
                        else:
                            logger.warning(f"Range class '{rc_name}' not found for object property '{prop_name}'.")
                    if prop_range:
                        new_prop.range = prop_range # Assign list directly for union range
                        logger.debug(f"Set range for {prop_name} to {[rc.name for rc in prop_range]}")
                    elif not valid_range_found:
                        logger.warning(f"Could not set any valid range for object property '{prop_name}'.")

                elif base_prop_type is DataProperty:
                    target_type = XSD_TYPE_MAP.get(range_str)
                    if target_type:
                        new_prop.range = [target_type]
                        logger.debug(f"Set range for {prop_name} to {target_type.__name__ if hasattr(target_type, '__name__') else target_type}")
                    else:
                        logger.warning(f"Unknown XSD type '{range_str}' for property '{prop_name}'. Skipping range assignment.")

                # Add annotations
                notes = row.get('Notes/Considerations', '')
                isa95 = row.get('ISA-95 Concept', '')
                comments = []
                if notes: comments.append(f"Notes: {notes}")
                if isa95: comments.append(f"ISA-95 Concept: {isa95}")
                if comments:
                    new_prop.comment = comments

                defined_properties[prop_name] = new_prop
                logger.debug(f"Defined Property: {new_prop.iri} of type {prop_type_str} with characteristics {' '.join([p.__name__ for p in parent_classes[1:]]) if len(parent_classes) > 1 else 'None'}")

                # Store inverse relationship for later processing
                if inverse_prop_name and base_prop_type is ObjectProperty:
                    temp_inverse_map[prop_name] = inverse_prop_name

            except Exception as e:
                logger.error(f"Error defining property '{prop_name}': {e}")

    # --- Pass 3: Set Inverse Properties ---
    logger.debug("--- Setting Inverse Properties ---")
    with onto: # Ensure changes are applied within the ontology context
        for prop_name, inverse_name in temp_inverse_map.items():
            prop = defined_properties.get(prop_name)
            inverse_prop = defined_properties.get(inverse_name)

            if prop and inverse_prop:
                try:
                    # Check if already set to the desired value to avoid unnecessary writes/warnings if possible
                    current_inverse = getattr(prop, "inverse_property", None)
                    if current_inverse != inverse_prop:
                        prop.inverse_property = inverse_prop
                        logger.debug(f"Set inverse_property for {prop.name} to {inverse_prop.name}")
                    # Also explicitly set the inverse's inverse property back
                    current_inverse_of_inverse = getattr(inverse_prop, "inverse_property", None)
                    if current_inverse_of_inverse != prop:
                        inverse_prop.inverse_property = prop
                        logger.debug(f"Set inverse_property for {inverse_prop.name} back to {prop.name}")
                except Exception as e:
                    logger.error(f"Error setting inverse property between '{prop_name}' and '{inverse_name}': {e}")
            elif not prop:
                logger.warning(f"Property '{prop_name}' not found while trying to set inverse '{inverse_name}'.")
            elif not inverse_prop:
                logger.warning(f"Inverse property '{inverse_name}' not found for property '{prop_name}'.")

    logger.info("Ontology structure definition complete.")
    return defined_classes, defined_properties, property_is_functional

def create_selective_classes(onto: Ontology, 
                          specification: List[Dict[str, str]], 
                          skip_classes: List[str] = None,
                          strict_adherence: bool = False) -> Dict[str, ThingClass]:
    """
    Creates only the necessary classes from the specification, 
    optionally skipping specified classes or enforcing strict spec adherence.
    
    Args:
        onto: The ontology object
        specification: Parsed specification
        skip_classes: List of class names to skip (won't be created)
        strict_adherence: If True, only create classes explicitly defined in spec
        
    Returns:
        Dict mapping class name to class object
    """
    logger.info(f"Creating classes selectively from specification")
    
    skip_classes = set(skip_classes or [])
    defined_classes = {}
    
    # Pre-process spec to find essential classes
    spec_classes = set()
    spec_parents = {}
    property_domains = set()
    property_ranges = set()
    
    for row in specification:
        # Get class names
        class_name = row.get('Proposed OWL Entity', '').strip()
        if class_name:
            spec_classes.add(class_name)
            parent_name = row.get(SPEC_PARENT_CLASS_COLUMN, '').strip()
            if parent_name and parent_name != class_name:
                spec_parents[class_name] = parent_name
                spec_classes.add(parent_name)  # Ensure parent is in spec classes
        
        # Get property domains and ranges
        prop_name = row.get('Proposed OWL Property', '').strip()
        if prop_name:
            # Get domains
            domain_str = row.get('Domain', '').strip()
            if domain_str:
                domains = [d.strip() for d in domain_str.split('|')]
                property_domains.update(domains)
            
            # Get ranges for object properties
            prop_type = row.get('OWL Property Type', '').strip()
            if prop_type == 'ObjectProperty':
                range_str = row.get('Target/Range (xsd:) / Target Class', '').strip()
                if range_str:
                    ranges = [r.strip() for r in range_str.split('|')]
                    property_ranges.update(ranges)
    
    # Determine which classes to create
    classes_to_create = set()
    
    if strict_adherence:
        # Only create classes explicitly defined in spec
        classes_to_create = spec_classes
    else:
        # Create spec classes plus any referenced in properties
        classes_to_create = spec_classes | property_domains | property_ranges
    
    # Remove classes to skip
    classes_to_create -= skip_classes
    
    # Create classes with proper hierarchy
    with onto:
        # First pass: create all classes as direct subclasses of Thing
        for class_name in classes_to_create:
            if class_name == "Thing" or class_name.lower() == "owl:thing":
                continue  # Skip Thing
            
            try:
                # Create as subclass of Thing initially
                new_class = types.new_class(class_name, (Thing,))
                defined_classes[class_name] = new_class
                logger.debug(f"Created class {class_name} (temp parent: Thing)")
            except Exception as e:
                logger.error(f"Error creating class {class_name}: {e}")
        
        # Second pass: set proper parent classes
        for class_name, class_obj in defined_classes.items():
            parent_name = spec_parents.get(class_name)
            if parent_name and parent_name in defined_classes:
                parent_class = defined_classes[parent_name]
                # Reset parent
                class_obj.is_a = [parent_class]
                logger.debug(f"Set parent of {class_name} to {parent_name}")
    
    classes_skipped = spec_classes - set(defined_classes.keys())
    if classes_skipped:
        logger.info(f"Skipped {len(classes_skipped)} classes: {', '.join(sorted(classes_skipped))}")
    
    logger.info(f"Selectively created {len(defined_classes)} classes from specification")
    return defined_classes


===========================================
FILE: ontology_generator/main.py
===========================================

"""
Main module for the ontology generator.

This module provides the main entry point for the ontology generator.
"""
import argparse
import csv
import logging
import os
import sys
import time as timing
from datetime import datetime, date, time
from typing import List, Dict, Any, Optional, Tuple

from owlready2 import (
    World, Ontology, sync_reasoner, Thing,
    OwlReadyInconsistentOntologyError, locstr, default_world,
    ThingClass, FunctionalProperty, InverseFunctionalProperty, TransitiveProperty, SymmetricProperty, AsymmetricProperty, ReflexiveProperty, IrreflexiveProperty, Nothing
)

from ontology_generator.config import DEFAULT_ONTOLOGY_IRI, init_xsd_type_map, DEFAULT_EQUIPMENT_SEQUENCE
from ontology_generator.utils.logging import (
    main_logger, configure_logging, analysis_logger
)
from ontology_generator.definition import (
    parse_specification, define_ontology_structure, create_selective_classes,
    parse_property_mappings, validate_property_mappings, read_data
)
from ontology_generator.population import (
    setup_equipment_sequence_relationships, # Need these post-population steps
    setup_equipment_instance_relationships,
    link_equipment_events_to_line_events
)
from ontology_generator.analysis import (
    analyze_ontology_population, generate_population_report,
    generate_optimization_recommendations, generate_reasoning_report,
    generate_equipment_sequence_report, analyze_equipment_sequences
)
from ontology_generator.utils import safe_cast # Import directly from utils now

# Initialize XSD type map and datetime types
init_xsd_type_map(locstr)

def populate_ontology_from_data(onto: Ontology,
                                data_rows: List[Dict[str, Any]],
                                defined_classes: Dict[str, object],
                                defined_properties: Dict[str, object],
                                property_is_functional: Dict[str, bool],
                                specification: List[Dict[str, str]],
                                property_mappings: Dict[str, Dict[str, Dict[str, Any]]] = None
                              ) -> Tuple[int, Dict[str, object], Dict[str, int], List[Tuple[object, object, object, object]]]:
    """
    Populates the ontology with individuals and relations from data rows using a two-pass approach.
    Pass 1: Creates individuals and sets data properties.
    Pass 2: Creates object property relationships between individuals.

    Args:
        onto: The ontology to populate
        data_rows: The data rows from the data CSV file
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        property_is_functional: Dictionary indicating functionality of properties
        specification: The parsed specification
        property_mappings: Optional property mappings dictionary
        
    Returns:
        tuple: (failed_rows_count, created_equipment_class_inds, equipment_class_positions, created_events_context)
    """
    # Ensure imports required *within* this function are present
    from ontology_generator.population.core import PopulationContext
    from ontology_generator.population.row_processor import process_single_data_row_pass1, process_single_data_row_pass2 # Import needed here

    main_logger.info(f"Starting ontology population with {len(data_rows)} data rows (Two-Pass Strategy).")

    # Create population context
    context = PopulationContext(onto, defined_classes, defined_properties, property_is_functional)

    # --- Pre-checks (Essential Classes and Properties) ---
    essential_classes_names = [
        "Plant", "Area", "ProcessCell", "ProductionLine", "Equipment",
        "EquipmentClass", "Material", "ProductionRequest", "EventRecord",
        "TimeInterval", "Shift", "OperationalState", "OperationalReason"
    ]
    missing_classes = [name for name in essential_classes_names if not context.get_class(name)]
    if missing_classes:
        # get_class already logged errors, just return failure
        main_logger.error(f"Cannot proceed. Missing essential classes definitions: {missing_classes}")
        return len(data_rows), {}, {}, []  # Empty context

    essential_prop_names = { # Focus on IDs and core structure for initial checks
        "plantId", "areaId", "processCellId", "lineId", "equipmentId", "equipmentName",
        "equipmentClassId", "materialId", "requestId", "shiftId", "startTime", "endTime"
        # Object properties checked implicitly later
    }
    missing_essential_props = [name for name in essential_prop_names if not context.get_prop(name)]
    if missing_essential_props:
        main_logger.error(f"Cannot reliably proceed. Missing essential data properties definitions: {missing_essential_props}")
        return len(data_rows), {}, {}, []  # Empty context

    # Warn about other missing properties defined in spec but not found
    all_spec_prop_names = {row.get('Proposed OWL Property','').strip() for row in specification if row.get('Proposed OWL Property')}
    for spec_prop in all_spec_prop_names:
        if spec_prop and not context.get_prop(spec_prop):
            main_logger.warning(f"Property '{spec_prop}' (from spec) not found in defined_properties. Population using this property will be skipped.")

    # --- Pass 1: Create Individuals and Apply Data Properties ---
    main_logger.info("--- Population Pass 1: Creating Individuals and Data Properties ---")
    all_created_individuals_by_uid = {} # {(entity_type, unique_id): individual_obj}
    individuals_by_row = {} # {row_index: {entity_type: individual_obj, ...}}
    created_equipment_class_inds = {}  # {eq_class_name_str: eq_class_ind_obj}
    equipment_class_positions = {}  # {eq_class_name_str: position_int}
    created_events_context = []  # List to store tuples for later linking: (event_ind, resource_ind, time_interval_ind, line_ind_associated)
    pass1_successful_rows = 0
    pass1_failed_rows = 0

    with onto:  # Use the ontology context for creating individuals
        for i, row in enumerate(data_rows):
            row_num = i + 2  # 1-based index + header row = line number in CSV

            # Call the dedicated row processing function for Pass 1
            success, created_inds_in_row, event_context, eq_class_info = process_single_data_row_pass1(
                row, row_num, context, property_mappings, all_created_individuals_by_uid # Pass registry for get_or_create logic
            )

            if success:
                pass1_successful_rows += 1
                individuals_by_row[i] = created_inds_in_row # Store individuals created from this row
                # Update the global registry (used by get_or_create and Pass 2 context)
                # Note: process_single_data_row_pass1 should already populate all_created_individuals_by_uid via get_or_create calls
                
                # Store event context if returned
                if event_context:
                    created_events_context.append(event_context)

                # Process equipment class info if returned
                if eq_class_info:
                    eq_class_name, eq_class_ind, eq_class_pos = eq_class_info
                    if eq_class_name not in created_equipment_class_inds:
                        created_equipment_class_inds[eq_class_name] = eq_class_ind
                    # Update position map if a position is defined and potentially different
                    if eq_class_pos is not None:
                        if eq_class_name in equipment_class_positions and equipment_class_positions[eq_class_name] != eq_class_pos:
                             main_logger.warning(f"Sequence position conflict for class '{eq_class_name}' during population. Existing: {equipment_class_positions[eq_class_name]}, New: {eq_class_pos}. Using new value: {eq_class_pos}")
                        equipment_class_positions[eq_class_name] = eq_class_pos
                        # main_logger.debug(f"Tracked position {eq_class_pos} for class '{eq_class_name}'.") # Can be noisy

            else:
                pass1_failed_rows += 1
                individuals_by_row[i] = {} # Ensure entry exists even if row failed
                # Error logging handled within process_single_data_row_pass1

    main_logger.info(f"Pass 1 Complete. Successful rows: {pass1_successful_rows}, Failed rows: {pass1_failed_rows}.")
    main_logger.info(f"Total unique individuals created (approx): {len(all_created_individuals_by_uid)}")

    # Log Equipment Class Summary (collected during pass 1)
    main_logger.info("--- Unique Equipment Classes Found/Created (Pass 1) ---")
    if created_equipment_class_inds:
        sorted_class_names = sorted(created_equipment_class_inds.keys())
        main_logger.info(f"Total unique equipment classes: {len(sorted_class_names)}")
        for class_name in sorted_class_names:
            main_logger.info(f"  • {class_name} (Position: {equipment_class_positions.get(class_name, 'Not Set')})")
        
        # Log information about default sequence positions from config
        defaults_used = [name for name in sorted_class_names if name in DEFAULT_EQUIPMENT_SEQUENCE]
        if defaults_used:
            main_logger.info(f"Using default sequence positions from config for {len(defaults_used)} equipment classes: {', '.join(defaults_used)}")
    else:
        main_logger.warning("No EquipmentClass individuals were created or tracked during population!")

    # --- Pass 2: Apply Object Property Mappings ---
    main_logger.info("--- Population Pass 2: Linking Individuals (Object Properties) ---")
    pass2_successful_rows = 0
    pass2_failed_rows = 0
    # Prepare the full context dictionary for linking (use the values from the UID map)
    full_context_individuals = {k[0]: v for k, v in all_created_individuals_by_uid.items()} # Simple context {type_name: ind_obj} - May need refinement based on linking needs
    main_logger.info(f"Prepared context for Pass 2 with {len(full_context_individuals)} potential link targets.")


    # Refine context based on actual needs - Needs careful thought on how apply_object_property_mappings uses context_individuals
    # The warning "Context entity 'Equipment' required for Equipment.isParallelWith not found in provided context_individuals dictionary"
    # suggests the key should be the *type* ('Equipment') and the value the *target* individual.
    # However, apply_property_mappings seems to look up `context_individuals[link_context_entity]`.
    # For linking Equipment to Equipment via 'isParallelWith', link_context_entity would be 'Equipment'.
    # The current `apply_property_mappings` expects ONE individual for that key. This is flawed for many-to-many or one-to-many via context.

    # Let's assume apply_object_property_mappings will handle lookup within all_created_individuals_by_uid.
    # We pass the full registry instead of a simplified context.
    linking_context = all_created_individuals_by_uid

    with onto: # Context manager might not be strictly needed here if only setting properties
        for i, row in enumerate(data_rows):
            row_num = i + 2
            # Skip rows that failed significantly in Pass 1 (e.g., couldn't create core individuals)
            if i not in individuals_by_row or not individuals_by_row[i]:
                 main_logger.debug(f"Skipping Pass 2 linking for row {row_num} as no individuals were successfully created in Pass 1.")
                 pass2_failed_rows += 1 # Count as failed for Pass 2
                 continue

            created_inds_this_row = individuals_by_row[i]

            # Call the dedicated row processing function for Pass 2
            success = process_single_data_row_pass2(
                row, row_num, context, property_mappings, created_inds_this_row, linking_context
            )

            if success:
                pass2_successful_rows += 1
            else:
                pass2_failed_rows += 1
                # Logging handled within process_single_data_row_pass2

    main_logger.info(f"Pass 2 Complete. Rows successfully linked: {pass2_successful_rows}, Rows failed/skipped linking: {pass2_failed_rows}.")

    # Determine overall failed count (consider a row failed if it failed either pass?)
    # For now, return Pass 1 failure count as it indicates primary data issues.
    # The warnings originally reported were link failures (Pass 2 type issues).
    final_failed_rows = pass1_failed_rows # Or potentially max(pass1_failed_rows, pass2_failed_rows) or other logic

    main_logger.info(f"Ontology population complete. Final failed row count (based on Pass 1): {final_failed_rows}.")
    # Return collected contexts from Pass 1
    return final_failed_rows, created_equipment_class_inds, equipment_class_positions, created_events_context


def _log_initial_parameters(args, logger):
    logger.info("--- Starting Ontology Generation ---")
    logger.info(f"Specification file: {args.spec_file}")
    logger.info(f"Data file: {args.data_file}")
    logger.info(f"Output OWL file: {args.output_file}")
    logger.info(f"Ontology IRI: {args.iri}")
    logger.info(f"Save format: {args.format}")
    logger.info(f"Run reasoner: {args.reasoner}")
    if args.worlddb:
        logger.info(f"Using persistent world DB: {args.worlddb}")
    logger.info(f"Reasoner report max entities: {args.max_report_entities}")
    logger.info(f"Reasoner report verbose: {args.full_report}")
    logger.info(f"Analyze population: {args.analyze_population}")
    logger.info(f"Strict adherence: {args.strict_adherence}")
    logger.info(f"Skip classes: {args.skip_classes}")
    logger.info(f"Optimize ontology: {args.optimize_ontology}")

def _parse_spec_and_mappings(spec_file_path, logger):
    logger.info(f"Parsing specification file: {spec_file_path}")
    specification = parse_specification(spec_file_path)
    if not specification:
        logger.error("Specification parsing failed or resulted in empty spec. Aborting.")
        return None, None # Indicate failure

    logger.info("Parsing property mappings from specification...")
    property_mappings = parse_property_mappings(specification)
    logger.info(f"Parsed property mappings for {len(property_mappings)} entities")

    logger.info("Validating property mappings...")
    validation_result = validate_property_mappings(property_mappings)
    if not validation_result:
        logger.warning("Property mapping validation had issues. Population may be incomplete.")
    else:
        logger.info("Property mapping validation passed.")
    return specification, property_mappings

def _setup_world_and_ontology(ontology_iri, world_db_path, logger):
    world = None
    onto = None
    if world_db_path:
        logger.info(f"Initializing persistent World at: {world_db_path}")
        db_dir = os.path.dirname(world_db_path)
        if db_dir and not os.path.exists(db_dir):
            try:
                os.makedirs(db_dir, exist_ok=True)
                logger.info(f"Created directory for world DB: {db_dir}")
            except OSError as e:
                 logger.error(f"Failed to create directory for world DB {db_dir}: {e}")
                 return None, None # Indicate failure
        try:
            world = World(filename=world_db_path)
            onto = world.get_ontology(ontology_iri).load()
            logger.info(f"Ontology object obtained from persistent world: {onto}")
        except Exception as db_err:
             logger.error(f"Failed to initialize or load from persistent world DB {world_db_path}: {db_err}", exc_info=True)
             return None, None # Indicate failure
    else:
        logger.info("Initializing in-memory World.")
        world = World()  # Create a fresh world
        onto = world.get_ontology(ontology_iri)
        logger.info(f"Ontology object created in memory: {onto}")
    return world, onto

def _define_tbox(onto, specification, strict_adherence, skip_classes, logger):
    logger.info("Defining ontology structure (TBox)...")
    if strict_adherence or skip_classes:
        logger.info("Using selective class creation based on config.")
        defined_classes = create_selective_classes(onto, specification,
                                                  skip_classes=skip_classes,
                                                  strict_adherence=strict_adherence)
        # Define properties separately when using selective classes
        _, defined_properties, property_is_functional = define_ontology_structure(onto, specification)
    else:
        defined_classes, defined_properties, property_is_functional = define_ontology_structure(onto, specification)

    if not defined_classes:
        logger.warning("Ontology structure definition resulted in no classes. Population might be empty.")
    logger.info("TBox definition complete.")
    return defined_classes, defined_properties, property_is_functional

def _read_operational_data(data_file_path, logger):
    logger.info(f"Reading operational data from: {data_file_path}")
    try:
        data_rows = read_data(data_file_path)
        logger.info(f"Read {len(data_rows)} data rows.")
        if not data_rows:
            logger.warning("No data rows read. Ontology population will be skipped.")
        return data_rows
    except Exception as read_err:
        logger.error(f"Failed to read data file {data_file_path}: {read_err}", exc_info=True)
        return None # Indicate failure

def _populate_abox(onto, data_rows, defined_classes, defined_properties, prop_is_functional, specification, property_mappings, logger):
    logger.info("Starting ontology population (ABox)...")
    population_successful = True
    failed_rows_count = 0
    created_eq_classes = {}
    eq_class_positions = {}
    created_events_context = []

    if not data_rows:
        logger.warning("Skipping population as no data rows were provided.")
        # Return success=True but with zero counts/empty contexts
        return True, 0, {}, {}, []

    try:
        failed_rows_count, created_eq_classes, eq_class_positions, created_events_context = populate_ontology_from_data(
            onto, data_rows, defined_classes, defined_properties, prop_is_functional,
            specification, property_mappings
        )
        if failed_rows_count == len(data_rows) and len(data_rows) > 0:
            logger.error(f"Population failed for all {len(data_rows)} data rows.")
            population_successful = False
        elif failed_rows_count > 0:
            logger.warning(f"Population completed with {failed_rows_count} out of {len(data_rows)} failed rows.")
        else:
            logger.info(f"Population completed successfully for all {len(data_rows)} rows.")

    except Exception as pop_exc:
        logger.error(f"Critical error during population: {pop_exc}", exc_info=True)
        population_successful = False

    logger.info("ABox population phase finished.")
    return population_successful, failed_rows_count, created_eq_classes, eq_class_positions, created_events_context

def _run_analysis_and_optimization(onto, defined_classes, specification, optimize_ontology, output_owl_path, logger):
    logger.info("Analyzing ontology population status...")
    try:
        population_counts, empty_classes, class_instances, class_usage_info = analyze_ontology_population(onto, defined_classes, specification)
        population_report = generate_population_report(population_counts, empty_classes, class_instances, defined_classes, class_usage_info)
        logger.info("Ontology Population Analysis Complete")
        print(population_report) # Print to console

        if optimize_ontology:
            logger.info("Generating detailed optimization recommendations...")
            optimization_recs = generate_optimization_recommendations(class_usage_info, defined_classes)
            print("\n=== DETAILED OPTIMIZATION RECOMMENDATIONS ===")
            if optimization_recs.get('classes_to_remove'):
                print(f"\nClasses that could be safely removed ({len(optimization_recs['classes_to_remove'])}):")
                for class_name in optimization_recs['classes_to_remove']:
                    print(f"  • {class_name}")
            if optimization_recs.get('configuration_options'):
                print("\nSuggested configuration for future runs:")
                for option in optimization_recs['configuration_options']:
                    print(f"  • {option}")
            # Save recommendations to file
            try:
                base_dir = os.path.dirname(output_owl_path)
                recs_file = os.path.join(base_dir, "ontology_optimization.txt")
                with open(recs_file, 'w') as f:
                    f.write("# Ontology Optimization Recommendations\n\n")
                    f.write("## Classes to Remove\n")
                    for cls in optimization_recs.get('classes_to_remove', []):
                        f.write(f"- {cls}\n")
                    f.write("\n## Configuration Options\n")
                    for opt in optimization_recs.get('configuration_options', []):
                        f.write(f"- {opt}\n")
                logger.info(f"Saved optimization recommendations to {recs_file}")
            except Exception as e:
                logger.error(f"Failed to save optimization recommendations: {e}")

    except Exception as analysis_exc:
        logger.error(f"Error analyzing ontology population: {analysis_exc}", exc_info=False)
        # Continue despite analysis failure

def _setup_sequence_relationships(onto, created_eq_classes, eq_class_positions, defined_classes, defined_properties, property_is_functional, logger):
    logger.info("Setting up sequence relationships...")
    try:
        setup_equipment_sequence_relationships(onto, eq_class_positions, defined_classes, defined_properties, created_eq_classes)
        setup_equipment_instance_relationships(onto, defined_classes, defined_properties, property_is_functional, eq_class_positions)
        logger.info("Sequence relationship setup complete.")
        
        # Add equipment sequence report generation
        sequence_report = generate_equipment_sequence_report(onto)
        logger.info("Equipment sequence report generated.")
        print(sequence_report)  # Print to console for immediate visibility
        
    except Exception as seq_exc:
        logger.error(f"Error during sequence relationship setup: {seq_exc}", exc_info=True)
        # Log error but continue

def _link_equipment_events(onto, created_events_context, defined_classes, defined_properties, logger):
    logger.info("Linking equipment events to line events...")
    try:
        links_made = link_equipment_events_to_line_events(
            onto, created_events_context, defined_classes, defined_properties
        )
        logger.info(f"Event linking pass created {links_made} links.")
    except Exception as link_exc:
        logger.error(f"Error during event linking pass: {link_exc}", exc_info=True)
        # Log error but continue

def _run_reasoning_phase(onto, world, world_db_path, reasoner_report_max_entities, reasoner_report_verbose, logger):
    logger.info("Applying reasoner (ensure HermiT or compatible reasoner is installed)...")
    reasoning_successful = True
    try:
        active_world = world if world_db_path else default_world
        with onto:
            pre_stats = {
                'classes': len(list(onto.classes())), 'object_properties': len(list(onto.object_properties())),
                'data_properties': len(list(onto.data_properties())), 'individuals': len(list(onto.individuals()))
            }
            logger.info("Starting reasoning process...")
            reasoning_start_time = timing.time()
            sync_reasoner(infer_property_values=True, debug=0) # Pass world implicitly via onto context?
            reasoning_end_time = timing.time()
            logger.info(f"Reasoning finished in {reasoning_end_time - reasoning_start_time:.2f} seconds.")

            # Post-reasoning analysis and report generation
            inconsistent = list(active_world.inconsistent_classes())
            inferred_hierarchy = {}
            inferred_properties = {}
            inferred_individuals = {}
            for cls in onto.classes():
                current_subclasses = set(cls.subclasses())
                inferred_subs = [sub.name for sub in current_subclasses if sub != cls and sub != Nothing] 
                equivalent_classes = [eq.name for eq in cls.equivalent_to if eq != cls and isinstance(eq, ThingClass)]
                if inferred_subs or equivalent_classes:
                    inferred_hierarchy[cls.name] = {'subclasses': inferred_subs, 'equivalent': equivalent_classes}

            inferrable_chars = {
                'FunctionalProperty': FunctionalProperty, 'InverseFunctionalProperty': InverseFunctionalProperty,
                'TransitiveProperty': TransitiveProperty, 'SymmetricProperty': SymmetricProperty,
                'AsymmetricProperty': AsymmetricProperty, 'ReflexiveProperty': ReflexiveProperty,
                'IrreflexiveProperty': IrreflexiveProperty,
            }
            for prop in list(onto.object_properties()) + list(onto.data_properties()):
                inferred_chars_for_prop = [char_name for char_name, char_class in inferrable_chars.items() if char_class in prop.is_a]
                if inferred_chars_for_prop: inferred_properties[prop.name] = inferred_chars_for_prop

            logger.info("Collecting simplified individual inferences (post-reasoning state).")
            for ind in onto.individuals():
                current_types = [c.name for c in ind.is_a if c is not Thing]
                current_props = {}
                for prop in list(onto.object_properties()) + list(onto.data_properties()):
                    try:
                        values = prop[ind]
                        if not isinstance(values, list): values = [values] if values is not None else []
                        if values:
                            formatted_values = []
                            for v in values:
                                if isinstance(v, Thing): formatted_values.append(v.name)
                                elif isinstance(v, locstr): formatted_values.append(f'"{v}"@{v.lang}')
                                else: formatted_values.append(repr(v))
                            if formatted_values: current_props[prop.name] = formatted_values
                    except Exception: continue
                if current_types or current_props:
                    inferred_individuals[ind.name] = {'types': current_types, 'properties': current_props}

            post_stats = {
                'classes': len(list(onto.classes())), 'object_properties': len(list(onto.object_properties())),
                'data_properties': len(list(onto.data_properties())), 'individuals': len(list(onto.individuals()))
            }
            report, has_issues = generate_reasoning_report(
                onto, pre_stats, post_stats, inconsistent, inferred_hierarchy,
                inferred_properties, inferred_individuals, True, # Assuming reasoner ran
                max_entities_per_category=reasoner_report_max_entities,
                verbose=reasoner_report_verbose
            )
            logger.info("\nReasoning Report:\n" + report)

            if has_issues or inconsistent:
                logger.warning("Reasoning completed but potential issues or inconsistencies were identified.")
                if inconsistent: reasoning_successful = False
            else: logger.info("Reasoning completed successfully.")

    except OwlReadyInconsistentOntologyError:
        logger.error("REASONING FAILED: Ontology is inconsistent!")
        reasoning_successful = False
        try:
            active_world = world if world_db_path else default_world
            inconsistent = list(active_world.inconsistent_classes())
            logger.error(f"Inconsistent classes detected: {[c.name for c in inconsistent]}")
        except Exception as e_inc: logger.error(f"Could not retrieve inconsistent classes: {e_inc}")
    except NameError as ne:
        if "sync_reasoner" in str(ne): logger.error("Reasoning failed: Reasoner (sync_reasoner) function not found.")
        else: logger.error(f"Unexpected NameError during reasoning: {ne}")
        reasoning_successful = False
    except Exception as e:
        logger.error(f"An error occurred during reasoning: {e}", exc_info=True)
        reasoning_successful = False

    logger.info("Reasoning phase finished.")
    return reasoning_successful

def _save_ontology_file(onto, world, output_owl_path, save_format, world_db_path, population_successful, reasoning_successful, logger):
    should_save_primary = population_successful and reasoning_successful
    final_output_path = output_owl_path
    save_failed = False

    if not should_save_primary:
        logger.error("Ontology generation had issues (population/reasoning failure/inconsistency). Saving to debug file instead.")
        base, ext = os.path.splitext(output_owl_path)
        debug_output_path = f"{base}_debug{ext}"
        if debug_output_path == output_owl_path:
            debug_output_path = output_owl_path + "_debug"
        final_output_path = debug_output_path
        logger.info(f"Attempting to save potentially problematic ontology to: {final_output_path}")
    else:
        logger.info(f"Attempting to save final ontology to: {final_output_path}")

    logger.info(f"Saving ontology in '{save_format}' format...")
    try:
        # Use the world associated with the ontology for saving, especially if persistent
        # If world is None (in-memory case after setup failure?), this will likely fail, which is ok.
        onto.save(file=final_output_path, format=save_format)
        logger.info("Ontology saved successfully.")
    except Exception as save_err:
        logger.error(f"Failed to save ontology to {final_output_path}: {save_err}", exc_info=True)
        save_failed = True # Indicate saving failed

    return save_failed

def main_ontology_generation(spec_file_path: str,
                             data_file_path: str,
                             output_owl_path: str,
                             ontology_iri: str = DEFAULT_ONTOLOGY_IRI,
                             save_format: str = "rdfxml",
                             use_reasoner: bool = False,
                             world_db_path: Optional[str] = None,
                             reasoner_report_max_entities: int = 10,
                             reasoner_report_verbose: bool = False,
                             analyze_population: bool = True,
                             strict_adherence: bool = False,
                             skip_classes: List[str] = None,
                             optimize_ontology: bool = False
                            ) -> bool:
    """
    Main function to generate the ontology by orchestrating helper functions.
    (Args documentation remains the same)
    Returns:
        bool: True on overall success, False on failure
    """
    start_time = timing.time()
    main_logger.info("--- Ontology Generation Process Started ---")

    # Use a dummy args object for logging if needed, or adapt helpers
    # For simplicity, let's create a temporary Namespace-like object
    class Args: pass
    args = Args()
    args.spec_file = spec_file_path
    args.data_file = data_file_path
    args.output_file = output_owl_path
    args.iri = ontology_iri
    args.format = save_format
    args.reasoner = use_reasoner
    args.worlddb = world_db_path
    args.max_report_entities = reasoner_report_max_entities
    args.full_report = reasoner_report_verbose
    args.analyze_population = analyze_population
    args.strict_adherence = strict_adherence
    args.skip_classes = skip_classes
    args.optimize_ontology = optimize_ontology

    world = None
    onto = None
    population_successful = False
    reasoning_successful = True # Assume success unless reasoner runs and fails
    save_failed = False

    try:
        # 1. Log Initial Parameters
        _log_initial_parameters(args, main_logger)

        # 2. Parse Specification and Mappings
        specification, property_mappings = _parse_spec_and_mappings(args.spec_file, main_logger)
        if specification is None: return False

        # 3. Setup World and Ontology
        world, onto = _setup_world_and_ontology(args.iri, args.worlddb, main_logger)
        if onto is None: return False

        # 4. Define Ontology Structure (TBox)
        defined_classes, defined_properties, property_is_functional = _define_tbox(
            onto, specification, args.strict_adherence, args.skip_classes, main_logger
        )
        # Handle case where TBox definition might yield nothing critical?
        # Current _define_tbox logs warning, main flow continues.

        # 5. Read Operational Data
        data_rows = _read_operational_data(args.data_file, main_logger)
        if data_rows is None: return False # Indicate failure if reading failed

        # 6. Populate Ontology (ABox)
        population_successful, failed_rows_count, created_eq_classes, eq_class_positions, created_events_context = _populate_abox(
            onto, data_rows, defined_classes, defined_properties, property_is_functional,
            specification, property_mappings, main_logger
        )
        # Population failure is handled later in saving/reasoning steps

        # 7. Analyze Population & Optimize (Optional)
        if population_successful and args.analyze_population:
            _run_analysis_and_optimization(onto, defined_classes, specification, args.optimize_ontology, args.output_file, main_logger)
        elif not args.analyze_population:
            main_logger.warning("Skipping ontology population analysis as requested.")

        # 8. Setup Sequence Relationships (Optional)
        if population_successful and created_eq_classes and eq_class_positions:
             _setup_sequence_relationships(onto, created_eq_classes, eq_class_positions, defined_classes, defined_properties, property_is_functional, main_logger)
        elif population_successful:
             main_logger.warning("Skipping sequence relationship setup: No EquipmentClass info available.")
        # No action needed if population failed, handled by checks below

        # 9. Link Events (Optional)
        if population_successful and created_events_context:
            _link_equipment_events(onto, created_events_context, defined_classes, defined_properties, main_logger)
        elif population_successful:
             main_logger.warning("Skipping event linking: No event context available.")
        # No action needed if population failed

        # 10. Apply Reasoning (Optional)
        if args.reasoner and population_successful:
            reasoning_successful = _run_reasoning_phase(onto, world, args.worlddb, args.max_report_entities, args.full_report, main_logger)
        elif args.reasoner and not population_successful:
            main_logger.warning("Skipping reasoning due to prior population failure.")
            reasoning_successful = False # Ensure overall success reflects this skipped step
        # If reasoner not used, reasoning_successful remains True

        # 11. Save Ontology
        # Saving logic depends on population and reasoning success
        # The helper returns True if saving *failed*
        save_failed = _save_ontology_file(onto, world, args.output_file, args.format, args.worlddb, population_successful, reasoning_successful, main_logger)
        if save_failed:
            return False # Saving failed, overall process is unsuccessful

        # 12. Determine overall success
        overall_success = population_successful and reasoning_successful and not save_failed
        return overall_success

    except Exception as e:
        main_logger.exception("A critical error occurred during the overall ontology generation process.")
        return False

    finally:
        end_time = timing.time()
        main_logger.info(f"--- Ontology Generation Finished --- Total time: {end_time - start_time:.2f} seconds")
        
        # Log suppressed message counts
        from ontology_generator.utils.logging import log_suppressed_message_counts
        log_suppressed_message_counts()


def test_property_mappings(spec_file_path: str):
    """
    Test function to verify property mapping functionality.
    
    This can be called manually for testing and provides detailed debug output
    about the parsed property mappings for all entity types.
    
    Args:
        spec_file_path: Path to the specification CSV file
    """
    # Configure more verbose logging for testing
    configure_logging(level=logging.DEBUG)
    
    test_logger = logging.getLogger("property_mapping_test")
    test_logger.info("=== Starting Property Mapping Test ===")
    
    try:
        # Parse spec file
        test_logger.info(f"Parsing specification file: {spec_file_path}")
        spec = parse_specification(spec_file_path)
        test_logger.info(f"Parsed {len(spec)} rows from specification file")
        
        # Parse and validate property mappings
        test_logger.info("Generating property mappings from specification")
        mappings = parse_property_mappings(spec)
        validation_passed = validate_property_mappings(mappings)
        test_logger.info(f"Validation result: {'PASSED' if validation_passed else 'FAILED'}")
        
        # Group entities by logical group for organization
        from collections import defaultdict
        entity_groups = defaultdict(list)
        for row in spec:
            entity = row.get('Proposed OWL Entity', '').strip()
            group = row.get('Logical Group', '').strip()
            if entity and group:
                if entity not in entity_groups[group]:
                    entity_groups[group].append(entity)
                    
        # Print summary by group
        test_logger.info("\n=== Entity Coverage by Logical Group ===")
        for group, entities in sorted(entity_groups.items()):
            mapped_entities = [e for e in entities if e in mappings]
            test_logger.info(f"{group}: {len(mapped_entities)}/{len(entities)} entities mapped")
            
            if mapped_entities:
                for entity in sorted(mapped_entities):
                    data_props = len(mappings[entity].get('data_properties', {}))
                    obj_props = len(mappings[entity].get('object_properties', {}))
                    test_logger.info(f"  ✓ {entity}: {data_props} data properties, {obj_props} object properties")
            
            missing = [e for e in entities if e not in mappings]
            if missing:
                for entity in sorted(missing):
                    test_logger.warning(f"  ✗ {entity}: No property mappings found")
        
        # Detailed analysis of common entities
        key_entities = [
            'EventRecord', 
            'Material', 
            'OperationalReason', 
            'OperationalState',
            'ProductionLine',
            'Equipment',
            'EquipmentClass',
            'Plant',
            'Area',
            'ProcessCell',
            'Shift',
            'TimeInterval',
            'ProductionRequest'
        ]
        
        for entity in key_entities:
            if entity in mappings:
                entity_map = mappings[entity]
                
                test_logger.info(f"\n=== {entity} Property Mappings ===")
                
                # Data properties
                data_props = entity_map.get('data_properties', {})
                test_logger.info(f"Found {len(data_props)} data properties for {entity}")
                
                if data_props:
                    for prop_name, details in sorted(data_props.items()):
                        test_logger.info(f"  ✓ {prop_name}: column='{details.get('column')}', type='{details.get('data_type')}', functional={details.get('functional')}")
                
                # Object properties
                obj_props = entity_map.get('object_properties', {})
                if obj_props:
                    test_logger.info(f"Found {len(obj_props)} object properties for {entity}")
                    for prop_name, details in sorted(obj_props.items()):
                        test_logger.info(f"  ✓ {prop_name}: column='{details.get('column')}', target='{details.get('target_class')}', functional={details.get('functional')}")
            else:
                test_logger.warning(f"\n=== {entity} Property Mappings ===")
                test_logger.warning(f"  ✗ {entity} entity not found in mappings!")
        
        # Summary stats        
        total_data_props = sum(len(entity_map.get('data_properties', {})) for entity_map in mappings.values())
        total_obj_props = sum(len(entity_map.get('object_properties', {})) for entity_map in mappings.values())
        
        test_logger.info("\n=== Property Mapping Summary ===")
        test_logger.info(f"Total entities mapped: {len(mappings)}")
        test_logger.info(f"Total data properties mapped: {total_data_props}")
        test_logger.info(f"Total object properties mapped: {total_obj_props}")
        test_logger.info(f"Total properties mapped: {total_data_props + total_obj_props}")
        
        test_logger.info("=== Property Mapping Test Complete ===")
        
    except Exception as e:
        test_logger.error(f"Error during property mapping test: {e}", exc_info=True)


def analyze_equipment_sequence_in_ontology(owl_file_path: str, verbose: bool = False) -> bool:
    """
    Analyze equipment sequences in an existing ontology file.
    
    Args:
        owl_file_path: Path to the OWL file to analyze
        verbose: Whether to output verbose logging
        
    Returns:
        bool: True if analysis was successful, False otherwise
    """
    # Configure logging
    log_level = logging.DEBUG if verbose else logging.INFO
    configure_logging(log_level)
    logger = main_logger
    
    logger.info(f"Analyzing equipment sequences in ontology file: {owl_file_path}")
    
    try:
        # Load the ontology
        world = World()
        onto = world.get_ontology(owl_file_path).load()
        logger.info(f"Loaded ontology: {onto.base_iri}")
        
        # Generate and print the equipment sequence report
        sequence_report = generate_equipment_sequence_report(onto)
        print(sequence_report)
        
        # Run deeper analysis if verbose
        if verbose:
            sequences, stats = analyze_equipment_sequences(onto)
            print("\n=== EQUIPMENT SEQUENCE STATISTICS ===")
            print(f"Total Lines: {stats['total_lines']}")
            print(f"Lines with Equipment Sequence: {stats['lines_with_sequence']}")
            print(f"Total Equipment in Sequences: {stats['total_equipment']}")
            print("\nEquipment Classes:")
            for cls, count in sorted(stats['class_counts'].items(), key=lambda x: x[1], reverse=True):
                print(f"  {cls}: {count}")
        
        return True
    except Exception as e:
        logger.error(f"Error analyzing equipment sequences: {e}", exc_info=True)
        return False

def main():
    """Main entry point for the ontology generator."""
    parser = argparse.ArgumentParser(description="Generate an OWL ontology from specification and data CSV files.")
    parser.add_argument("spec_file", help="Path to the ontology specification CSV file (e.g., opera_spec.csv).")
    parser.add_argument("data_file", help="Path to the operational data CSV file (e.g., sample_data.csv).")
    parser.add_argument("output_file", help="Path to save the generated OWL ontology file (e.g., manufacturing.owl).")
    parser.add_argument("--iri", default=DEFAULT_ONTOLOGY_IRI, help=f"Base IRI for the ontology (default: {DEFAULT_ONTOLOGY_IRI}).")
    parser.add_argument("--format", default="rdfxml", choices=["rdfxml", "ntriples", "nquads", "owlxml"], help="Format for saving the ontology (default: rdfxml).")
    parser.add_argument("--reasoner", action="store_true", help="Run the reasoner after population.")
    parser.add_argument("--worlddb", default=None, help="Path to use/create a persistent SQLite world database (e.g., my_ontology.sqlite3).")
    parser.add_argument("--max-report-entities", type=int, default=10, help="Maximum number of entities to show per category in the reasoner report (default: 10).")
    parser.add_argument("--full-report", action="store_true", help="Show full details in the reasoner report (all entities).")
    parser.add_argument("--no-analyze-population", action="store_false", dest="analyze_population", help="Skip analysis and reporting of ontology population (analysis is on by default).")
    parser.add_argument("--strict-adherence", action="store_true", help="Only create classes explicitly defined in the specification.")
    parser.add_argument("--skip-classes", type=str, nargs='+', help="List of class names to skip during ontology creation.")
    parser.add_argument("--optimize", action="store_true", dest="optimize_ontology", help="Generate detailed optimization recommendations.")
    parser.add_argument("--test-mappings", action="store_true", help="Test the property mapping functionality only, without generating the ontology.")
    parser.add_argument("--analyze-sequences", metavar="OWL_FILE", help="Analyze equipment sequences in an existing ontology file.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Enable verbose (DEBUG level) logging.")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress INFO level logging.")

    args = parser.parse_args()

    # If analyze-sequences mode is requested, just run the analysis and exit
    if hasattr(args, 'analyze_sequences') and args.analyze_sequences:
        success = analyze_equipment_sequence_in_ontology(args.analyze_sequences, args.verbose)
        sys.exit(0 if success else 1)

    # If test mode is requested, just run the test and exit
    if hasattr(args, 'test_mappings') and args.test_mappings:
        test_property_mappings(args.spec_file)
        sys.exit(0)

    # Setup Logging Level
    log_level = logging.INFO
    if args.verbose: 
        log_level = logging.DEBUG
    elif args.quiet: 
        log_level = logging.WARNING

    # Configure logging
    configure_logging(log_level=log_level)

    # Execute main function
    success = main_ontology_generation(
        args.spec_file, args.data_file, args.output_file,
        args.iri, args.format, args.reasoner, args.worlddb,
        reasoner_report_max_entities=args.max_report_entities,
        reasoner_report_verbose=args.full_report,
        analyze_population=args.analyze_population,
        strict_adherence=args.strict_adherence,
        skip_classes=args.skip_classes,
        optimize_ontology=args.optimize_ontology
    )
    
    # Exit with appropriate code
    if success:
        main_logger.info("Ontology generation process completed.")
        sys.exit(0)
    else:
        main_logger.error("Ontology generation process failed or encountered errors.")
        sys.exit(1)


if __name__ == "__main__":
    main()


===========================================
FILE: ontology_generator/population/__init__.py
===========================================

from .sequence import setup_equipment_sequence_relationships, setup_equipment_instance_relationships
from .linking import link_equipment_events_to_line_events
from .row_processor import process_single_data_row_pass1, process_single_data_row_pass2

# List functions/classes to expose at the package level
__all__ = [
    # 'populate_ontology_from_data', # Removed - Defined in main.py
    'setup_equipment_sequence_relationships',
    'setup_equipment_instance_relationships',
    'link_equipment_events_to_line_events',
    'process_single_data_row_pass1',
    'process_single_data_row_pass2',
    # Add other core components if needed, e.g., PopulationContext? No, likely used internally.
]


===========================================
FILE: ontology_generator/population/asset.py
===========================================

"""
Asset population module for the ontology generator.

This module provides functions for processing asset hierarchy data.
"""
from typing import Dict, Any, Optional, Tuple

from owlready2 import Thing

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, apply_data_property_mappings
)

# Type Alias for registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object

def process_asset_hierarchy(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
) -> Tuple[Optional[Thing], Optional[Thing], Optional[Thing], Optional[Thing]]:
    """
    Processes the asset hierarchy (Plant, Area, ProcessCell, ProductionLine)
    from a single data row for a specific population pass.

    Pass 1: Creates individuals, applies data properties, adds to registry.
    Pass 2: (Currently not handled here, linking done by apply_object_property_mappings)

    Args:
        row: The data row.
        context: Population context.
        property_mappings: Property mappings dictionary.
        all_created_individuals_by_uid: The central registry of individuals.
        pass_num: The current population pass (1 or 2).

    Returns:
        A tuple containing the Plant, Area, ProcessCell, and ProductionLine
        individuals created or retrieved for this row.
    """
    if not property_mappings:
        pop_logger.warning("Property mappings not provided to process_asset_hierarchy. Skipping.")
        return None, None, None, None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_asset_hierarchy. Skipping.")
         return None, None, None, None

    # Get classes from context
    cls_Plant = context.get_class("Plant")
    cls_Area = context.get_class("Area")
    cls_ProcessCell = context.get_class("ProcessCell")
    cls_ProductionLine = context.get_class("ProductionLine")

    if not all([cls_Plant, cls_Area, cls_ProcessCell, cls_ProductionLine]):
        pop_logger.error("One or more essential asset classes (Plant, Area, ProcessCell, ProductionLine) not found. Cannot process hierarchy.")
        return None, None, None, None

    plant_ind: Optional[Thing] = None
    area_ind: Optional[Thing] = None
    pcell_ind: Optional[Thing] = None
    line_ind: Optional[Thing] = None

    # --- Plant ---
    plant_id_map = property_mappings.get('Plant', {}).get('data_properties', {}).get('plantId')
    if not plant_id_map or not plant_id_map.get('column'):
        pop_logger.error("Cannot determine the column for Plant.plantId from property mappings. Skipping Plant creation.")
        return None, None, None, None
    plant_id_col = plant_id_map['column']
    plant_id = safe_cast(row.get(plant_id_col), str)
    if not plant_id:
        pop_logger.error(f"Missing or invalid Plant ID in column '{plant_id_col}'. Skipping Plant creation.")
        return None, None, None, None  # Plant is essential

    plant_labels = [plant_id]
    plant_ind = get_or_create_individual(cls_Plant, plant_id, context.onto, all_created_individuals_by_uid, add_labels=plant_labels)

    if plant_ind and pass_num == 1 and "Plant" in property_mappings:
        apply_data_property_mappings(plant_ind, property_mappings["Plant"], row, context, "Plant", pop_logger)
    elif not plant_ind:
         pop_logger.error(f"Failed to create/retrieve Plant individual for ID '{plant_id}'. Cannot proceed with hierarchy.")
         return None, None, None, None
    # elif pass_num == 1: # Mappings missing or wrong pass
    #      pop_logger.warning(f"No property mappings found for Plant '{plant_id}' or wrong pass ({pass_num}). Only basic individual created/retrieved.")
    #      # Ensure ID is set if mapping was missing but creation succeeded (redundant if get_or_create works)
    #      # if not getattr(plant_ind, "plantId", None):
    #      #     context.set_prop(plant_ind, "plantId", plant_id)

    # --- Area ---
    area_id_map = property_mappings.get('Area', {}).get('data_properties', {}).get('areaId')
    if not area_id_map or not area_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for Area.areaId from property mappings. Skipping Area/ProcessCell/Line creation.")
        return plant_ind, None, None, None
    area_id_col = area_id_map['column']

    raw_area_id = row.get(area_id_col)
    if not raw_area_id:
        pop_logger.warning(f"Missing or invalid Area ID in column '{area_id_col}'. Skipping Area/ProcessCell/Line creation.")
        return plant_ind, None, None, None
    area_id = safe_cast(raw_area_id, str)

    # Need plant_id for unique name
    plant_id_for_name = plant_id # Already checked plant_id exists
    area_unique_base = f"{plant_id_for_name}_{area_id}"
    area_labels = [area_id]
    area_ind = get_or_create_individual(cls_Area, area_unique_base, context.onto, all_created_individuals_by_uid, add_labels=area_labels)

    if area_ind and pass_num == 1 and "Area" in property_mappings:
        # Linking to Plant (locatedInPlant) happens in Pass 2 via apply_object_property_mappings
        apply_data_property_mappings(area_ind, property_mappings["Area"], row, context, "Area", pop_logger)
    elif not area_ind:
         pop_logger.warning(f"Failed to create/retrieve Area individual for base '{area_unique_base}'. Skipping ProcessCell/Line creation.")
         # Return what we have so far
         return plant_ind, None, None, None
    # elif pass_num == 1: # Mappings missing or wrong pass
    #      pop_logger.warning(f"No property mappings found for Area '{area_id}' or wrong pass ({pass_num}). Only basic individual created/retrieved.")
         # Ensure ID is set and link to plant if possible (DEFER LINKING)
         # if not getattr(area_ind, "areaId", None):
         #     context.set_prop(area_ind, "areaId", area_id)
         # if plant_ind and not getattr(area_ind, "locatedInPlant", None):
         #     pop_logger.warning(f"Manually linking Area '{area_id}' to Plant '{plant_id}' due to missing mapping.") # DEFER/REMOVE
         #     context.set_prop(area_ind, "locatedInPlant", plant_ind) # DEFER/REMOVE

    # --- ProcessCell ---
    pcell_id_map = property_mappings.get('ProcessCell', {}).get('data_properties', {}).get('processCellId')
    if not pcell_id_map or not pcell_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for ProcessCell.processCellId from property mappings. Skipping ProcessCell/Line creation.")
        return plant_ind, area_ind, None, None
    pcell_id_col = pcell_id_map['column']

    pcell_id = safe_cast(row.get(pcell_id_col), str)
    if not pcell_id:
        pop_logger.warning(f"Missing or invalid ProcessCell ID in column '{pcell_id_col}'. Skipping ProcessCell/Line creation.")
        return plant_ind, area_ind, None, None

    # Need area_unique_base for unique name
    area_base_for_name = area_unique_base # Already checked area_unique_base exists
    pcell_unique_base = f"{area_base_for_name}_{pcell_id}"
    pcell_labels = [pcell_id]
    pcell_ind = get_or_create_individual(cls_ProcessCell, pcell_unique_base, context.onto, all_created_individuals_by_uid, add_labels=pcell_labels)

    if pcell_ind and pass_num == 1 and "ProcessCell" in property_mappings:
        # Linking to Area (partOfArea) happens in Pass 2
        apply_data_property_mappings(pcell_ind, property_mappings["ProcessCell"], row, context, "ProcessCell", pop_logger)
    elif not pcell_ind:
        pop_logger.warning(f"Failed to create/retrieve ProcessCell individual for base '{pcell_unique_base}'. Skipping Line creation.")
        return plant_ind, area_ind, None, None
    # elif pass_num == 1: # Mappings missing or wrong pass
    #     pop_logger.warning(f"No property mappings found for ProcessCell '{pcell_id}' or wrong pass ({pass_num}). Only basic individual created/retrieved.")
        # Ensure ID is set and link to area if possible (DEFER LINKING)
        # if not getattr(pcell_ind, "processCellId", None):
        #     context.set_prop(pcell_ind, "processCellId", pcell_id)
        # if area_ind and not getattr(pcell_ind, "partOfArea", None):
        #     pop_logger.warning(f"Manually linking ProcessCell '{pcell_id}' to Area '{area_id}' due to missing mapping.") # DEFER/REMOVE
        #     context.set_prop(pcell_ind, "partOfArea", area_ind) # DEFER/REMOVE

    # --- ProductionLine ---
    line_id_map = property_mappings.get('ProductionLine', {}).get('data_properties', {}).get('lineId')
    if not line_id_map or not line_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for ProductionLine.lineId from property mappings. Skipping Line creation.")
        return plant_ind, area_ind, pcell_ind, None
    line_id_col = line_id_map['column']

    raw_line_id = row.get(line_id_col)
    if not raw_line_id:
        pop_logger.warning(f"Missing or invalid Line ID in column '{line_id_col}'. Skipping Line creation.")
        return plant_ind, area_ind, pcell_ind, None # Return created individuals up to this point
    else:
        line_id = safe_cast(raw_line_id, str)
        # Need pcell_unique_base for unique name
        pcell_base_for_name = pcell_unique_base # Already checked pcell_unique_base exists
        line_unique_base = f"{pcell_base_for_name}_{line_id}"
        line_labels = [line_id]
        line_ind = get_or_create_individual(cls_ProductionLine, line_unique_base, context.onto, all_created_individuals_by_uid, add_labels=line_labels)

        if line_ind and pass_num == 1 and "ProductionLine" in property_mappings:
            # Linking to ProcessCell (locatedInProcessCell) happens in Pass 2
            apply_data_property_mappings(line_ind, property_mappings["ProductionLine"], row, context, "ProductionLine", pop_logger)
        elif not line_ind:
             pop_logger.warning(f"Failed to create/retrieve ProductionLine individual for base '{line_unique_base}'.")
             # Return what we have (line_ind will be None)
             return plant_ind, area_ind, pcell_ind, None
        # elif pass_num == 1: # Mappings missing or wrong pass
        #     pop_logger.warning(f"No property mappings found for ProductionLine '{line_id}' or wrong pass ({pass_num}). Only basic individual created/retrieved.")
            # Ensure ID is set and link to process cell if possible (DEFER LINKING)
            # if not getattr(line_ind, "lineId", None):
            #     context.set_prop(line_ind, "lineId", line_id)
            # if pcell_ind and not getattr(line_ind, "locatedInProcessCell", None):
            #     pop_logger.warning(f"Manually linking Line '{line_id}' to ProcessCell '{pcell_id}' due to missing mapping.") # DEFER/REMOVE
            #     context.set_prop(line_ind, "locatedInProcessCell", pcell_ind) # DEFER/REMOVE

    # Return all individuals created/retrieved in this hierarchy for this row
    return plant_ind, area_ind, pcell_ind, line_ind


def process_material(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
    ) -> Optional[Thing]:
    """
    Processes Material from a row using property mappings (Pass 1: Create/Data Props).

    Args:
        row: The data row
        context: The population context
        property_mappings: Property mappings dictionary
        all_created_individuals_by_uid: Central individual registry.
        pass_num: Current pass number.

    Returns:
        The Material individual or None
    """
    if not property_mappings or "Material" not in property_mappings:
        pop_logger.warning("Property mappings for 'Material' not provided or empty. Skipping material processing.")
        return None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_material. Skipping.")
         return None

    cls_Material = context.get_class("Material")
    if not cls_Material:
        # Error logged by get_class
        return None

    mat_id_map = property_mappings['Material'].get('data_properties', {}).get('materialId')
    if not mat_id_map or not mat_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for Material.materialId from property mappings. Skipping material creation.")
        return None
    mat_id_col = mat_id_map['column']
    mat_id = safe_cast(row.get(mat_id_col), str)
    if not mat_id:
        pop_logger.debug(f"No Material ID found in column '{mat_id_col}', skipping material creation.")
        return None

    # Try to get description for label
    mat_desc_map = property_mappings['Material'].get('data_properties', {}).get('materialDescription')
    mat_desc = None
    if mat_desc_map and mat_desc_map.get('column'):
         mat_desc = safe_cast(row.get(mat_desc_map['column']), str)

    mat_labels = [mat_id]
    if mat_desc:
        mat_labels.append(mat_desc)

    mat_ind = get_or_create_individual(cls_Material, mat_id, context.onto, all_created_individuals_by_uid, add_labels=mat_labels)
    if not mat_ind:
        pop_logger.error(f"Failed to create/retrieve Material individual for ID '{mat_id}'.")
        return None

    # Apply data properties in Pass 1
    if pass_num == 1:
        apply_data_property_mappings(mat_ind, property_mappings["Material"], row, context, "Material", pop_logger)
        # Minimal check if ID wasn't set by mapping (redundant?)
        # if not getattr(mat_ind, "materialId", None):
        #      pop_logger.warning(f"Material.materialId was not set via mappings for {mat_id}, setting manually.")
        #      context.set_prop(mat_ind, "materialId", mat_id)

    return mat_ind


def process_production_request(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
    ) -> Optional[Thing]:
    """
    Processes ProductionRequest from a row (Pass 1: Create/Data Props).

    Args:
        row: The data row
        context: The population context
        property_mappings: Property mappings dictionary
        all_created_individuals_by_uid: Central individual registry.
        pass_num: Current pass number.

    Returns:
        The ProductionRequest individual or None
    """
    if not property_mappings or "ProductionRequest" not in property_mappings:
        pop_logger.warning("Property mappings for 'ProductionRequest' not provided or empty. Skipping request processing.")
        return None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_production_request. Skipping.")
         return None

    cls_ProductionRequest = context.get_class("ProductionRequest")
    if not cls_ProductionRequest:
        # Error logged by get_class
        return None

    req_id_map = property_mappings['ProductionRequest'].get('data_properties', {}).get('requestId')
    if not req_id_map or not req_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for ProductionRequest.requestId from property mappings. Skipping request creation.")
        return None
    req_id_col = req_id_map['column']

    req_id = safe_cast(row.get(req_id_col), str)
    if not req_id:
        pop_logger.debug(f"No Production Request ID in column '{req_id_col}', skipping request creation.")
        return None

    # Try to get description for label
    req_desc_map = property_mappings['ProductionRequest'].get('data_properties', {}).get('requestDescription')
    req_desc = None
    if req_desc_map and req_desc_map.get('column'):
         req_desc = safe_cast(row.get(req_desc_map['column']), str)

    req_labels = [f"ID:{req_id}"]
    if req_desc:
        req_labels.insert(0, req_desc) # Prepend description if available

    req_ind = get_or_create_individual(cls_ProductionRequest, req_id, context.onto, all_created_individuals_by_uid, add_labels=req_labels)
    if not req_ind:
        pop_logger.error(f"Failed to create/retrieve ProductionRequest individual for ID '{req_id}'.")
        return None

    # Apply data properties in Pass 1
    if pass_num == 1:
        apply_data_property_mappings(req_ind, property_mappings["ProductionRequest"], row, context, "ProductionRequest", pop_logger)
        # Minimal check if ID wasn't set by mapping
        # if not getattr(req_ind, "requestId", None):
        #      pop_logger.warning(f"ProductionRequest.requestId was not set via mappings for {req_id}, setting manually.")
        #      context.set_prop(req_ind, "requestId", req_id)

    # Note: Linking happens in Pass 2 via EventRecord mappings

    return req_ind


===========================================
FILE: ontology_generator/population/core.py
===========================================

"""
Core population module for the ontology generator.

This module provides the base functionality for ontology population, including the
PopulationContext class and property application functions.
"""
from typing import Dict, Any, Optional, List, Set, Tuple, Union, Callable
import logging
import pandas as pd

from owlready2 import (
    Ontology, Thing, ThingClass, PropertyClass,
    locstr, FunctionalProperty, ObjectProperty, DataProperty, ObjectPropertyClass, DataPropertyClass
)

from ontology_generator.utils.logging import pop_logger
from ontology_generator.config import XSD_TYPE_MAP
from ontology_generator.utils.types import safe_cast, sanitize_name

# Type Alias for registry used in linking
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object

class PopulationContext:
    """
    Holds references to ontology elements needed during population.
    
    Attributes:
        onto: The ontology being populated
        classes: Dictionary of defined classes
        props: Dictionary of defined properties
        is_functional: Dictionary indicating whether properties are functional
    """
    def __init__(self, 
                 onto: Ontology, 
                 defined_classes: Dict[str, ThingClass], 
                 defined_properties: Dict[str, PropertyClass], 
                 property_is_functional: Dict[str, bool]):
        """
        Initialize the population context.
        
        Args:
            onto: The ontology being populated
            defined_classes: Dictionary mapping class names to class objects
            defined_properties: Dictionary mapping property names to property objects
            property_is_functional: Dictionary mapping property names to boolean functionality flags
        """
        self.onto = onto
        self.defined_classes = defined_classes
        self.defined_properties = defined_properties
        self.property_is_functional = property_is_functional
        self._property_cache = {} # Cache for faster property lookup
        self._class_cache = {} # Cache for faster class lookup

    def get_class(self, name: str) -> Optional[ThingClass]:
        """
        Get a class by name.
        
        Args:
            name: The name of the class
            
        Returns:
            The class object or None if not found
        """
        if name in self._class_cache:
            return self._class_cache[name]

        cls = self.defined_classes.get(name)
        if not cls: 
            pop_logger.error(f"Essential class '{name}' not found in defined_classes.")
            return None
        # Basic validation (could add more specific checks if needed)
        if not isinstance(cls, ThingClass):
             pop_logger.error(f"Item '{name}' found but is not a ThingClass (checked via isinstance).")
             return None

        self._class_cache[name] = cls
        return cls

    def get_prop(self, name: str) -> Optional[PropertyClass]:
        """
        Get a property by name.
        
        Args:
            name: The name of the property
            
        Returns:
            The property object or None if not found
        """
        if name in self._property_cache:
            return self._property_cache[name]

        prop = self.defined_properties.get(name)
        if not prop:
            pop_logger.warning(f"Property '{name}' not found in defined properties.")
            return None
        # Basic validation (could add more specific checks if needed)
        if not isinstance(prop, (ObjectPropertyClass, DataPropertyClass)):
            pop_logger.error(f"Item '{name}' found but is not a PropertyClass (Object or Data).")
            return None

        self._property_cache[name] = prop
        return prop

    def set_prop(self, individual: Thing, prop_name: str, value: Any) -> None:
        """
        Safely sets a property value using the context.
        
        Args:
            individual: The individual to set the property on
            prop_name: The name of the property to set
            value: The value to set
        """
        prop = self.get_prop(prop_name)
        if not prop:
            # Error logged by get_prop
            return
        is_functional = self.property_is_functional.get(prop_name, False) # Assume non-functional if not specified

        try:
            _set_property_value(individual, prop, value, is_functional)
        except Exception as e:
            pop_logger.error(f"Error setting property '{prop_name}' on individual '{individual.name}' with value '{value}': {e}", exc_info=True)


def _set_property_value(individual: Thing, prop: PropertyClass, value: Any, is_functional: bool) -> None:
    """
    Helper to set functional or non-functional properties, checking existence first.
    
    Args:
        individual: The individual to set the property on
        prop: The property to set
        value: The value to set
        is_functional: Whether the property is functional
    """
    if value is None: 
        return  # Don't set None values

    prop_name = prop.python_name  # Use Python name for attribute access

    try:
        if is_functional:
            # Functional: Use setattr, potentially overwriting. Check if different first.
            current_value = getattr(individual, prop_name, None)
            # Handle comparison carefully, especially for complex types like lists/individuals
            # Simple direct comparison works for primitives and owlready individuals/locstr
            if current_value != value:
                setattr(individual, prop_name, value)
                pop_logger.debug(f"Set functional property {individual.name}.{prop.name} = {repr(value)}")
        else:
            # Non-Functional: Use append, check if value already exists.
            current_values = getattr(individual, prop_name, [])
            if not isinstance(current_values, list):  # Ensure it's a list for append
                current_values = [current_values] if current_values is not None else []

            if value not in current_values:
                # owlready handles adding to the list via direct attribute access
                getattr(individual, prop_name).append(value)
                pop_logger.debug(f"Appended non-functional property {individual.name}.{prop.name} = {repr(value)}")

    except Exception as e:
        pop_logger.error(f"Error setting property '{prop.name}' on individual '{individual.name}' with value '{repr(value)}': {e}", exc_info=False)


def set_prop_if_col_exists(
    context: PopulationContext,
    individual: Thing,
    prop_name: str,
    col_name: str,
    row: Dict[str, Any],
    cast_func: Callable,
    target_type: type,
    logger
) -> bool:
    """Helper function to check if column exists, cast value, and set property if value exists."""
    # Check if column exists in the row
    if col_name not in row:
        logger.error(f"Missing required column '{col_name}' for property '{prop_name}' on individual '{individual.name}' in row: {truncate_row_repr(row)}")
        return False

    # Column exists but might be empty/None/NaN
    raw_value = row.get(col_name)
    if pd.isna(raw_value) or raw_value == '' or raw_value is None:
        logger.debug(f"Column '{col_name}' exists but has null/empty value for property '{prop_name}' on individual '{individual.name}'")
        return False

    # Cast value to target type
    value = cast_func(raw_value, target_type)
    if value is None:  # Cast failed
        logger.warning(f"Failed to cast value '{raw_value}' from column '{col_name}' to type {target_type.__name__} for property '{prop_name}' on individual '{individual.name}'")
        return False

    # Set the property
    context.set_prop(individual, prop_name, value)
    return True

def truncate_row_repr(row: Dict[str, Any], max_length: int = 100) -> str:
    """Create a truncated string representation of a row for logging."""
    row_str = str(row)
    if len(row_str) > max_length:
        return row_str[:max_length] + "..."
    return row_str


def get_or_create_individual(
    onto_class: ThingClass,
    individual_name_base: Any,
    onto: Ontology,
    registry: IndividualRegistry, # Use the defined type alias
    add_labels: Optional[List[str]] = None
) -> Optional[Thing]:
    """
    Gets an individual from the registry or creates a new one if it doesn't exist.
    Uses a combination of class name and a base ID/name for the registry key.
    Adds labels if provided.

    Args:
        onto_class: The owlready2 class of the individual.
        individual_name_base: The base name or ID (will be sanitized) used for the individual's name and registry key.
        onto: The ontology instance.
        registry: The dictionary acting as the central registry.
        add_labels: Optional list of labels to add to the individual (if created or found).

    Returns:
        The existing or newly created individual, or None if creation fails.
    """
    if not onto_class or not individual_name_base:
        pop_logger.error(f"Missing onto_class ({onto_class}) or individual_name_base ({individual_name_base}) for get_or_create.")
        return None

    # Sanitize the base name for use in IRI and registry key
    sanitized_name_base = sanitize_name(str(individual_name_base))
    if not sanitized_name_base:
        pop_logger.error(f"Could not sanitize base name '{individual_name_base}' for individual of class '{onto_class.name}'.")
        return None

    class_name_str = onto_class.name
    registry_key = (class_name_str, sanitized_name_base)

    # Check registry first
    if registry_key in registry:
        existing_individual = registry[registry_key]
        pop_logger.debug(f"Found existing individual '{existing_individual.name}' (Key: {registry_key}) in registry.")
        # Optionally add labels even if found? Decide based on requirements.
        # if add_labels:
        #     for label in add_labels:
        #         if label and label not in existing_individual.label:
        #             existing_individual.label.append(label)
        return existing_individual

    # --- If not found, create ---
    individual_name = f"{class_name_str}_{sanitized_name_base}"

    try:
        with onto: # Ensure operation within ontology context
             # Check if an individual with this *exact* name already exists in owlready's cache
             # This can happen if safe_name produces the same result for different inputs,
             # or if an individual was created outside the registry mechanism.
            existing_by_name = onto.search_one(iri=f"*{individual_name}")
            if existing_by_name and isinstance(existing_by_name, onto_class):
                pop_logger.warning(f"Individual with name '{individual_name}' already exists in ontology but not registry (Key: {registry_key}). Returning existing one and adding to registry.")
                new_individual = existing_by_name
            elif existing_by_name:
                 # Name collision with an individual of a DIFFERENT class - should be rare with prefixing
                 pop_logger.error(f"Cannot create individual '{individual_name}': Name collision with existing individual '{existing_by_name.name}' of different class ({type(existing_by_name).__name__})")
                 return None
            else:
                # Create the new individual
                new_individual = onto_class(individual_name)
                pop_logger.info(f"Created new individual '{individual_name}' (Class: {class_name_str}, Base: '{individual_name_base}')")

                # Add labels if provided
                if add_labels:
                    for label in add_labels:
                        if label: # Ensure label is not empty
                            new_individual.label.append(str(label)) # Ensure labels are strings

        # Add to registry *after* successful creation
        registry[registry_key] = new_individual
        return new_individual

    except Exception as e:
        pop_logger.error(f"Failed to create individual '{individual_name}' of class '{class_name_str}': {e}", exc_info=True)
        return None


# --- Mappings Application Functions ---

def apply_data_property_mappings(
    individual: Thing,
    mappings: Dict[str, Dict[str, Any]],
    row: Dict[str, Any],
    context: PopulationContext,
    entity_name: str, # Name of the entity type being processed (for logging)
    logger # Pass logger explicitly
) -> None:
    """Applies data property mappings defined in the configuration."""
    if not mappings or 'data_properties' not in mappings:
        return

    data_prop_mappings = mappings.get('data_properties', {})

    for prop_name, details in data_prop_mappings.items():
        col_name = details.get('column')
        # Get cast type from mapping, default to string
        data_type_str = details.get('data_type', 'xsd:string')
        target_type = XSD_TYPE_MAP.get(data_type_str, str) # Map XSD type to Python type
        cast_func = safe_cast # Use the safe_cast utility

        if not col_name:
            logger.warning(f"Data property mapping for {entity_name}.{prop_name} is missing 'column'. Skipping.")
            continue

        # Use helper function to handle casting, existence check, and setting
        set_prop_if_col_exists(
            context=context,
            individual=individual,
            prop_name=prop_name,
            col_name=col_name,
            row=row,
            cast_func=cast_func,
            target_type=target_type,
            logger=logger
        )

def apply_object_property_mappings(
    individual: Thing,
    mappings: Dict[str, Dict[str, Any]],
    row: Dict[str, Any],
    context: PopulationContext,
    entity_name: str, # Name of the entity type being processed (for logging)
    logger, # Pass logger explicitly
    linking_context: IndividualRegistry, # The GLOBAL registry of ALL individuals
    individuals_in_row: Dict[str, Thing] # Individuals created/found specifically for THIS row in Pass 1
) -> None:
    """Applies ONLY object property mappings, using linking_context or individuals_in_row to find targets."""
    if not mappings or 'object_properties' not in mappings:
        return

    obj_prop_mappings = mappings.get('object_properties', {})
    links_applied_count = 0
    
    # Track missing entities per row to log only once
    missing_context_entities = set()

    for prop_name, details in obj_prop_mappings.items():
        target_class_name = details.get('target_class')
        col_name = details.get('column') # For linking via ID lookup in GLOBAL registry
        link_context_key = details.get('target_link_context') # For linking via key lookup in CURRENT row context

        if not target_class_name:
            logger.warning(f"Object property mapping for {entity_name}.{prop_name} is missing 'target_class'. Skipping link.")
            continue

        prop = context.get_prop(prop_name)
        if not prop or not isinstance(prop, ObjectPropertyClass): # Ensure it's an ObjectProperty
            logger.warning(f"Object property '{prop_name}' not found or not an ObjectProperty. Skipping link for {entity_name} {individual.name}.")
            continue

        # Add debug for EventRecord.involvesResource specifically
        if entity_name == "EventRecord" and prop_name == "involvesResource":
            if hasattr(individual, "involvesResource") and individual.involvesResource:
                logger.debug(f"EventRecord {individual.name} already has involvesResource set to {individual.involvesResource.name if hasattr(individual.involvesResource, 'name') else individual.involvesResource}")
                # Skip this property if already set
                continue

        # Find the target individual
        target_individual: Optional[Thing] = None
        lookup_method = "None"

        if col_name:
            # --- Link via Column Lookup (using GLOBAL registry) ---
            target_base_id = safe_cast(row.get(col_name), str)
            lookup_method = f"Column '{col_name}' (Registry Lookup)"
            if not target_base_id:
                logger.debug(f"Row {row.get('row_num', 'N/A')} - No target ID found in column '{col_name}' for link {entity_name}.{prop_name}. Skipping link.")
                continue

            # Find target in the GLOBAL registry
            registry_key = (target_class_name, target_base_id)
            target_individual = linking_context.get(registry_key)
            if not target_individual:
                 logger.warning(f"Link target {target_class_name} with ID '{target_base_id}' (from {lookup_method}) not found in global registry for relation {entity_name}.{prop_name}. Skipping link for {individual.name}.")
                 continue
            else:
                 logger.debug(f"Found link target {target_individual.name} for {entity_name}.{prop_name} via registry key {registry_key}.")

        elif link_context_key:
             # --- Link via Context Key (using CURRENT row's individuals) ---
             lookup_method = f"Context Key '{link_context_key}' (Row Lookup)"
             # Ensure individuals_in_row is provided and is a dictionary
             if not isinstance(individuals_in_row, dict):
                 logger.warning(f"Cannot link via context key '{link_context_key}' for {entity_name}.{prop_name}: individuals_in_row dictionary was not provided or invalid for row {row.get('row_num', 'N/A')}. Skipping link.")
                 continue

             target_individual = individuals_in_row.get(link_context_key)
             if not target_individual:
                 # Track missing context entity to log only once
                 missing_key = f"{link_context_key} for {entity_name}.{prop_name}"
                 if missing_key not in missing_context_entities:
                     missing_context_entities.add(missing_key)
                     logger.warning(f"Context entity '{link_context_key}' required for {entity_name}.{prop_name} not found in individuals_in_row dictionary for row {row.get('row_num', 'N/A')}. Skipping link.")
                 continue
             else:
                 logger.debug(f"Found link target {target_individual.name} for {entity_name}.{prop_name} via row context key '{link_context_key}'.")

        else:
            # Should not happen if parser validation is correct
            logger.error(f"Invalid mapping for object property {entity_name}.{prop_name}: Missing both 'column' and 'target_link_context'. Skipping.")
            continue

        # --- Type Check and Set Property ---
        if target_individual:
            target_cls = context.get_class(target_class_name)
            # Check if the found individual is an instance of the target class (or subclass)
            if not target_cls or not isinstance(target_individual, target_cls):
                 logger.error(f"Type mismatch for link {entity_name}.{prop_name}: Expected {target_class_name} but found target '{target_individual.name}' of type {type(target_individual).__name__} via {lookup_method}. Skipping link.")
                 continue

            # Set the property
            context.set_prop(individual, prop_name, target_individual)
            links_applied_count += 1
            
            # Add specific debug for important links
            if entity_name == "EventRecord":
                if prop_name == "involvesResource":
                    logger.debug(f"Successfully linked EventRecord {individual.name} to resource {target_individual.name} via {prop_name}")

    # logger.debug(f"Applied {links_applied_count} object property links for {entity_name} individual {individual.name}. Row {row.get('row_num', 'N/A')}.")


# --- DEPRECATED - Combined function (keep for reference temporarily?) ---


===========================================
FILE: ontology_generator/population/equipment.py
===========================================

"""
Equipment population module for the ontology generator.

This module provides functions for processing equipment data.
"""
import re
from typing import Dict, Any, Optional, Tuple

from owlready2 import Thing

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, 
    apply_data_property_mappings, apply_object_property_mappings
)
from ontology_generator.config import DEFAULT_EQUIPMENT_SEQUENCE

def parse_equipment_class(equipment_name: Optional[str]) -> Optional[str]:
    """
    Parses the EquipmentClass from the EQUIPMENT_NAME.
    
    Rules:
    1. Extracts the part after the last underscore
    2. Removes trailing digits from class name to handle instance identifiers
    3. Validates the resulting class name has letters
    4. Falls back to appropriate alternatives if validation fails
    
    Args:
        equipment_name: The equipment name to parse
        
    Returns:
        The parsed equipment class name or None
    
    Examples:
    - FIPCO009_Filler -> Filler
    - FIPCO009_Filler2 -> Filler
    - FIPCO009_CaseFormer3 -> CaseFormer
    - FIPCO009_123 -> FIPCO009 (fallback to part before underscore if after is all digits)
    """
    if not equipment_name or not isinstance(equipment_name, str):
        return None

    if '_' in equipment_name:
        parts = equipment_name.split('_')
        class_part = parts[-1]

        # Try to extract base class name by removing trailing digits
        base_class = re.sub(r'\d+$', '', class_part)

        # Validate the base class name
        if base_class and re.search(r'[a-zA-Z]', base_class):
            pop_logger.debug(f"Parsed equipment class '{base_class}' from '{equipment_name}' (original part: '{class_part}')")
            return base_class
        else:
            # If stripping digits results in empty/invalid class, try the part before underscore
            if len(parts) > 1 and re.search(r'[a-zA-Z]', parts[-2]):
                fallback_class = parts[-2]
                pop_logger.warning(f"Class part '{class_part}' became invalid after stripping digits. Using fallback from previous part: '{fallback_class}'")
                return fallback_class
            else:
                # Last resort: use original class_part if it has letters, otherwise whole name
                if re.search(r'[a-zA-Z]', class_part):
                    pop_logger.warning(f"Using original class part '{class_part}' as class name (could not extract better alternative)")
                    return class_part
                else:
                    pop_logger.warning(f"No valid class name found in parts of '{equipment_name}'. Using full name as class.")
                    return equipment_name

    # No underscore case
    if re.search(r'[a-zA-Z]', equipment_name):
        # If the full name has letters, try to extract base class by removing trailing digits
        base_class = re.sub(r'\d+$', '', equipment_name)
        if base_class and re.search(r'[a-zA-Z]', base_class):
            pop_logger.debug(f"Extracted base class '{base_class}' from non-underscore name '{equipment_name}'")
            return base_class
        else:
            pop_logger.debug(f"Using full name '{equipment_name}' as class (no underscore, has letters)")
            return equipment_name
    else:
        pop_logger.warning(f"Equipment name '{equipment_name}' contains no letters. Using as is.")
        return equipment_name

def process_equipment_and_class(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: Dict[Tuple[str, str], Thing] = None,
    line_ind: Optional[Thing] = None,
    pass_num: int = 1
) -> Tuple[Optional[Thing], Optional[Thing], Optional[Tuple]]:
    """
    Processes Equipment and its associated EquipmentClass from a row.

    Pass 1: Creates individuals, applies data properties, adds to registry, collects class info.
    Pass 2: (Currently not handled here, linking done by apply_object_property_mappings)

    Relies on property_mappings for Equipment and EquipmentClass.
    Assumes IDs for Equipment and EquipmentClass can be found via mappings.

    Args:
        row: The data row.
        context: Population context.
        property_mappings: Property mappings dictionary.
        all_created_individuals_by_uid: Central individual registry.
        line_ind: The ProductionLine individual associated with this row (for context).
        pass_num: The current population pass (1 or 2).

    Returns:
        Tuple: (equipment_individual, equipment_class_individual, equipment_class_info)
               - equipment_individual: The created/retrieved Equipment individual.
               - equipment_class_individual: The created/retrieved EquipmentClass individual.
               - equipment_class_info: Tuple (class_name, class_ind, position) for tracking.
    """
    if not property_mappings:
        pop_logger.warning("Property mappings not provided to process_equipment_and_class. Skipping.")
        return None, None, None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_equipment_and_class. Skipping.")
         return None, None, None

    # Get classes
    cls_Equipment = context.get_class("Equipment")
    cls_EquipmentClass = context.get_class("EquipmentClass")
    if not cls_Equipment or not cls_EquipmentClass:
        pop_logger.error("Essential classes 'Equipment' or 'EquipmentClass' not found. Cannot process equipment.")
        return None, None, None

    equipment_ind: Optional[Thing] = None
    eq_class_ind: Optional[Thing] = None
    eq_class_info_out: Optional[Tuple] = None

    # --- 1. Process Equipment Class ---
    # Equipment Class is often needed before Equipment (for linking memberOfClass)
    # We need a unique identifier for the class. Often the name itself, or a dedicated ID column.

    # Attempt 1: Use equipmentClassId property mapping
    eq_class_id_map = property_mappings.get('EquipmentClass', {}).get('data_properties', {}).get('equipmentClassId')
    eq_class_name_map = property_mappings.get('EquipmentClass', {}).get('data_properties', {}).get('equipmentClassName') # Optional name
    eq_class_col = None
    eq_class_id_value = None # Raw value from the determined column
    eq_class_base_name = None # The final name/ID to use for the class individual
    is_parsed_from_name = False # Flag to track if we used parsing

    if eq_class_id_map and eq_class_id_map.get('column'):
        eq_class_col = eq_class_id_map['column']
        eq_class_id_value = safe_cast(row.get(eq_class_col), str) # Get raw value
        pop_logger.debug(f"Found EquipmentClass.equipmentClassId mapping to column '{eq_class_col}'")
        # *** MODIFIED LOGIC: Check if we need to parse from EQUIPMENT_NAME ***
        if eq_class_col == 'EQUIPMENT_NAME':
            pop_logger.debug(f"Parsing EquipmentClass from EQUIPMENT_NAME column value: '{eq_class_id_value}'")
            eq_class_base_name = parse_equipment_class(eq_class_id_value)
            is_parsed_from_name = True
            if not eq_class_base_name:
                 pop_logger.warning(f"Parsing EQUIPMENT_NAME '{eq_class_id_value}' failed to yield a class name.")
                 # Skip further class processing if parsing fails
                 return None, None, None
            else:
                 pop_logger.debug(f"Parsed equipment class name: '{eq_class_base_name}'")
        else:
            # Use the value directly if column is not EQUIPMENT_NAME
            eq_class_base_name = eq_class_id_value
            pop_logger.debug(f"Using direct value '{eq_class_base_name}' from column '{eq_class_col}' as class ID.")

    else:
        # Attempt 2: Fallback to using equipmentClassName property mapping as the ID source
        pop_logger.debug("No mapping found for equipmentClassId, trying equipmentClassName as fallback ID source.")
        if eq_class_name_map and eq_class_name_map.get('column'):
            eq_class_col = eq_class_name_map['column']
            eq_class_id_value = safe_cast(row.get(eq_class_col), str)
            eq_class_base_name = eq_class_id_value # Use directly when falling back to name column
            pop_logger.debug(f"Falling back to EquipmentClass.equipmentClassName mapping (column '{eq_class_col}') for class ID: '{eq_class_base_name}'.")
        else:
            pop_logger.warning("Cannot determine column for EquipmentClass ID (tried equipmentClassId, equipmentClassName). Skipping EquipmentClass processing.")
            # Cannot proceed without class ID
            return None, None, None

    # Check if we obtained a base name
    if not eq_class_base_name:
        pop_logger.warning(f"Missing or invalid EquipmentClass ID/Name from column '{eq_class_col}' (value: '{eq_class_id_value}'). Skipping EquipmentClass processing.")
        return None, None, None

    # Now use eq_class_base_name for creating the individual
    eq_class_labels = [eq_class_base_name]
    # Optionally add the original EQUIPMENT_NAME as a label if we parsed
    if is_parsed_from_name and eq_class_id_value and eq_class_id_value != eq_class_base_name:
        if eq_class_id_value not in eq_class_labels:
            eq_class_labels.append(f"Source Name: {eq_class_id_value}")
    # Optionally add name from dedicated name column if different from ID source and base name
    elif eq_class_name_map and eq_class_name_map.get('column') != eq_class_col:
        class_name_val = safe_cast(row.get(eq_class_name_map['column']), str)
        if class_name_val and class_name_val not in eq_class_labels:
             eq_class_labels.append(class_name_val)

    eq_class_ind = get_or_create_individual(cls_EquipmentClass, eq_class_base_name, context.onto, all_created_individuals_by_uid, add_labels=eq_class_labels)

    eq_class_pos = None
    if eq_class_ind and pass_num == 1 and "EquipmentClass" in property_mappings:
        apply_data_property_mappings(eq_class_ind, property_mappings["EquipmentClass"], row, context, "EquipmentClass", pop_logger)
        # Extract sequence position after applying data properties
        pos_prop_name = "defaultSequencePosition"
        if context.get_prop(pos_prop_name):
            try:
                # Use safe_cast directly on the potential attribute value
                # getattr might return None or the value
                raw_pos = getattr(eq_class_ind, pos_prop_name, None)
                eq_class_pos = safe_cast(raw_pos, int) if raw_pos is not None else None
                
                # If no position set, try using the default from config
                if eq_class_pos is None and eq_class_base_name in DEFAULT_EQUIPMENT_SEQUENCE:
                    default_pos = DEFAULT_EQUIPMENT_SEQUENCE.get(eq_class_base_name)
                    pop_logger.info(f"Using default sequence position {default_pos} for equipment class '{eq_class_base_name}' from config.DEFAULT_EQUIPMENT_SEQUENCE")
                    # Set the position in the individual
                    context.set_prop(eq_class_ind, pos_prop_name, default_pos)
                    eq_class_pos = default_pos
                elif eq_class_pos is None:
                    pop_logger.debug(f"No sequence position available for equipment class '{eq_class_base_name}' (not in config defaults)")
            except Exception as e:
                 pop_logger.warning(f"Could not read or cast {pos_prop_name} for {eq_class_ind.name}: {e}")
        # Prepare info for tracking
        eq_class_info_out = (eq_class_base_name, eq_class_ind, eq_class_pos)

    elif not eq_class_ind:
        pop_logger.warning(f"Failed to create/retrieve EquipmentClass individual for base '{eq_class_base_name}'. Equipment processing might be incomplete.")
        # Continue to process Equipment if possible, but linking to class will fail later

    # --- 2. Process Equipment --- 
    # Equipment needs an ID
    equip_id_map = property_mappings.get('Equipment', {}).get('data_properties', {}).get('equipmentId')
    if not equip_id_map or not equip_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for Equipment.equipmentId from property mappings. Skipping Equipment creation.")
        return None, eq_class_ind, eq_class_info_out # Return class if created
    equip_id_col = equip_id_map['column']
    equip_id = safe_cast(row.get(equip_id_col), str)
    if not equip_id:
        pop_logger.warning(f"Missing or invalid Equipment ID in column '{equip_id_col}'. Skipping Equipment creation.")
        return None, eq_class_ind, eq_class_info_out # Return class if created

    # Equipment name needs context (line) for uniqueness if ID is not globally unique
    # Using LineID_EquipmentID as base name
    line_id_str = line_ind.lineId[0] if line_ind and hasattr(line_ind, 'lineId') and line_ind.lineId else "UnknownLine"
    equip_unique_base = f"{line_id_str}_{equip_id}"
    
    # Try to get name for label
    equip_name_map = property_mappings.get('Equipment', {}).get('data_properties', {}).get('equipmentName')
    equip_name = None
    if equip_name_map and equip_name_map.get('column'):
         equip_name = safe_cast(row.get(equip_name_map['column']), str)
         
    equip_labels = [equip_id]
    if equip_name:
        equip_labels.append(equip_name)
    else: # Use unique base if name missing
        equip_labels.append(equip_unique_base) 
        
    equipment_ind = get_or_create_individual(cls_Equipment, equip_unique_base, context.onto, all_created_individuals_by_uid, add_labels=equip_labels)

    if equipment_ind and pass_num == 1 and "Equipment" in property_mappings:
        # Linking (isPartOfProductionLine, memberOfClass, isUpstreamOf etc.) happens in Pass 2
        apply_data_property_mappings(equipment_ind, property_mappings["Equipment"], row, context, "Equipment", pop_logger)

        # --- Special Handling for memberOfClass in Pass 1? ---
        # While full linking is in Pass 2, memberOfClass is crucial and links to the class *we just processed*.
        # It might be beneficial to set this single object property here if the class exists.
        # However, the generic apply_object_property_mappings in Pass 2 should handle it via the registry.
        # Let's stick to the strict separation for now.
        # if eq_class_ind and context.get_prop("memberOfClass"):
        #     if not getattr(equipment_ind, "memberOfClass", None): # Set only if not already set (e.g., by prior run)
        #         context.set_prop(equipment_ind, "memberOfClass", eq_class_ind)
        #         pop_logger.debug(f"Set memberOfClass link for {equipment_ind.name} to {eq_class_ind.name} during Pass 1.")
        # elif not eq_class_ind:
        #      pop_logger.warning(f"Cannot set memberOfClass for {equipment_ind.name} in Pass 1: EquipmentClass individual is missing.")
        pass # Defer memberOfClass linking to Pass 2

    elif not equipment_ind:
         pop_logger.warning(f"Failed to create/retrieve Equipment individual for base '{equip_unique_base}'.")
         # Return class if created
         return None, eq_class_ind, eq_class_info_out

    return equipment_ind, eq_class_ind, eq_class_info_out


===========================================
FILE: ontology_generator/population/events.py
===========================================

"""
Events population module for the ontology generator.

This module provides functions for processing event-related data.
"""
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List

from owlready2 import Thing, locstr

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, apply_data_property_mappings
)
from ontology_generator.config import COUNTRY_TO_LANGUAGE, DEFAULT_LANGUAGE
from ontology_generator.population.linking import link_equipment_events_to_line_events

# Type Alias for registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object
RowIndividuals = Dict[str, Thing] # Key: entity_type_str, Value: Individual Object for this row

def process_shift(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes Shift from a row (Pass 1: Create/Data Props).
    Requires shiftId, startTime, endTime mappings.
    """
    if not property_mappings or "Shift" not in property_mappings:
        pop_logger.debug("Property mappings for 'Shift' not provided. Skipping shift processing.")
        return None
    if all_created_individuals_by_uid is None: return None # Error logged upstream

    cls_Shift = context.get_class("Shift")
    if not cls_Shift: return None

    shift_id_map = property_mappings['Shift'].get('data_properties', {}).get('shiftId')
    start_time_map = property_mappings['Shift'].get('data_properties', {}).get('shiftStartTime')
    end_time_map = property_mappings['Shift'].get('data_properties', {}).get('shiftEndTime')

    if not shift_id_map or not shift_id_map.get('column'):
        pop_logger.warning("Mapping for Shift.shiftId column not found. Skipping shift.")
        return None
    # Start/end times are crucial for identification/labeling if ID isn't unique
    if not start_time_map or not start_time_map.get('column') or not end_time_map or not end_time_map.get('column'):
         pop_logger.warning("Mapping for Shift start/end time columns not found. Skipping shift.")
         return None

    shift_id_col = shift_id_map['column']
    shift_id = safe_cast(row.get(shift_id_col), str)
    start_time_str = safe_cast(row.get(start_time_map['column']), str)
    end_time_str = safe_cast(row.get(end_time_map['column']), str)

    if not shift_id or not start_time_str:
        pop_logger.debug(f"Missing shift ID ('{shift_id_col}') or start time ('{start_time_map['column']}') in row. Skipping shift.")
        return None

    # Create a unique base name, e.g., ShiftID_StartTime
    shift_unique_base = f"{shift_id}_{start_time_str}"
    shift_labels = [shift_id, f"{start_time_str} to {end_time_str or '?'}"]

    shift_ind = get_or_create_individual(cls_Shift, shift_unique_base, context.onto, all_created_individuals_by_uid, add_labels=shift_labels)

    if shift_ind and pass_num == 1:
        apply_data_property_mappings(shift_ind, property_mappings["Shift"], row, context, "Shift", pop_logger)
        # Potential: Calculate duration if mapping exists? Or leave to reasoner/post-processing?
        # duration_map = property_mappings['Shift'].get('data_properties', {}).get('shiftDurationMinutes')
        # if duration_map ... calculate ... context.set_prop(...)

    return shift_ind

def process_state(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes OperationalState from a row (Pass 1: Create/Data Props).
    Requires stateDescription mapping.
    """
    if not property_mappings or "OperationalState" not in property_mappings:
        pop_logger.debug("Property mappings for 'OperationalState' not provided. Skipping state processing.")
        return None
    if all_created_individuals_by_uid is None: return None

    cls_State = context.get_class("OperationalState")
    if not cls_State: return None

    state_desc_map = property_mappings['OperationalState'].get('data_properties', {}).get('stateDescription')
    if not state_desc_map or not state_desc_map.get('column'):
        pop_logger.warning("Mapping for OperationalState.stateDescription column not found. Skipping state.")
        return None

    state_desc_col = state_desc_map['column']
    state_desc = safe_cast(row.get(state_desc_col), str)
    if not state_desc:
        pop_logger.debug(f"No State Description found in column '{state_desc_col}'. Skipping state.")
        return None

    # Use description as the base name (assuming descriptions are reasonably unique states)
    state_unique_base = state_desc
    state_labels = [state_desc]

    state_ind = get_or_create_individual(cls_State, state_unique_base, context.onto, all_created_individuals_by_uid, add_labels=state_labels)

    if state_ind and pass_num == 1:
        apply_data_property_mappings(state_ind, property_mappings["OperationalState"], row, context, "OperationalState", pop_logger)

    return state_ind

def process_reason(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes OperationalReason from a row (Pass 1: Create/Data Props).
    Requires reasonDescription or altReasonDescription mapping.
    """
    if not property_mappings or "OperationalReason" not in property_mappings:
        pop_logger.debug("Property mappings for 'OperationalReason' not provided. Skipping reason processing.")
        return None
    if all_created_individuals_by_uid is None: return None

    cls_Reason = context.get_class("OperationalReason")
    if not cls_Reason: return None

    reason_desc_map = property_mappings['OperationalReason'].get('data_properties', {}).get('reasonDescription')
    alt_reason_desc_map = property_mappings['OperationalReason'].get('data_properties', {}).get('altReasonDescription')

    reason_desc_col = None
    reason_desc = None

    if reason_desc_map and reason_desc_map.get('column'):
        reason_desc_col = reason_desc_map['column']
        reason_desc = safe_cast(row.get(reason_desc_col), str)
    elif alt_reason_desc_map and alt_reason_desc_map.get('column'):
        reason_desc_col = alt_reason_desc_map['column']
        reason_desc = safe_cast(row.get(reason_desc_col), str)
        pop_logger.debug(f"Using altReasonDescription column '{reason_desc_col}' for reason.")
    else:
        pop_logger.warning("Mapping for OperationalReason description column (reasonDescription or altReasonDescription) not found. Skipping reason.")
        return None

    if not reason_desc:
        pop_logger.debug(f"No Reason Description found in column '{reason_desc_col}'. Skipping reason.")
        return None

    # Use description as the base name
    reason_unique_base = reason_desc
    reason_labels = [reason_desc]

    reason_ind = get_or_create_individual(cls_Reason, reason_unique_base, context.onto, all_created_individuals_by_uid, add_labels=reason_labels)

    if reason_ind and pass_num == 1:
        apply_data_property_mappings(reason_ind, property_mappings["OperationalReason"], row, context, "OperationalReason", pop_logger)

    return reason_ind

def process_time_interval(
    row: Dict[str, Any],
    context: PopulationContext,
    resource_base_id: str,
    row_num: int,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes TimeInterval from a row (Pass 1: Create/Data Props).
    Requires startTime mapping. endTime mapping is needed for the property but not strictly for creation.
    Uses resource_base_id and row_num for unique naming.
    """
    if not property_mappings or "TimeInterval" not in property_mappings:
        pop_logger.debug("Property mappings for 'TimeInterval' not provided. Skipping interval processing.")
        return None
    if all_created_individuals_by_uid is None: return None

    cls_Interval = context.get_class("TimeInterval")
    if not cls_Interval: return None

    start_map = property_mappings['TimeInterval'].get('data_properties', {}).get('startTime')
    end_map = property_mappings['TimeInterval'].get('data_properties', {}).get('endTime')

    if not start_map or not start_map.get('column'):
        pop_logger.warning("Mapping for TimeInterval.startTime column not found. Cannot create unique TimeInterval. Skipping interval.")
        return None
    # Relaxed: End time mapping presence check removed for interval creation itself
    # if not end_map or not end_map.get('column'):
    #     pop_logger.warning("Mapping for TimeInterval.endTime column not found. Skipping interval property setting but attempting creation.")
        # Allow creation but property won't be set

    start_col = start_map['column']
    end_col = end_map.get('column') if end_map else None # Safely get end column if mapping exists

    start_time_str = safe_cast(row.get(start_col), str)
    end_time_str = safe_cast(row.get(end_col), str) if end_col else None

    interval_unique_base: str
    interval_labels: List[str]

    if not start_time_str:
        pop_logger.warning(f"Row {row_num}: Missing startTime value from column '{start_col}'. Using fallback naming for TimeInterval based on resource and row number.")
        # Fallback naming strategy
        interval_unique_base = f"Interval_{resource_base_id}_Row{row_num}"
        interval_labels = [f"Interval for {resource_base_id} (Row {row_num}, StartTime Missing)"]
        # Do not return None here, allow creation with missing start time if resource/row are available
    else:
        # Create unique base name using resource, start time, and row number (like reference)
        # Sanitize start_time_str for IRI compatibility if needed (though get_or_create handles basic cases)
        safe_start_time_str = start_time_str.replace(":", "").replace("+", "plus").replace(" ", "T") # Basic sanitization example
        interval_unique_base = f"Interval_{resource_base_id}_{safe_start_time_str}_{row_num}"
        end_label_part = f"to {end_time_str}" if end_time_str else "(No End Time)"
        interval_labels = [f"Interval for {resource_base_id} starting {start_time_str} {end_label_part}"]

    interval_ind = get_or_create_individual(cls_Interval, interval_unique_base, context.onto, all_created_individuals_by_uid, add_labels=interval_labels)

    if interval_ind and pass_num == 1:
        # Apply properties using mappings (will skip endTime if mapping/value missing)
        apply_data_property_mappings(interval_ind, property_mappings["TimeInterval"], row, context, "TimeInterval", pop_logger)

    return interval_ind

def process_event_record(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    # Pass individuals created earlier in the row processing for context if needed
    time_interval_ind: Optional[Thing] = None,
    shift_ind: Optional[Thing] = None,
    state_ind: Optional[Thing] = None,
    reason_ind: Optional[Thing] = None,
    equipment_ind: Optional[Thing] = None, # The primary resource involved
    line_ind: Optional[Thing] = None, # The line context
    material_ind: Optional[Thing] = None, # Optional context
    request_ind: Optional[Thing] = None, # Optional context
    pass_num: int = 1,
    row_num: int = -1
) -> Tuple[Optional[Thing], Optional[Tuple]]:
    """
    Processes EventRecord from a row (Pass 1: Create/Data Props).
    Links to other entities (State, Reason, Shift, Resource, etc.) are deferred to Pass 2.

    Uses hints like 'EQUIPMENT_TYPE' column to determine if it's a line or equipment event.
    Requires mappings for startTime, endTime (to identify interval).

    Returns:
        Tuple: (event_individual, event_context_for_linking)
               event_context_for_linking: (event_ind, primary_resource_ind, time_interval_ind, line_ind_for_context)
    """
    if not property_mappings or "EventRecord" not in property_mappings:
        pop_logger.debug("Property mappings for 'EventRecord' not provided. Skipping event record processing.")
        return None, None
    if all_created_individuals_by_uid is None: return None, None

    cls_Event = context.get_class("EventRecord")
    if not cls_Event: return None, None

    # --- Determine Primary Resource --- 
    # Use EQUIPMENT_TYPE column as a hint (based on linking log errors)
    resource_type_hint = row.get('EQUIPMENT_TYPE', 'Equipment').strip()
    primary_resource_ind = None
    resource_id_for_name = None

    if resource_type_hint == 'Line':
        if line_ind:
            primary_resource_ind = line_ind
            # Try to get a stable ID for the name
            resource_id_for_name = line_ind.lineId[0] if hasattr(line_ind, 'lineId') and line_ind.lineId else line_ind.name
            pop_logger.debug(f"Identified event for Line: {resource_id_for_name}")
        else:
            pop_logger.warning(f"Row indicates a Line event based on '{resource_type_hint}', but no Line individual found. Skipping event.")
            return None, None
    # Default to Equipment if hint is 'Equipment' or unknown/fallback
    else: 
        if resource_type_hint != 'Equipment':
             pop_logger.debug(f"Unknown resource type hint '{resource_type_hint}' in row. Defaulting event resource to Equipment.")
        
        if equipment_ind:
            primary_resource_ind = equipment_ind
            # Try to get a stable ID for the name
            resource_id_for_name = equipment_ind.equipmentId[0] if hasattr(equipment_ind, 'equipmentId') and equipment_ind.equipmentId else equipment_ind.name
            pop_logger.debug(f"Identified event for Equipment: {resource_id_for_name}")
        else:
            # If it should be an equipment event but no equipment_ind, we cannot proceed
            pop_logger.warning(f"Row indicates an Equipment event (type: '{resource_type_hint}'), but no Equipment individual found. Skipping event.")
            return None, None

    # --- Check Dependencies (Time Interval) --- 
    if not time_interval_ind:
         pop_logger.warning("TimeInterval individual not provided for event record. Skipping event.")
         return None, None

    # Extract start time from the interval for the unique name
    start_time_str = None
    # Retrieve start time value directly from the interval individual for naming consistency
    start_time_val = getattr(time_interval_ind, 'startTime', None)
    if isinstance(start_time_val, datetime):
        start_time_str = start_time_val.isoformat()
    elif start_time_val: # Handle if it's stored as string or other type
         start_time_str = str(start_time_val)
    # Fallback if startTime property is missing on the interval
    elif interval_ind := all_created_individuals_by_uid.get(("TimeInterval", time_interval_ind.name)): # Check registry
         start_time_val = getattr(interval_ind, 'startTime', None)
         if isinstance(start_time_val, datetime): start_time_str = start_time_val.isoformat()
         elif start_time_val: start_time_str = str(start_time_val)

    # If start_time_str is *still* None after checks, we cannot reliably name the event
    if start_time_str is None:
        pop_logger.warning(f"Cannot find startTime property value on TimeInterval {time_interval_ind.name}. Using fallback event naming based on interval name.")
        # Use the interval's name itself as part of the event name base
        start_time_str = time_interval_ind.name # Fallback to interval name

    # --- Create Unique ID & Labels --- 
    # Ensure resource_id_for_name was set
    if not resource_id_for_name:
         pop_logger.error(f"Could not determine a valid ID for the primary resource ('{resource_type_hint}'). Skipping event.")
         return None, None

    # Use row_num in the unique base name for robustness
    event_unique_base = f"Event_{resource_id_for_name}_{start_time_str}_{row_num}"
    event_labels = [f"Event for {resource_id_for_name} at {start_time_str} (Row: {row_num})"]
    # Add state/reason descriptions to label if available?
    if state_ind and hasattr(state_ind, 'stateDescription') and state_ind.stateDescription:
        # Handle potential locstr or list
        state_desc = state_ind.stateDescription[0] if isinstance(state_ind.stateDescription, list) else state_ind.stateDescription
        event_labels.append(f"State: {state_desc}")
    if reason_ind and hasattr(reason_ind, 'reasonDescription') and reason_ind.reasonDescription:
        reason_desc = reason_ind.reasonDescription[0] if isinstance(reason_ind.reasonDescription, list) else reason_ind.reasonDescription
        event_labels.append(f"Reason: {reason_desc}")

    # --- Create Individual --- 
    event_ind = get_or_create_individual(cls_Event, event_unique_base, context.onto, all_created_individuals_by_uid, add_labels=event_labels)

    # --- Apply Data Properties & Prepare Context --- 
    event_context_out = None
    if event_ind and pass_num == 1:
        # Apply data properties
        apply_data_property_mappings(event_ind, property_mappings["EventRecord"], row, context, "EventRecord", pop_logger)

        # FIX: Directly link the resource right away during creation (involvesResource property)
        if primary_resource_ind:
            involves_resource_prop = context.get_prop("involvesResource")
            if involves_resource_prop:
                context.set_prop(event_ind, "involvesResource", primary_resource_ind)
                pop_logger.debug(f"Linked event {event_ind.name} directly to resource {primary_resource_ind.name} via involvesResource")

        # Prepare context for later linking steps if needed
        # Use the *determined* primary_resource_ind as the 2nd element
        # The 4th element (line_ind) provides context for where equipment events occurred
        event_context_out = (event_ind, primary_resource_ind, time_interval_ind, line_ind)

    return event_ind, event_context_out


def process_event_related(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    # Pass required context individuals identified earlier in the row
    equipment_ind: Optional[Thing] = None,
    line_ind: Optional[Thing] = None,
    material_ind: Optional[Thing] = None,
    request_ind: Optional[Thing] = None,
    pass_num: int = 1
) -> Tuple[RowIndividuals, Optional[Tuple]]:
    """
    Orchestrates the processing of all event-related individuals for a row in a given pass.

    Pass 1: Creates Shift, TimeInterval, State, Reason, EventRecord. Applies data props.
            Returns created individuals and event context tuple.
    Pass 2: (No actions needed here - linking done externally via apply_object_property_mappings)

    Args:
        row: Data row.
        context: Population context.
        property_mappings: Property mappings.
        all_created_individuals_by_uid: Central individual registry.
        equipment_ind: Equipment individual for this event context.
        line_ind: Line individual for this event context.
        material_ind: Optional material individual for context.
        request_ind: Optional production request for context.
        pass_num: Current population pass.

    Returns:
        Tuple: (created_individuals_dict, event_context_tuple)
                - created_individuals_dict: Dict of event-related individuals created/found.
                - event_context_tuple: Context for later linking steps (from process_event_record).
    """
    created_inds: RowIndividuals = {}
    event_context_out = None

    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_event_related. Skipping.")
        return {}, None

    # Determine resource_base_id needed for interval processing
    # This duplicates some logic from process_event_record but is needed early for interval naming
    resource_base_id = None
    resource_type_hint = row.get('EQUIPMENT_TYPE', 'Equipment').strip()

    if resource_type_hint == 'Line':
        if line_ind:
            resource_base_id = line_ind.lineId[0] if hasattr(line_ind, 'lineId') and line_ind.lineId else line_ind.name
    else: # Default to Equipment
        if equipment_ind:
            resource_base_id = equipment_ind.equipmentId[0] if hasattr(equipment_ind, 'equipmentId') and equipment_ind.equipmentId else equipment_ind.name

    if not resource_base_id:
         pop_logger.warning(f"Row: Could not determine resource_base_id early for interval naming (TypeHint: {resource_type_hint}, Line: {line_ind}, Eq: {equipment_ind}). Using fallback.")
         resource_base_id = f"UnknownResource_{hash(str(row))}" # Example: Use row hash for fallback uniqueness

    # Process in dependency order (roughly)
    shift_ind = process_shift(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if shift_ind: created_inds["Shift"] = shift_ind

    # Call process_time_interval, explicitly passing -1 for row_num
    time_interval_ind = process_time_interval(row, context, resource_base_id, -1, property_mappings, all_created_individuals_by_uid, pass_num)
    if time_interval_ind: created_inds["TimeInterval"] = time_interval_ind

    state_ind = process_state(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if state_ind: created_inds["OperationalState"] = state_ind

    reason_ind = process_reason(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if reason_ind: created_inds["OperationalReason"] = reason_ind

    # --- Process the main EventRecord --- 
    # Call process_event_record, explicitly passing -1 for row_num
    event_ind, event_context_tuple = process_event_record(
        row, context, property_mappings, all_created_individuals_by_uid,
        time_interval_ind=time_interval_ind,
        shift_ind=shift_ind,
        state_ind=state_ind,
        reason_ind=reason_ind,
        equipment_ind=equipment_ind,
        line_ind=line_ind,
        material_ind=material_ind,
        request_ind=request_ind,
        pass_num=pass_num,
        row_num=-1 # Explicitly pass -1
    )
    if event_ind:
        created_inds["EventRecord"] = event_ind
        event_context_out = event_context_tuple # Capture context from successful event creation

    # Only return individuals created/found in this scope
    return created_inds, event_context_out


===========================================
FILE: ontology_generator/population/linking.py
===========================================

"""
Event linking module for the ontology generator.

This module provides functions for linking equipment events to line events.
"""
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple, Set, Optional

from owlready2 import Thing, Ontology, ThingClass, PropertyClass

from ontology_generator.utils.logging import link_logger

def link_equipment_events_to_line_events(onto: Ontology,
                                        created_events_context: List[Tuple[Thing, Thing, Thing, Thing]],
                                        defined_classes: Dict[str, ThingClass],
                                        defined_properties: Dict[str, PropertyClass]) -> int:
    """
    Second pass function to link equipment EventRecords to their containing line EventRecords,
    using relaxed temporal containment logic.
    
    Args:
        onto: The ontology
        created_events_context: List of tuples (event_ind, resource_ind, time_interval_ind, line_ind_associated)
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        
    Returns:
        The number of links created
    """
    link_logger.info("Starting second pass: Linking equipment events to line events (Relaxed Temporal Logic)...")

    # --- Get required classes and properties ---
    cls_EventRecord = defined_classes.get("EventRecord")
    cls_ProductionLine = defined_classes.get("ProductionLine")
    cls_Equipment = defined_classes.get("Equipment")
    prop_isPartOfLineEvent = defined_properties.get("isPartOfLineEvent")
    # Optional: Inverse property
    prop_hasDetailedEquipmentEvent = defined_properties.get("hasDetailedEquipmentEvent")
    prop_startTime = defined_properties.get("startTime")
    prop_endTime = defined_properties.get("endTime")

    if not all([cls_EventRecord, cls_ProductionLine, cls_Equipment, prop_isPartOfLineEvent, prop_startTime, prop_endTime]):
        link_logger.error("Missing essential classes or properties (EventRecord, ProductionLine, Equipment, isPartOfLineEvent, startTime, endTime) for linking. Aborting.")
        return 0  # Return count of links created

    # --- Prepare Lookups ---
    line_events_by_line: Dict[Thing, List[Tuple[Thing, Optional[datetime], Optional[datetime]]]] = defaultdict(list)
    equipment_events_to_link: List[Tuple[Thing, Thing, Optional[datetime], Optional[datetime]]] = []  # (eq_event, line_ind, start, end)

    link_logger.debug("Indexing created events...")
    processed_intervals = 0
    for event_ind, resource_ind, time_interval_ind, associated_line_ind in created_events_context:
        start_time = None
        end_time = None
        if time_interval_ind:
            start_time = getattr(time_interval_ind, prop_startTime.python_name, None)
            end_time = getattr(time_interval_ind, prop_endTime.python_name, None)
            processed_intervals += 1
            # Basic validation: Need at least a start time for meaningful comparison
            if not isinstance(start_time, datetime):
                link_logger.warning(f"Event {event_ind.name} has invalid or missing start time in interval {getattr(time_interval_ind, 'name', 'UnnamedInterval')}. Cannot use for linking.")
                continue  # Skip this event for linking if start time is bad
        else:
            link_logger.warning(f"Event {event_ind.name} has no associated TimeInterval. Cannot use for linking.")
            continue  # Skip this event if no interval

        # Check if it's a line event or equipment event
        if isinstance(resource_ind, cls_ProductionLine):
            if associated_line_ind and associated_line_ind.name == resource_ind.name:
                line_events_by_line[associated_line_ind].append((event_ind, start_time, end_time))
                link_logger.debug(f"Indexed line event {event_ind.name} for line {associated_line_ind.name}")
            else:
                 link_logger.warning(f"Line event {event_ind.name} has mismatch between resource ({resource_ind.name}) and stored associated line ({associated_line_ind.name if associated_line_ind else 'None'}). Skipping indexing.")
        elif isinstance(resource_ind, cls_Equipment):
            if associated_line_ind:
                equipment_events_to_link.append((event_ind, associated_line_ind, start_time, end_time))

    link_logger.info(f"Indexed {len(line_events_by_line)} lines with line events.")
    link_logger.info(f"Found {len(equipment_events_to_link)} equipment events with context to potentially link.")
    if processed_intervals == 0 and (len(line_events_by_line) > 0 or len(equipment_events_to_link) > 0):
        link_logger.warning("Processed events but found 0 valid time intervals. Linking will likely fail.")


    # --- Perform Linking ---
    links_created = 0
    link_logger.info("Attempting to link equipment events to containing line events...")
    with onto:  # Use ontology context for modifications
        for eq_event_ind, line_ind, eq_start, eq_end in equipment_events_to_link:
            potential_parents = line_events_by_line.get(line_ind, [])
            parent_found = False

            # Equipment start time must be valid (already checked during indexing)
            if not isinstance(eq_start, datetime):
                continue

            for line_event_ind, line_start, line_end in potential_parents:
                # Line event start time must be valid
                if not isinstance(line_start, datetime):
                    continue

                # --- Temporal Containment Logic (Modified for Relaxation) ---
                link = False
                link_method = "None"

                # 1. Check for Strict Containment (requires valid start/end for both)
                strict_cond1 = (line_start <= eq_start)
                strict_cond2 = False
                if isinstance(eq_end, datetime) and isinstance(line_end, datetime):
                    strict_cond2 = (eq_end <= line_end)

                if strict_cond1 and strict_cond2:
                    link = True
                    link_method = "Strict Containment"
                    link_logger.debug(f"Potential match via strict containment for {eq_event_ind.name} in {line_event_ind.name}")
                else:
                    # 2. Fallback: Check if eq_start is within the line interval
                    #    (Handles cases where line_end or eq_end might be None)
                    fallback_cond1 = (line_start <= eq_start)  # Eq starts at or after line starts
                    # Eq starts before line ends (or line never ends)
                    fallback_cond2 = (line_end is None or (isinstance(line_end, datetime) and eq_start < line_end))

                    if fallback_cond1 and fallback_cond2:
                        link = True
                        link_method = "Start-Time Containment"
                        link_logger.debug(f"Potential match via start-time containment for {eq_event_ind.name} in {line_event_ind.name}")

                # --- End of Containment Logic ---

                if link:
                    try:
                        # Link: Equipment Event ---isPartOfLineEvent---> Line Event
                        current_parents = getattr(eq_event_ind, prop_isPartOfLineEvent.python_name, [])
                        if not isinstance(current_parents, list): 
                            current_parents = [current_parents] if current_parents is not None else []

                        if line_event_ind not in current_parents:
                            getattr(eq_event_ind, prop_isPartOfLineEvent.python_name).append(line_event_ind)
                            links_created += 1
                            link_logger.info(f"Linked ({link_method}): {eq_event_ind.name} isPartOfLineEvent {line_event_ind.name}")  # Changed to INFO for successful links

                            # Optional: Link inverse if property exists
                            if prop_hasDetailedEquipmentEvent:
                                current_children = getattr(line_event_ind, prop_hasDetailedEquipmentEvent.python_name, [])
                                if not isinstance(current_children, list): 
                                    current_children = [current_children] if current_children is not None else []

                                if eq_event_ind not in current_children:
                                    getattr(line_event_ind, prop_hasDetailedEquipmentEvent.python_name).append(eq_event_ind)
                                    link_logger.debug(f"Linked Inverse: {line_event_ind.name} hasDetailedEquipmentEvent {eq_event_ind.name}")

                            parent_found = True
                            break  # Stop searching for parents for this equipment event
                        else:
                            # Log if the link already existed (useful for debugging duplicates/re-runs)
                            link_logger.debug(f"Link already exists: {eq_event_ind.name} isPartOfLineEvent {line_event_ind.name}. Skipping append.")
                            parent_found = True  # Treat existing link as success
                            break


                    except Exception as e:
                        link_logger.error(f"Error creating link between {eq_event_ind.name} and {line_event_ind.name} (Method: {link_method}): {e}", exc_info=False)

            # Log if no parent was found after checking all candidates
            if not parent_found:
                eq_interval_str = f"{eq_start}" + (f" - {eq_end}" if isinstance(eq_end, datetime) else " - NoEnd")
                line_event_count = len(potential_parents)
                link_logger.warning(f"Could not find suitable containing line event for equipment event {eq_event_ind.name} (Interval: {eq_interval_str}) on line {line_ind.name} (checked {line_event_count} candidates).")


    link_logger.info(f"Finished linking pass. Created {links_created} 'isPartOfLineEvent' relationships.")
    return links_created


===========================================
FILE: ontology_generator/population/processing.py
===========================================

"""
Module for processing individual data rows during ontology population.
"""
import logging
from typing import Any, Dict, Optional, Tuple

from owlready2 import Thing, ThingClass

# Assuming PopulationContext is defined elsewhere and imported appropriately
from .core import PopulationContext
# Assuming processing functions are available
from .asset import process_asset_hierarchy, process_material, process_production_request
from .equipment import process_equipment
from .events import process_shift, process_state_reason, process_time_interval, process_event_record

# Use a logger specific to this module
proc_logger = logging.getLogger(__name__)

# Define a return type structure for clarity
RowProcessingResult = Tuple[
    bool,  # Success status
    Optional[Tuple[Thing, Thing, Thing, Thing]], # event_context: (event_ind, resource_ind, time_interval_ind, line_ind_associated)
    Optional[Tuple[str, Thing, Optional[int]]] # eq_class_info: (eq_class_name, eq_class_ind, position)
]

def process_single_data_row(row: Dict[str, Any],
                            row_num: int,
                            context: PopulationContext,
                            property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None) \
                            -> RowProcessingResult:
    """
    Processes a single data row to create ontology individuals and relationships.

    Args:
        row: The data dictionary for the current row.
        row_num: The original row number (for logging).
        context: The PopulationContext object.
        property_mappings: The parsed property mappings.

    Returns:
        A tuple containing:
        - bool: True if processing was successful, False otherwise.
        - Optional[Tuple]: Event context tuple (event_ind, resource_ind, time_interval_ind, line_ind_associated)
                         if an event was successfully created and linked, otherwise None.
        - Optional[Tuple]: Equipment class info (name, individual, position) if relevant,
                         otherwise None.
    """
    proc_logger.debug(f"--- Processing Row {row_num} ---")
    try:
        # 1. Process Asset Hierarchy -> plant, area, pcell, line individuals
        plant_ind, area_ind, pcell_ind, line_ind = process_asset_hierarchy(row, context, property_mappings)
        if not plant_ind:  # Plant is essential
            raise ValueError("Failed to establish Plant individual, cannot proceed with row.")

        # 2. Determine Resource (Line or Equipment) for the Event
        eq_type = row.get('EQUIPMENT_TYPE', '')
        resource_individual: Optional[Thing] = None
        resource_base_id: Optional[str] = None  # For naming related individuals
        equipment_ind: Optional[Thing] = None
        eq_class_ind: Optional[ThingClass] = None
        eq_class_name: Optional[str] = None
        eq_class_pos: Optional[int] = None
        eq_class_info_result: Optional[Tuple[str, Thing, Optional[int]]] = None

        if eq_type == 'Line' and line_ind:
            resource_individual = line_ind
            resource_base_id = line_ind.name
            proc_logger.debug(f"Row {row_num}: Identified as Line record for: {line_ind.name}")

        elif eq_type == 'Equipment':
            equipment_ind, eq_class_ind, eq_class_name = process_equipment(row, context, line_ind, property_mappings)
            if equipment_ind:
                resource_individual = equipment_ind
                resource_base_id = f"Eq_{equipment_ind.name}"
                if eq_class_ind and eq_class_name:
                    # Attempt to get position; getattr returns None if attribute doesn't exist
                    pos_val = getattr(eq_class_ind, "defaultSequencePosition", None)
                    # Ensure position is an integer if found
                    eq_class_pos = int(pos_val) if isinstance(pos_val, (int, float, str)) and str(pos_val).isdigit() else None
                    eq_class_info_result = (eq_class_name, eq_class_ind, eq_class_pos)
                    proc_logger.debug(f"Row {row_num}: Processed Equipment {equipment_ind.name} of class {eq_class_name} (Pos: {eq_class_pos})")
            else:
                 proc_logger.warning(f"Row {row_num}: Identified as Equipment record, but failed to process Equipment individual. Event linkages might be incomplete.")

        else:
            proc_logger.warning(f"Row {row_num}: Could not determine resource. EQUIPMENT_TYPE='{eq_type}', EQUIPMENT_ID='{row.get('EQUIPMENT_ID')}', LINE_NAME='{row.get('LINE_NAME')}'. Event linkages might be incomplete.")

        if not resource_individual:
            proc_logger.error(f"Row {row_num}: No valid resource (Line or Equipment) identified. Cannot link event record correctly.")
            resource_base_id = f"UnknownResource_Row{row_num}" # Fallback for naming
            # Continue processing other parts, but event linking will fail later

        # 3. Process Material
        material_ind = process_material(row, context, property_mappings)

        # 4. Process Production Request
        request_ind = process_production_request(row, context, property_mappings)

        # 5. Process Shift
        shift_ind = process_shift(row, context, property_mappings)

        # 6. Process State & Reason
        state_ind, reason_ind = process_state_reason(row, context, property_mappings)

        # 7. Process Time Interval
        time_interval_ind = process_time_interval(row, context, resource_base_id, row_num, property_mappings)

        # 8. Process Event Record and Links
        event_ind: Optional[Thing] = None
        event_context_result: Optional[Tuple[Thing, Thing, Thing, Thing]] = None
        if resource_individual and time_interval_ind: # Need resource and interval for meaningful event
            event_ind = process_event_record(row, context, resource_individual, resource_base_id, row_num,
                                             request_ind, material_ind, time_interval_ind,
                                             shift_ind, state_ind, reason_ind, property_mappings)
            if not event_ind:
                raise ValueError("Failed to create EventRecord individual.")
            else:
                # Determine associated line for linking context
                associated_line_ind: Optional[Thing] = None
                prod_line_class = context.get_class("ProductionLine")
                equipment_class = context.get_class("Equipment")
                part_of_prop = context.get_prop("isPartOfProductionLine")

                if prod_line_class and isinstance(resource_individual, prod_line_class):
                    associated_line_ind = resource_individual
                elif equipment_class and part_of_prop and isinstance(resource_individual, equipment_class):
                    # Safely access potentially multi-valued property
                    line_val = getattr(resource_individual, part_of_prop.python_name, None)
                    if isinstance(line_val, list) and line_val:
                        associated_line_ind = line_val[0] # Take first if multiple
                    elif line_val and not isinstance(line_val, list):
                        associated_line_ind = line_val # Assign if single value

                # Check if associated_line_ind is indeed a ProductionLine instance
                if prod_line_class and isinstance(associated_line_ind, prod_line_class):
                    event_context_result = (event_ind, resource_individual, time_interval_ind, associated_line_ind)
                    proc_logger.debug(f"Row {row_num}: Stored context for Event {event_ind.name} (Resource: {resource_individual.name}, Line: {associated_line_ind.name})")
                else:
                    proc_logger.warning(f"Row {row_num}: Could not determine associated ProductionLine for Event {event_ind.name} (Resource: {resource_individual.name}). Skipping context for isPartOfLineEvent linking.")
        elif not resource_individual:
             proc_logger.warning(f"Row {row_num}: Skipping EventRecord creation as no valid resource individual was found.")
        elif not time_interval_ind:
             proc_logger.warning(f"Row {row_num}: Skipping EventRecord creation as no valid time interval individual was found or created.")

        # Return success and any gathered context/info
        return True, event_context_result, eq_class_info_result

    except (KeyError, ValueError, TypeError, AttributeError) as specific_err:
        # Log specific errors with traceback
        proc_logger.error(f"Specific error processing data row {row_num} (Type: {type(specific_err).__name__}): {row if len(str(row)) < 500 else str(row)[:500] + '...'}", exc_info=True)
        return False, None, None # Indicate failure
    except Exception as e:
        # Log unexpected errors with traceback
        proc_logger.error(f"An unexpected error processing data row {row_num}: {row if len(str(row)) < 500 else str(row)[:500] + '...'}", exc_info=True)
        return False, None, None # Indicate failure

===========================================
FILE: ontology_generator/population/row_processor.py
===========================================

import logging
from typing import Any, Dict, Optional, Tuple, List
from owlready2 import Thing

# Assuming imports for processing functions from other modules
from .asset import process_asset_hierarchy, process_material, process_production_request
from .equipment import process_equipment_and_class
from .events import process_event_related
# Need Person processing if added?
# from .person import process_person

# Import core components needed
from .core import PopulationContext, apply_object_property_mappings

# Logger setup
row_proc_logger = logging.getLogger(__name__)

# Type Alias for the central registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object
RowIndividuals = Dict[str, Thing] # Key: entity_type_str, Value: Individual Object for this row


def process_single_data_row_pass1(
    row: Dict[str, Any],
    row_num: int,
    context: PopulationContext,
    property_mappings: Dict[str, Dict[str, Dict[str, Any]]],
    all_created_individuals_by_uid: IndividualRegistry
) -> Tuple[bool, RowIndividuals, Optional[Tuple], Optional[Tuple]]:
    """
    Processes a single data row during Pass 1: Creates individuals and applies data properties.

    Args:
        row: The data row dictionary.
        row_num: The original row number (for logging).
        context: The PopulationContext.
        property_mappings: The parsed property mappings.
        all_created_individuals_by_uid: The central registry to populate and use for get_or_create.

    Returns:
        Tuple: (success_flag, created_individuals_in_row, event_context_tuple, eq_class_info_tuple)
               - success_flag (bool): True if the essential parts of the row were processed.
               - created_individuals_in_row (Dict[str, Thing]): Individuals created/retrieved for this row.
               - event_context_tuple (Optional[Tuple]): Context for linking events later.
               - eq_class_info_tuple (Optional[Tuple]): Info for equipment class tracking.
    """
    row_proc_logger.debug(f"Row {row_num} - Pass 1 Start")
    created_inds_this_row: RowIndividuals = {}
    event_context = None
    eq_class_info = None
    success = True # Assume success unless critical failure

    try:
        # --- 1. Process Asset Hierarchy (Plant, Area, ProcessCell, ProductionLine) ---
        plant_ind, area_ind, pcell_ind, line_ind = process_asset_hierarchy(
            row, context, property_mappings, all_created_individuals_by_uid, pass_num=1
        )
        if plant_ind: created_inds_this_row["Plant"] = plant_ind
        if area_ind: created_inds_this_row["Area"] = area_ind
        if pcell_ind: created_inds_this_row["ProcessCell"] = pcell_ind
        if line_ind: created_inds_this_row["ProductionLine"] = line_ind
        if not plant_ind:
             row_proc_logger.error(f"Row {row_num} - Pass 1: Failed to process mandatory Plant. Aborting row.")
             return False, {}, None, None

        # --- 2. Process Equipment & Equipment Class ---
        equipment_ind, eq_class_ind, eq_class_info_out = process_equipment_and_class(
             row, context, property_mappings, all_created_individuals_by_uid, line_ind, pass_num=1
        )
        if equipment_ind: created_inds_this_row["Equipment"] = equipment_ind
        if eq_class_ind: created_inds_this_row["EquipmentClass"] = eq_class_ind
        if eq_class_info_out: eq_class_info = eq_class_info_out

        # --- 3. Process Material ---
        material_ind = process_material(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        if material_ind: created_inds_this_row["Material"] = material_ind

        # --- 4. Process Production Request ---
        request_ind = process_production_request(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        if request_ind: created_inds_this_row["ProductionRequest"] = request_ind

        # --- 5. Process Events (EventRecord, TimeInterval, Shift, State, Reason) ---
        event_related_inds, event_context_out = process_event_related(
            row, context, property_mappings, all_created_individuals_by_uid,
            equipment_ind=equipment_ind,
            line_ind=line_ind,
            material_ind=material_ind, # Pass context
            request_ind=request_ind, # Pass context
            pass_num=1
        )
        created_inds_this_row.update(event_related_inds)
        if event_context_out: event_context = event_context_out

        # --- 6. Process Person (Example) ---
        # person_ind = process_person(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        # if person_ind: created_inds_this_row["Person"] = person_ind

        row_proc_logger.debug(f"Row {row_num} - Pass 1 End. Created/found {len(created_inds_this_row)} individuals.")

    except Exception as e:
        row_proc_logger.error(f"Row {row_num} - Pass 1: Critical error processing row: {e}", exc_info=True)
        success = False
        created_inds_this_row = {} # Clear partial results on error

    return success, created_inds_this_row, event_context, eq_class_info


def process_single_data_row_pass2(
    row: Dict[str, Any],
    row_num: int,
    context: PopulationContext,
    property_mappings: Dict[str, Dict[str, Dict[str, Any]]],
    individuals_in_row: RowIndividuals,
    linking_context: IndividualRegistry
) -> bool:
    """
    Processes a single data row during Pass 2: Applies object property mappings.

    Args:
        row: The data row dictionary.
        row_num: The original row number (for logging).
        context: The PopulationContext.
        property_mappings: The parsed property mappings.
        individuals_in_row: Dictionary of individuals created/retrieved for THIS row in Pass 1.
        linking_context: The central registry of ALL created individuals from Pass 1.

    Returns:
        bool: True if linking was attempted successfully (even if some links failed safely), False on critical error.
    """
    row_proc_logger.debug(f"Row {row_num} - Pass 2 Start")
    success = True # Assume success unless critical error

    # Add row_num to row dict temporarily for potential use in logging within apply funcs
    row['row_num'] = row_num 

    try:
        # Iterate through the individuals created for this row in Pass 1
        for entity_type, individual in individuals_in_row.items():
            if not individual:
                continue

            if entity_type in property_mappings:
                # Apply object property mappings for this entity type using the full context
                apply_object_property_mappings(
                    individual,
                    property_mappings[entity_type],
                    row,
                    context,
                    entity_type,
                    row_proc_logger,
                    linking_context,
                    individuals_in_row
                )

        row_proc_logger.debug(f"Row {row_num} - Pass 2 End.")

    except Exception as e:
        row_proc_logger.error(f"Row {row_num} - Pass 2: Critical error during linking: {e}", exc_info=True)
        success = False
    finally:
        # Clean up temporary key
        if 'row_num' in row: del row['row_num']

    return success 

===========================================
FILE: ontology_generator/population/sequence.py
===========================================

"""
Sequence relationship module for the ontology generator.

This module provides functions for setting up equipment sequence relationships.
"""
from typing import Dict, Any, List, Optional, Tuple

from owlready2 import Thing, Ontology, ThingClass, PropertyClass

from ontology_generator.utils.logging import pop_logger
from ontology_generator.population.core import PopulationContext, _set_property_value

def _safe_sort_by_position(items, default_position=999999):
    """
    Safely sorts items by position value, handling None values gracefully.
    
    Args:
        items: Dictionary items (key, value) where value might be None
        default_position: Default value to use for None positions
        
    Returns:
        Sorted list of (key, value) tuples
    """
    def get_safe_position(item):
        key, position = item
        if position is None:
            pop_logger.warning(f"Found None position for {key}, using default position {default_position} for sorting")
            return default_position
        return position
        
    return sorted(items, key=get_safe_position)

def setup_equipment_sequence_relationships(onto: Ontology,
                                          equipment_class_positions: Dict[str, int],
                                          defined_classes: Dict[str, ThingClass],
                                          defined_properties: Dict[str, PropertyClass],
                                          created_equipment_class_inds: Dict[str, Thing]):
    """
    Establish upstream/downstream relationships between equipment *classes* based on sequence positions.
    
    Args:
        onto: The ontology
        equipment_class_positions: Dictionary mapping equipment class names to sequence positions
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        created_equipment_class_inds: Dictionary mapping equipment class names to class individuals
    """
    pop_logger.info("Setting up CLASS-LEVEL equipment sequence relationships based on position...")

    # Get context for properties/classes
    context = PopulationContext(onto, defined_classes, defined_properties, {})  # is_functional map not needed here

    # Get the CLASS-LEVEL properties
    prop_classIsUpstreamOf = context.get_prop("classIsUpstreamOf")
    prop_classIsDownstreamOf = context.get_prop("classIsDownstreamOf")  # Optional for inverse

    if not prop_classIsUpstreamOf:
        pop_logger.error("Cannot establish CLASS-LEVEL sequence relationships: 'classIsUpstreamOf' property not defined.")
        return
    if not prop_classIsDownstreamOf:
        pop_logger.warning("'classIsDownstreamOf' inverse property not found. Only forward class relationships will be set.")

    cls_EquipmentClass = context.get_class("EquipmentClass")
    if not cls_EquipmentClass: 
        return  # Should have been caught earlier, but safe check

    # Verify domain/range compatibility (optional but good practice)
    if cls_EquipmentClass not in prop_classIsUpstreamOf.domain:
        pop_logger.warning(f"Property 'classIsUpstreamOf' ({prop_classIsUpstreamOf}) does not have EquipmentClass in its domain {prop_classIsUpstreamOf.domain}.")
    # Range check assumes list of classes is expected
    if not any(issubclass(cls_EquipmentClass, r_cls) for r_cls in (prop_classIsUpstreamOf.range if isinstance(prop_classIsUpstreamOf.range, list) else [prop_classIsUpstreamOf.range])):
        pop_logger.warning(f"Property 'classIsUpstreamOf' ({prop_classIsUpstreamOf}) does not have EquipmentClass in its range {prop_classIsUpstreamOf.range}.")


    if not created_equipment_class_inds:
        pop_logger.warning("No created EquipmentClass individuals provided. Cannot establish class relationships.")
        return
    if not equipment_class_positions:
        pop_logger.warning("Equipment class positions dictionary is empty. Cannot establish class relationships.")
        return

    # Sort classes by their position number (safely handling None values)
    sorted_classes = _safe_sort_by_position(equipment_class_positions.items())
    
    # Log position information for debugging
    pop_logger.debug("Equipment class positions for sequence setup:")
    for cls_name, pos in sorted_classes:
        pop_logger.debug(f"  Class: {cls_name}, Position: {pos}")

    if len(sorted_classes) < 2:
        pop_logger.warning("Not enough equipment classes with sequence positions (< 2) to establish relationships.")
        return

    # Create relationships based on sequence order
    relationships_created = 0
    with onto:
        for i in range(len(sorted_classes) - 1):
            upstream_class_name, up_pos = sorted_classes[i]
            downstream_class_name, down_pos = sorted_classes[i + 1]

            upstream_ind = created_equipment_class_inds.get(upstream_class_name)
            downstream_ind = created_equipment_class_inds.get(downstream_class_name)

            if not upstream_ind:
                pop_logger.warning(f"Sequence setup: Upstream class individual '{upstream_class_name}' not found in provided dict.")
                continue
            if not downstream_ind:
                pop_logger.warning(f"Sequence setup: Downstream class individual '{downstream_class_name}' not found in provided dict.")
                continue

            pop_logger.debug(f"Evaluating CLASS relationship: {upstream_ind.name} (Pos {up_pos}) -> {downstream_ind.name} (Pos {down_pos})")

            # Set relationships (classIsUpstreamOf is NON-functional per spec)
            try:
                # Use helper to check if relationship already exists before appending
                _set_property_value(upstream_ind, prop_classIsUpstreamOf, downstream_ind, is_functional=False)

                # Explicitly set the inverse relationship if available and needed
                if prop_classIsDownstreamOf:
                    _set_property_value(downstream_ind, prop_classIsDownstreamOf, upstream_ind, is_functional=False)

                # Check if the forward relationship was actually added (or already existed)
                if downstream_ind in getattr(upstream_ind, prop_classIsUpstreamOf.python_name, []):
                    relationships_created += 1  # Count successful links (new or existing is fine)
                    pop_logger.debug(f"Confirmed CLASS relationship: {upstream_class_name} classIsUpstreamOf {downstream_class_name}")

            except Exception as e:
                pop_logger.error(f"Error setting class relationship {upstream_class_name} -> {downstream_class_name}: {e}")

    pop_logger.info(f"Established/verified {relationships_created} CLASS-LEVEL upstream relationships.")

    # Print relationship summary to stdout
    print("\n=== EQUIPMENT CLASS SEQUENCE RELATIONSHIP REPORT ===")
    if relationships_created > 0:
        print(f"Established/verified {relationships_created} upstream relationships between Equipment Classes:")
        # Re-iterate to print the established sequence
        for i in range(len(sorted_classes) - 1):
            upstream_class_name, _ = sorted_classes[i]
            downstream_class_name, _ = sorted_classes[i + 1]
            # Check if both individuals exist to avoid errors if one was missing during linking
            if created_equipment_class_inds.get(upstream_class_name) and created_equipment_class_inds.get(downstream_class_name):
                print(f"  {upstream_class_name} → {downstream_class_name}")
    else:
        print("No class-level sequence relationships were created or verified.")
    print(f"Total classes with positions considered: {len(sorted_classes)}")


def setup_equipment_instance_relationships(onto: Ontology,
                                          defined_classes: Dict[str, ThingClass],
                                          defined_properties: Dict[str, PropertyClass],
                                          property_is_functional: Dict[str, bool],
                                          equipment_class_positions: Dict[str, int]):
    """
    Establish upstream/downstream relationships between equipment *instances* within the same production line.
    
    The approach:
    1. Group equipment instances by production line and equipment class
    2. For each line, sequence equipment classes based on the positions
    3. For each class on a line, sort its instances by equipmentId
    4. Chain instances within the same class sequentially
    5. Chain the last instance of one class to the first instance of the next class
    
    Args:
        onto: The ontology
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        property_is_functional: Dictionary indicating whether properties are functional
        equipment_class_positions: Dictionary mapping equipment class names to sequence positions
    """
    pop_logger.info("Setting up INSTANCE-LEVEL equipment relationships within production lines...")

    # Get context for properties/classes
    context = PopulationContext(onto, defined_classes, defined_properties, property_is_functional)

    # Get the required classes and properties
    cls_Equipment = context.get_class("Equipment")
    cls_ProductionLine = context.get_class("ProductionLine")
    cls_EquipmentClass = context.get_class("EquipmentClass")
    prop_isPartOfProductionLine = context.get_prop("isPartOfProductionLine")
    prop_memberOfClass = context.get_prop("memberOfClass")
    prop_equipmentClassId = context.get_prop("equipmentClassId")  # Needed to get class name string
    prop_equipmentId = context.get_prop("equipmentId")  # Needed for sorting instances
    prop_equipment_isUpstreamOf = context.get_prop("equipmentIsUpstreamOf")
    prop_equipment_isDownstreamOf = context.get_prop("equipmentIsDownstreamOf")  # Optional for inverse

    # Check essentials
    if not all([cls_Equipment, cls_ProductionLine, cls_EquipmentClass,
                prop_isPartOfProductionLine, prop_memberOfClass, prop_equipmentClassId,
                prop_equipmentId, prop_equipment_isUpstreamOf]):
        pop_logger.error("Missing required classes or properties for equipment instance relationships.")
        return

    if not prop_equipment_isDownstreamOf:
        pop_logger.warning("'equipmentIsDownstreamOf' inverse property not found. Only forward instance relationships will be set.")

    if not equipment_class_positions:
        pop_logger.warning("Equipment class positions dictionary is empty. Cannot establish instance relationships.")
        return

    # Sort class names by position (safely handling None values)
    sorted_classes = _safe_sort_by_position(equipment_class_positions.items())
    sorted_class_names_by_pos = [item[0] for item in sorted_classes]

    if len(sorted_class_names_by_pos) < 1:  # Changed from 2 to 1 since we now chain within classes too
        pop_logger.warning("No equipment classes with sequence positions found. Cannot establish instance relationships.")
        return

    # Group equipment instances by line and class name
    pop_logger.debug("Grouping equipment instances by production line and class name...")
    line_equipment_map: Dict[Thing, Dict[str, List[Thing]]] = {}  # {line_individual: {class_name_str: [equipment_instances]}}

    # Iterate through all Equipment individuals in the ontology
    for equipment_inst in onto.search(type=cls_Equipment):
        # Get the line(s) this equipment belongs to (Non-functional)
        equipment_lines = getattr(equipment_inst, "isPartOfProductionLine", [])
        if not equipment_lines:
            pop_logger.debug(f"Equipment {equipment_inst.name} is not linked to any ProductionLine. Skipping.")
            continue

        # Get the EquipmentClass this equipment belongs to (Functional)
        equipment_class_ind = getattr(equipment_inst, "memberOfClass", None)
        if not equipment_class_ind or not isinstance(equipment_class_ind, cls_EquipmentClass):
            pop_logger.debug(f"Equipment {equipment_inst.name} is not linked to an EquipmentClass. Skipping.")
            continue

        # Get the class name string from the EquipmentClass individual (Functional)
        class_name_str = getattr(equipment_class_ind, "equipmentClassId", None)
        if not class_name_str:
            pop_logger.warning(f"EquipmentClass {equipment_class_ind.name} (linked from {equipment_inst.name}) is missing 'equipmentClassId'. Skipping.")
            continue

        # Check if this class name is in our sequence map
        if class_name_str not in equipment_class_positions:
            pop_logger.debug(f"Equipment {equipment_inst.name}'s class '{class_name_str}' not in sequence map. Skipping.")
            continue

        # Add equipment to the map for each line it belongs to
        for equipment_line in equipment_lines:
            if not isinstance(equipment_line, cls_ProductionLine):
                pop_logger.warning(f"Equipment {equipment_inst.name} linked to non-ProductionLine '{equipment_line}'. Skipping this link.")
                continue

            # Add equipment to the map structure
            if equipment_line not in line_equipment_map:
                line_equipment_map[equipment_line] = {cn: [] for cn in sorted_class_names_by_pos}  # Pre-initialize with sequenced classes
            # Ensure the specific class bucket exists (might not if class wasn't in initial sequence list but had a position)
            if class_name_str not in line_equipment_map[equipment_line]:
                line_equipment_map[equipment_line][class_name_str] = []

            if equipment_inst not in line_equipment_map[equipment_line][class_name_str]:
                line_equipment_map[equipment_line][class_name_str].append(equipment_inst)
                pop_logger.debug(f"Mapped Equipment {equipment_inst.name} to Line {equipment_line.name} under Class '{class_name_str}'")

    # Create instance-level relationships within each line
    total_relationships = 0
    line_relationship_counts: Dict[str, int] = {}
    pop_logger.info(f"Found {len(line_equipment_map)} lines with sequenced equipment.")

    def safe_get_equipment_id(equipment: Thing) -> str:
        """Helper to safely get equipmentId or fallback to name for sorting."""
        equipment_id = getattr(equipment, "equipmentId", None)
        if equipment_id:
            return str(equipment_id)
        return equipment.name

    with onto:
        for line_ind, class_equipment_map_on_line in line_equipment_map.items():
            line_id_str = getattr(line_ind, "lineId", line_ind.name)
            line_relationships = 0
            pop_logger.info(f"Processing equipment instance relationships for line: {line_id_str}")

            # Track the last instance in the chain to link between classes
            last_instance_in_chain = None

            # Process each equipment class in sequence order
            for class_name in sorted_class_names_by_pos:
                equipment_instances = class_equipment_map_on_line.get(class_name, [])

                if not equipment_instances:
                    pop_logger.debug(f"No instances of '{class_name}' found on line {line_id_str}. Continuing to next class.")
                    continue  # No instances for this class on this line, but keep last_instance_in_chain

                # Sort equipment instances by equipmentId for sequential chaining
                sorted_instances = sorted(equipment_instances, key=safe_get_equipment_id)

                # Log the instances being chained
                instance_ids = [safe_get_equipment_id(e) for e in sorted_instances]
                pop_logger.info(f"Chaining {len(sorted_instances)} instances of '{class_name}' on line '{line_id_str}' by equipmentId: {', '.join(instance_ids)}")

                # If there's a previous class's last instance, link it to the first instance of this class
                if last_instance_in_chain:
                    try:
                        # Link the last instance of previous class to first instance of current class
                        _set_property_value(last_instance_in_chain, prop_equipment_isUpstreamOf, sorted_instances[0], is_functional=False)

                        # Set the inverse relation if available
                        if prop_equipment_isDownstreamOf:
                            _set_property_value(sorted_instances[0], prop_equipment_isDownstreamOf, last_instance_in_chain, is_functional=False)

                        line_relationships += 1
                        prev_class = getattr(last_instance_in_chain.memberOfClass, "equipmentClassId", "Unknown")
                        pop_logger.info(f"Linked end of '{prev_class}' chain ({safe_get_equipment_id(last_instance_in_chain)}) " +
                                        f"to start of '{class_name}' chain ({safe_get_equipment_id(sorted_instances[0])}) on line '{line_id_str}'")
                    except Exception as e:
                        pop_logger.error(f"Error linking between class chains on line {line_id_str}: {e}")

                # Chain instances within this class sequentially
                if len(sorted_instances) > 1:  # Only need to chain if there are multiple instances
                    internal_links = 0
                    for i in range(len(sorted_instances) - 1):
                        try:
                            upstream_eq = sorted_instances[i]
                            downstream_eq = sorted_instances[i + 1]

                            # Create forward relationship
                            _set_property_value(upstream_eq, prop_equipment_isUpstreamOf, downstream_eq, is_functional=False)

                            # Create inverse relationship if property exists
                            if prop_equipment_isDownstreamOf:
                                _set_property_value(downstream_eq, prop_equipment_isDownstreamOf, upstream_eq, is_functional=False)

                            line_relationships += 1
                            internal_links += 1

                            # Debug level for internal chainings as there could be many
                            pop_logger.debug(f"Chained {class_name} instances: {safe_get_equipment_id(upstream_eq)} → {safe_get_equipment_id(downstream_eq)}")
                        except Exception as e:
                            pop_logger.error(f"Error chaining instances within {class_name} on line {line_id_str}: {e}")

                    if internal_links > 0:
                        pop_logger.info(f"Created {internal_links} internal chain links among {class_name} instances on line {line_id_str}")

                # Update last_instance_in_chain to the last instance of the current class
                last_instance_in_chain = sorted_instances[-1]

            # Record relationships for this line
            if line_relationships > 0:
                line_relationship_counts[line_id_str] = line_relationships
                total_relationships += line_relationships
                pop_logger.info(f"Established/verified {line_relationships} instance relationships for line {line_id_str}.")

    # Print summary report
    print("\n=== EQUIPMENT INSTANCE RELATIONSHIP REPORT ===")
    if total_relationships > 0:
        pop_logger.info(f"Established/verified {total_relationships} equipment instance relationships across {len(line_relationship_counts)} production lines.")
        print(f"Established/verified {total_relationships} equipment instance relationships on {len(line_relationship_counts)} lines:")
        for line_id_str, count in sorted(line_relationship_counts.items()):
            print(f"  Line {line_id_str}: {count} relationships")

        # Print info about the chaining approach
        print("\nChaining approach:")
        print("  • Equipment instances of the same class are chained in sequence by their equipmentId")
        print("  • Last instance of each class is linked to first instance of the next class in sequence")
        print("  • Class sequence is determined by the DEFAULT_EQUIPMENT_SEQUENCE dictionary")
    else:
        pop_logger.warning("No equipment instance relationships were created or verified.")
        print("No equipment instance relationships could be established or verified.")
        print("Possible reasons: Equipment not linked to lines/classes, missing sequence positions, or no equipment found on the same line.")


===========================================
FILE: ontology_generator/utils/__init__.py
===========================================

from .types import safe_cast
from .logging import main_logger, configure_logging, analysis_logger # Also expose logging functions here if desired, or keep them module-specific


===========================================
FILE: ontology_generator/utils/logging.py
===========================================

"""
Logging utilities for the ontology generator.

This module provides functions for setting up and configuring logging.
"""
import logging
import sys
from typing import Optional, List

from ontology_generator.config import LOG_FORMAT, SUPPRESSED_WARNINGS

# --- Logger Instances ---
# Main module loggers that will be configured at initialization
logger = logging.getLogger("ontology_definition")  # Logger for definition module
pop_logger = logging.getLogger("ontology_population")  # Logger for population module
link_logger = logging.getLogger("event_linking")  # Logger for event linking pass
main_logger = logging.getLogger("create_ontology")  # Logger for main script
analysis_logger = logging.getLogger("ontology_analysis")  # Logger for analysis module

class WarningSuppressionFilter(logging.Filter):
    """
    A logging filter that suppresses specific warning messages based on configured substrings.
    """
    def __init__(self, suppressed_warnings=None):
        super().__init__()
        self.suppressed_warnings = suppressed_warnings or []
        self.suppressed_count = 0
        
    def filter(self, record):
        if record.levelno == logging.WARNING:
            # Get the formatted message
            message = record.getMessage()
            # Check if any suppressed warning substring is in the message
            for suppressed in self.suppressed_warnings:
                if suppressed in message:
                    self.suppressed_count += 1
                    return False  # Suppress this warning
        return True  # Let other messages through

class InfoSuppressionFilter(logging.Filter):
    """
    A logging filter that suppresses specific INFO messages based on configured substrings.
    """
    def __init__(self, suppressed_info=None):
        super().__init__()
        self.suppressed_info = suppressed_info or []
        self.suppressed_count = 0
        
    def filter(self, record):
        if record.levelno == logging.INFO:
            # Get the formatted message
            message = record.getMessage()
            # Check if any suppressed info substring is in the message
            for suppressed in self.suppressed_info:
                if suppressed in message:
                    self.suppressed_count += 1
                    return False  # Suppress this info message
        return True  # Let other messages through

def configure_logging(
    log_level: int = logging.INFO,
    log_file: Optional[str] = None,
    handlers: Optional[List[logging.Handler]] = None
) -> None:
    """
    Configure the root logger with the specified settings.
    
    Args:
        log_level: The logging level to use (default: INFO)
        log_file: Optional path to a log file
        handlers: Optional list of handlers to add to the root logger
    """
    # Reset root logger configuration
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Set up basic configuration
    logging.basicConfig(level=log_level, format=LOG_FORMAT, stream=sys.stdout)
    root_logger.setLevel(log_level)
    
    # Set all handler levels
    for handler in root_logger.handlers:
        handler.setLevel(log_level)
    
    # Add file handler if specified
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(LOG_FORMAT))
        file_handler.setLevel(log_level)
        root_logger.addHandler(file_handler)
    
    # Add additional handlers if provided
    if handlers:
        for handler in handlers:
            root_logger.addHandler(handler)
    
    # Set up warning suppression filter for specific loggers
    warning_filter = WarningSuppressionFilter(SUPPRESSED_WARNINGS)
    for logger_name in [
        "ontology_generator.population.row_processor",
        "ontology_population",
        "ontology_generator.population.equipment",
        "ontology_generator.population.events",
        "ontology_generator.population.core"
    ]:
        logging.getLogger(logger_name).addFilter(warning_filter)
    
    # Set up info suppression filter for individual creation messages
    info_filter = InfoSuppressionFilter(["Created new individual"])
    pop_logger.addFilter(info_filter)
    
    # Store filters for later access
    global _warning_filter, _info_filter
    _warning_filter = warning_filter
    _info_filter = info_filter
    
    # Log confirmation
    main_logger.info("Logging configured.")
    if log_level == logging.DEBUG:
        main_logger.info("Verbose logging enabled (DEBUG level).")
    elif log_level == logging.WARNING:
        main_logger.info("Quiet logging enabled (WARNING level).")
    else:
        main_logger.info("Standard logging enabled (INFO level).")
    main_logger.info(f"Warning suppression filter applied for {len(SUPPRESSED_WARNINGS)} message patterns.")

# Global variables to store filter instances
_warning_filter = None
_info_filter = None

def get_suppressed_message_counts():
    """
    Get counts of suppressed messages.
    
    Returns:
        A tuple of (warning_count, info_count)
    """
    warning_count = _warning_filter.suppressed_count if _warning_filter else 0
    info_count = _info_filter.suppressed_count if _info_filter else 0
    return warning_count, info_count

def log_suppressed_message_counts():
    """
    Log the counts of suppressed messages.
    """
    warning_count, info_count = get_suppressed_message_counts()
    total_count = warning_count + info_count
    
    if total_count > 0:
        main_logger.info(f"Message suppression summary: {total_count} messages suppressed "
                         f"({warning_count} warnings, {info_count} info messages)")
    else:
        main_logger.info("No messages have been suppressed by filters")

def get_module_logger(name: str) -> logging.Logger:
    """
    Get a logger for a specific module with the specified name.
    
    Args:
        name: The name of the logger
        
    Returns:
        A configured logger instance
    """
    return logging.getLogger(name)


===========================================
FILE: ontology_generator/utils/types.py
===========================================

"""
Type conversion utilities for the ontology generator.

This module provides functions for safe type conversion and handling.
"""
import re
from datetime import datetime, date, time
from decimal import Decimal, InvalidOperation
from typing import Any, Optional, Type, List, Dict, TypeVar, Union

from dateutil import parser as dateutil_parser
from dateutil.parser import ParserError

from ontology_generator.utils.logging import pop_logger

T = TypeVar('T')

def safe_cast(value: Any, target_type: Type[T], default: Optional[T] = None) -> Optional[T]:
    """
    Safely casts a value to a target type, returning default on failure.
    
    Args:
        value: The value to cast
        target_type: The target type to cast to
        default: The default value to return on failure
        
    Returns:
        The cast value, or the default if casting fails
    """
    if value is None or value == '':
        return default
    try:
        original_value_repr = repr(value)  # For logging
        value_str = str(value).strip()

        if target_type is str:
            return value_str
        if target_type is int:
            # Handle potential floats in data like '224.0' -> 224
            # Also handle direct integers or strings representing integers
            try:
                return int(float(value_str))
            except ValueError:
                # Maybe it was already an int disguised as string?
                return int(value_str)
        if target_type is float:
            # Handles standard float conversion
            return float(value_str)
        # Note: xsd:decimal maps to float based on XSD_TYPE_MAP
        if target_type is bool:
            val_lower = value_str.lower()
            if val_lower in ['true', '1', 't', 'y', 'yes']:
                return True
            elif val_lower in ['false', '0', 'f', 'n', 'no']:
                return False
            else:
                pop_logger.warning(f"Could not interpret {original_value_repr} as boolean.")
                return None  # Explicitly return None for uninterpretable bools
        if target_type is datetime:
            # --- Use dateutil.parser for robust parsing ---
            try:
                # No need for extensive pre-cleaning or format list with dateutil
                # It handles various formats, including spaces and timezones
                parsed_dt = dateutil_parser.parse(value_str)

                # dateutil returns an AWARE datetime if offset is present.
                # owlready2 stores naive datetimes.
                # Maintain existing behavior: make it naive (loses original offset info).
                if parsed_dt.tzinfo:
                    pop_logger.debug(f"Parsed datetime {original_value_repr} with timezone {parsed_dt.tzinfo}, storing as naive datetime.")
                    parsed_dt = parsed_dt.replace(tzinfo=None)
                else:
                    pop_logger.debug(f"Parsed datetime {original_value_repr} without timezone, storing as naive datetime.")

                pop_logger.debug(f"Successfully parsed datetime {original_value_repr} using dateutil -> {parsed_dt}")
                return parsed_dt

            except (ParserError, ValueError, TypeError) as e:  # Catch errors from dateutil and potential downstream issues
                pop_logger.warning(f"Could not parse datetime string {original_value_repr} using dateutil parser: {e}")
                return default
            except Exception as e:  # Catch any other unexpected errors
                pop_logger.error(f"Unexpected error parsing datetime {original_value_repr} with dateutil: {e}", exc_info=False)
                return default
            # --- End of dateutil parsing block ---

        if target_type is date:
            try:
                # Try ISO first
                return date.fromisoformat(value_str)  # Assumes YYYY-MM-DD
            except ValueError:
                # Try other common formats if needed
                try:
                    dt_obj = datetime.strptime(value_str, "%m/%d/%Y")  # Example alternative
                    return dt_obj.date()
                except ValueError:
                    pop_logger.warning(f"Could not parse date string {original_value_repr} as ISO or m/d/Y date.")
                    return default
        if target_type is time:
            try:
                # Try ISO first
                return time.fromisoformat(value_str)  # Assumes HH:MM:SS[.ffffff][+/-HH:MM]
            except ValueError:
                # Try other common formats if needed
                try:
                    dt_obj = datetime.strptime(value_str, "%H:%M:%S")  # Just H:M:S
                    return dt_obj.time()
                except ValueError:
                    pop_logger.warning(f"Could not parse time string {original_value_repr} as ISO or H:M:S time.")
                    return default

        # Final fallback cast attempt
        return target_type(value_str)

    except (ValueError, TypeError, InvalidOperation) as e:
        target_type_name = target_type.__name__ if target_type else "None"
        original_value_repr = repr(value)[:50] + ('...' if len(repr(value)) > 50 else '') # Added for clarity
        pop_logger.warning(f"Failed to cast {original_value_repr} to {target_type_name}: {e}. Returning default: {default}")
        return default
    except Exception as e:
        target_type_name = target_type.__name__ if target_type else "None"
        original_value_repr = repr(value)[:50] + ('...' if len(repr(value)) > 50 else '') # Added for clarity
        pop_logger.error(f"Unexpected error casting {original_value_repr} to {target_type_name}: {e}", exc_info=False)
        return default

def sanitize_name(name: Any) -> str:
    """
    Sanitizes a name to be valid for use in identifiers.
    
    Args:
        name: The name to sanitize
        
    Returns:
        A sanitized version of the name
    """
    if name is None or str(name).strip() == '':
        return "unnamed"
        
    # Convert to string and strip whitespace
    name_str = str(name).strip()
    
    # Replace spaces and common problematic chars with underscore
    safe_name = re.sub(r'\s+|[<>:"/\\|?*#%\']', '_', name_str)
    
    # Remove any remaining non-alphanumeric, non-hyphen, non-underscore chars (allows periods)
    safe_name = re.sub(r'[^\w\-._]', '', safe_name)
    
    # Ensure it doesn't start with a number or hyphen (common restriction)
    if safe_name and (safe_name[0].isdigit() or safe_name[0] == '-'):
        safe_name = "_" + safe_name
        
    # Check if empty after sanitization
    if not safe_name:
        fallback_hash = abs(hash(name_str))  # Hash the original string
        safe_name = f"UnnamedData_{fallback_hash}"
        
    return safe_name


