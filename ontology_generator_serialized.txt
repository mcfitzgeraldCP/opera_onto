===========================================
FILE: ontology_generator/__init__.py
===========================================

"""
Ontology Generator

A modular Python application for generating OWL ontologies from CSV specifications and data.
"""

__version__ = '1.0.0'
__author__ = 'Ontology Generator Team'


===========================================
FILE: ontology_generator/analysis/__init__.py
===========================================

from .population import (
    analyze_ontology_population, generate_population_report,
    generate_optimization_recommendations
)
from .reasoning import generate_reasoning_report
from .sequence_analysis import (
    get_equipment_sequence_for_line, generate_equipment_sequence_report,
    analyze_equipment_sequences
)


===========================================
FILE: ontology_generator/analysis/population.py
===========================================

"""
Population analysis module for the ontology generator.

This module provides functions for analyzing the ontology population.
"""
from typing import Dict, List, Set, Tuple, Any, Optional

from owlready2 import Ontology, Thing, ThingClass

from ontology_generator.utils.logging import analysis_logger

def analyze_ontology_population(onto: Ontology, 
                                defined_classes: Dict[str, ThingClass], 
                                specification: List[Dict[str, str]]
                               ) -> Tuple[Dict[str, int], List[str], Dict[str, List[str]], Dict[str, List[str]]]:
    """
    Analyzes the population status of each class in the ontology.
    
    Args:
        onto: The ontology object
        defined_classes: Dictionary mapping class names to class objects
        specification: The original ontology specification
        
    Returns:
        tuple: (population_counts, empty_classes, class_instances, class_usage_info)
            - population_counts: Dict mapping class name to count of individuals
            - empty_classes: List of class names with no individuals
            - class_instances: Dict mapping class name to list of individual names
            - class_usage_info: Dict with additional usage analysis
    """
    analysis_logger.info("Starting analysis of ontology population")
    
    population_counts = {}
    empty_classes = []
    class_instances = {}
    
    # Extract the spec-defined classes
    spec_defined_classes = set()
    for row in specification:
        class_name = row.get('Proposed OWL Entity', '').strip()
        if class_name:
            spec_defined_classes.add(class_name)
    
    # Classes used in domain/range of properties
    property_domain_classes = set()
    property_range_classes = set()
    
    # Analyze property domains and ranges
    for prop in list(onto.object_properties()) + list(onto.data_properties()):
        if hasattr(prop, 'domain') and prop.domain:
            domains = prop.domain if isinstance(prop.domain, list) else [prop.domain]
            for domain in domains:
                if isinstance(domain, ThingClass):
                    property_domain_classes.add(domain.name)
        
        if hasattr(prop, 'range') and prop.range:
            ranges = prop.range if isinstance(prop.range, list) else [prop.range]
            for range_item in ranges:
                if isinstance(range_item, ThingClass):
                    property_range_classes.add(range_item.name)
    
    # Analyze instances
    for class_name, class_obj in defined_classes.items():
        # Skip owl:Thing which will have everything
        if class_obj is Thing:
            continue
            
        # Get all individuals of this class
        instances = list(onto.search(is_a=class_obj))
        count = len(instances)
        population_counts[class_name] = count
        
        if count == 0:
            empty_classes.append(class_name)
        else:
            # Store up to 10 instance names as examples
            sample_instances = [ind.name for ind in instances[:10]]
            class_instances[class_name] = sample_instances
    
    # Create class usage analysis
    class_usage_info = {
        'spec_defined': list(spec_defined_classes),
        'implemented_in_ontology': list(defined_classes.keys()),
        'in_property_domains': list(property_domain_classes),
        'in_property_ranges': list(property_range_classes),
        'populated_classes': list(set(defined_classes.keys()) - set(empty_classes)),
        'empty_classes': empty_classes,
        'extraneous_classes': list(set(defined_classes.keys()) - spec_defined_classes)
    }
    
    analysis_logger.info(f"Analysis complete. Found {len(population_counts)} classes, {len(empty_classes)} empty classes")
    return population_counts, empty_classes, class_instances, class_usage_info

def generate_population_report(population_counts: Dict[str, int], 
                               empty_classes: List[str], 
                               class_instances: Dict[str, List[str]],
                               defined_classes: Dict[str, ThingClass],
                               class_usage_info: Dict[str, List[str]] = None) -> str:
    """
    Generates a formatted report of the ontology population status.
    
    Args:
        population_counts: Dict mapping class name to count of individuals
        empty_classes: List of class names with no individuals
        class_instances: Dict mapping class name to list of individual names
        defined_classes: Dict mapping class names to class objects
        class_usage_info: Dict with additional usage analysis
        
    Returns:
        str: Formatted report text
    """
    report_lines = []
    
    # Add header
    report_lines.append("\n" + "="*80)
    report_lines.append(f"ONTOLOGY POPULATION REPORT")
    report_lines.append("="*80)
    
    # Summary statistics
    total_classes = len(defined_classes)
    populated_classes = total_classes - len(empty_classes)
    total_individuals = sum(population_counts.values())
    
    report_lines.append(f"\nSUMMARY:")
    report_lines.append(f"  • Total Classes: {total_classes}")
    report_lines.append(f"  • Populated Classes: {populated_classes} ({populated_classes/total_classes*100:.1f}%)")
    report_lines.append(f"  • Empty Classes: {len(empty_classes)} ({len(empty_classes)/total_classes*100:.1f}%)")
    report_lines.append(f"  • Total Individuals: {total_individuals}")
    
    # Spec vs. Implementation Analysis
    if class_usage_info:
        spec_defined = set(class_usage_info.get('spec_defined', []))
        implemented = set(class_usage_info.get('implemented_in_ontology', []))
        extraneous = set(class_usage_info.get('extraneous_classes', []))
        not_implemented = spec_defined - implemented
        
        report_lines.append("\nSPECIFICATION ANALYSIS:")
        report_lines.append(f"  • Classes in Specification: {len(spec_defined)}")
        report_lines.append(f"  • Classes Implemented in Ontology: {len(implemented)}")
        if extraneous:
            report_lines.append(f"  • Extraneous Classes (implemented but not in spec): {len(extraneous)}")
            report_lines.append(f"      {', '.join(sorted(list(extraneous)))}")
        if not_implemented:
            report_lines.append(f"  • Classes in Spec but Not Implemented: {len(not_implemented)}")
            report_lines.append(f"      {', '.join(sorted(list(not_implemented)))}")
        
        # Identify unused but defined classes
        used_in_properties = set(class_usage_info.get('in_property_domains', [])) | set(class_usage_info.get('in_property_ranges', []))
        populated = set(class_usage_info.get('populated_classes', []))
        unused_classes = implemented - used_in_properties - populated
        if unused_classes:
            report_lines.append(f"  • Completely Unused Classes (empty and not used in properties): {len(unused_classes)}")
            report_lines.append(f"      {', '.join(sorted(list(unused_classes)))}")
    
    # Populated classes
    report_lines.append("\nPOPULATED CLASSES (Class: Count)")
    populated_items = sorted([(k, v) for k, v in population_counts.items() if v > 0], 
                            key=lambda x: x[1], reverse=True)
    for class_name, count in populated_items:
        report_lines.append(f"  • {class_name}: {count}")
        # Add sample instances for classes with reasonable counts
        if count <= 20:  # Only show examples for classes with fewer instances
            examples = class_instances.get(class_name, [])
            if examples:
                report_lines.append(f"      Examples: {', '.join(examples[:5])}")
                if len(examples) > 5:
                    report_lines.append(f"      ... and {min(count, len(examples) - 5)} more")
    
    # Empty classes 
    if empty_classes:
        report_lines.append("\nEMPTY CLASSES:")
        for class_name in sorted(empty_classes):
            # Get parent class info for context
            class_obj = defined_classes.get(class_name)
            if class_obj and hasattr(class_obj, 'is_a') and class_obj.is_a:
                parent_names = [p.name for p in class_obj.is_a if p is not Thing]
                if parent_names:
                    # Check if used in property domain/range
                    usage = []
                    if class_usage_info and class_name in class_usage_info.get('in_property_domains', []):
                        usage.append("used in property domains")
                    if class_usage_info and class_name in class_usage_info.get('in_property_ranges', []):
                        usage.append("used in property ranges")
                    
                    if usage:
                        report_lines.append(f"  • {class_name} (subclass of: {', '.join(parent_names)}) - {', '.join(usage)}")
                    else:
                        report_lines.append(f"  • {class_name} (subclass of: {', '.join(parent_names)}) - COMPLETELY UNUSED")
                else:
                    report_lines.append(f"  • {class_name} (direct subclass of owl:Thing)")
            else:
                report_lines.append(f"  • {class_name}")
    
    # Add optimization recommendations
    report_lines.append("\nOPTIMIZATION RECOMMENDATIONS:")
    
    if class_usage_info and 'extraneous_classes' in class_usage_info and class_usage_info['extraneous_classes']:
        report_lines.append("  • Consider adding the extraneous classes to your specification for completeness")
    
    if class_usage_info and 'spec_defined' in class_usage_info and implemented - spec_defined:
        report_lines.append("  • Review and consider removing classes that are implemented but not in your spec")
    
    if unused_classes:
        report_lines.append("  • Consider removing completely unused classes that are neither populated nor referenced in properties")
    
    return "\n".join(report_lines)

def generate_optimization_recommendations(class_usage_info: Dict[str, List[str]],
                                      defined_classes: Dict[str, ThingClass]) -> Dict[str, List[str]]:
    """
    Generates specific recommendations for optimizing the ontology structure.
    
    Args:
        class_usage_info: Dict with usage analysis data
        defined_classes: Dictionary mapping class names to class objects
        
    Returns:
        Dict with categorized recommendations
    """
    recommendations = {
        'classes_to_remove': [],
        'extraneous_classes': [],
        'unused_properties': [],
        'configuration_options': []
    }
    
    # Extract necessary data from usage info
    implemented = set(class_usage_info.get('implemented_in_ontology', []))
    spec_defined = set(class_usage_info.get('spec_defined', []))
    extraneous = set(class_usage_info.get('extraneous_classes', []))
    empty_classes = set(class_usage_info.get('empty_classes', []))
    in_domains = set(class_usage_info.get('in_property_domains', []))
    in_ranges = set(class_usage_info.get('in_property_ranges', []))
    
    # Find used defined classes
    populated_classes = implemented - empty_classes
    used_in_properties = in_domains | in_ranges
    
    # Find completely unused classes (not populated and not in domains/ranges)
    completely_unused = implemented - populated_classes - used_in_properties
    
    # Classes that are extraneous AND empty
    unused_extraneous = extraneous & empty_classes & completely_unused
    
    # Generate recommendations
    if unused_extraneous:
        recommendations['classes_to_remove'].extend(list(unused_extraneous))
        recommendations['configuration_options'].append(
            "Add a 'CLASSES_TO_SKIP' list in your configuration to avoid creating these classes"
        )
    
    if extraneous:
        recommendations['extraneous_classes'].extend(list(extraneous))
        if len(extraneous) > 5:
            recommendations['configuration_options'].append(
                "Consider using a 'STRICT_SPEC_ADHERENCE' option to only create classes defined in the spec"
            )
    
    # Check parent-child relationships for optimization
    class_hierarchies = {}
    for class_name, class_obj in defined_classes.items():
        if hasattr(class_obj, 'is_a'):
            parents = [p.name for p in class_obj.is_a if p is not Thing]
            if parents:
                class_hierarchies[class_name] = parents
    
    # Find unused leaf classes (classes that are completely unused and have no children)
    leaf_classes = set()
    for class_name in completely_unused:
        has_children = False
        for _, parents in class_hierarchies.items():
            if class_name in parents:
                has_children = True
                break
        if not has_children:
            leaf_classes.add(class_name)
    
    if leaf_classes:
        recommendations['classes_to_remove'].extend(list(leaf_classes))
        recommendations['configuration_options'].append(
            "Consider adding a 'PRUNE_LEAF_CLASSES' option to automatically remove unused leaf classes"
        )
    
    # Remove duplicates and sort for consistency
    for key in recommendations:
        recommendations[key] = sorted(list(set(recommendations[key])))
    
    return recommendations


===========================================
FILE: ontology_generator/analysis/reasoning.py
===========================================

"""
Reasoning analysis module for the ontology generator.

This module provides functions for generating reasoning reports.
"""
from typing import Dict, List, Tuple, Any, Set

from owlready2 import Ontology, ThingClass

from ontology_generator.utils.logging import analysis_logger

def generate_reasoning_report(onto: Ontology,
                             pre_stats: Dict[str, int],
                             post_stats: Dict[str, int],
                             inconsistent_classes: List[ThingClass],
                             inferred_hierarchy: Dict[str, Dict[str, List[str]]],
                             inferred_properties: Dict[str, List[str]],
                             inferred_individuals: Dict[str, Dict[str, Any]],
                             use_reasoner: bool,
                             max_entities_per_category: int = 10,  # Parameter to limit entities shown
                             verbose: bool = False  # Parameter to control detail level
                            ) -> Tuple[str, bool]:
    """
    Generates a structured report from reasoning results.
    
    Args:
        onto: The ontology object
        pre_stats: Dict with pre-reasoning statistics
        post_stats: Dict with post-reasoning statistics
        inconsistent_classes: List of inconsistent classes
        inferred_hierarchy: Dict of inferred class relationships
        inferred_properties: Dict of inferred property characteristics
        inferred_individuals: Dict of inferred individual relationships
        use_reasoner: Whether the reasoner was used
        max_entities_per_category: Maximum number of entities to show per category
        verbose: Whether to show all details
    
    Returns:
        tuple: (report_str, has_issues)
    """
    report_lines = []
    has_issues = False

    def add_section(title):
        report_lines.extend(["\n" + "="*80, f"{title}", "="*80])

    # 1. Executive Summary
    add_section("REASONING REPORT EXECUTIVE SUMMARY")
    if inconsistent_classes:
        has_issues = True
        report_lines.append("❌ ONTOLOGY STATUS: Inconsistent")
        report_lines.append(f"    Found {len(inconsistent_classes)} inconsistent classes (see details below)")
    else:
        report_lines.append("✅ ONTOLOGY STATUS: Consistent")

    class_diff = post_stats['classes'] - pre_stats['classes']
    prop_diff = (post_stats['object_properties'] - pre_stats['object_properties'] +
                 post_stats['data_properties'] - pre_stats['data_properties'])
    ind_diff = post_stats['individuals'] - pre_stats['individuals']
    report_lines.extend([
        f"\nStructural Changes (Post-Reasoning vs Pre-Reasoning):",
        f"  • Classes: {class_diff:+d}", f"  • Properties (Obj + Data): {prop_diff:+d}", f"  • Individuals: {ind_diff:+d}"
    ])
    inferences_made = bool(inferred_hierarchy or inferred_properties or inferred_individuals)
    report_lines.append(f"\nInferences Made: {'Yes' if inferences_made else 'No'}")

    # 2. Detailed Statistics
    add_section("DETAILED STATISTICS")
    report_lines.extend([
        "\nPre-Reasoning:",
        f"  • Classes: {pre_stats['classes']}", f"  • Object Properties: {pre_stats['object_properties']}",
        f"  • Data Properties: {pre_stats['data_properties']}", f"  • Individuals: {pre_stats['individuals']}",
        "\nPost-Reasoning:",
        f"  • Classes: {post_stats['classes']}", f"  • Object Properties: {post_stats['object_properties']}",
        f"  • Data Properties: {post_stats['data_properties']}", f"  • Individuals: {post_stats['individuals']}"
    ])

    # 3. Consistency Issues
    if inconsistent_classes:
        add_section("CONSISTENCY ISSUES")
        report_lines.append("\nInconsistent Classes:")
        
        # Show all inconsistent classes regardless of verbosity - these are critical
        for cls in inconsistent_classes: 
            report_lines.append(f"  • {cls.name} ({cls.iri})")
        has_issues = True

    # 4. Inferred Knowledge
    add_section("INFERRED KNOWLEDGE")
    if inferred_hierarchy:
        report_lines.append("\nClass Hierarchy Changes:")
        
        # Apply entity limitation based on verbosity
        hierarchy_items = list(inferred_hierarchy.items())
        if not verbose and len(hierarchy_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(hierarchy_items)} classes with hierarchy changes")
            hierarchy_items = hierarchy_items[:max_entities_per_category]
            
        for parent, data in hierarchy_items:
            if data.get('subclasses') or data.get('equivalent'):
                report_lines.append(f"\n  Class: {parent}")
                if data.get('subclasses'):
                    subclass_items = data['subclasses']
                    if not verbose and len(subclass_items) > max_entities_per_category:
                        report_lines.append(f"    ↳ Inferred Subclasses: ({len(subclass_items)} total, showing {max_entities_per_category})")
                        for sub in subclass_items[:max_entities_per_category]:
                            report_lines.append(f"        • {sub}")
                        report_lines.append(f"        • ... and {len(subclass_items) - max_entities_per_category} more")
                    else:
                        report_lines.append("    ↳ Inferred Subclasses:")
                        for sub in subclass_items:
                            report_lines.append(f"        • {sub}")
                
                if data.get('equivalent'):
                    equiv_items = data['equivalent']
                    if not verbose and len(equiv_items) > max_entities_per_category:
                        report_lines.append(f"    ≡ Inferred Equivalent Classes: {', '.join(equiv_items[:max_entities_per_category])} ... and {len(equiv_items) - max_entities_per_category} more")
                    else:
                        report_lines.append(f"    ≡ Inferred Equivalent Classes: {', '.join(equiv_items)}")
    else: 
        report_lines.append("\nNo new class hierarchy relationships inferred.")

    if inferred_properties:
        report_lines.append("\nInferred Property Characteristics:")
        
        # Apply entity limitation based on verbosity
        property_items = list(inferred_properties.items())
        if not verbose and len(property_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(property_items)} properties with inferred characteristics")
            property_items = property_items[:max_entities_per_category]
            
        for prop, chars in property_items:
            report_lines.append(f"\n  Property: {prop}")
            if not verbose and len(chars) > max_entities_per_category:
                for char in chars[:max_entities_per_category]:
                    report_lines.append(f"    • {char}")
                report_lines.append(f"    • ... and {len(chars) - max_entities_per_category} more")
            else:
                for char in chars:
                    report_lines.append(f"    • {char}")
    else: 
        report_lines.append("\nNo new property characteristics inferred.")

    if inferred_individuals:
        report_lines.append("\nIndividual Inferences:")
        
        # Apply entity limitation based on verbosity
        individual_items = list(inferred_individuals.items())
        if not verbose and len(individual_items) > max_entities_per_category:
            report_lines.append(f"  Showing {max_entities_per_category} of {len(individual_items)} individuals with inferences")
            individual_items = individual_items[:max_entities_per_category]
            
        for ind_name, data in individual_items:
            report_lines.append(f"\n  Individual: {ind_name}")
            if data.get('types'):
                types_items = data['types']
                if not verbose and len(types_items) > max_entities_per_category:
                    report_lines.append(f"    Inferred Types: ({len(types_items)} total, showing {max_entities_per_category})")
                    for t in types_items[:max_entities_per_category]:
                        report_lines.append(f"      • {t}")
                    report_lines.append(f"      • ... and {len(types_items) - max_entities_per_category} more")
                else:
                    report_lines.append("    Inferred Types:")
                    for t in types_items:
                        report_lines.append(f"      • {t}")
                        
            if data.get('properties'):
                props_items = list(data['properties'].items())
                if not verbose and len(props_items) > max_entities_per_category:
                    report_lines.append(f"    Inferred Property Values: ({len(props_items)} total, showing {max_entities_per_category})")
                    for p, vals in props_items[:max_entities_per_category]:
                        if not verbose and len(vals) > max_entities_per_category:
                            report_lines.append(f"      • {p}: {', '.join(vals[:max_entities_per_category])} ... and {len(vals) - max_entities_per_category} more")
                        else:
                            report_lines.append(f"      • {p}: {', '.join(vals)}")
                    report_lines.append(f"      • ... and {len(props_items) - max_entities_per_category} more properties")
                else:
                    report_lines.append("    Inferred Property Values:")
                    for p, vals in props_items:
                        if not verbose and len(vals) > max_entities_per_category:
                            report_lines.append(f"      • {p}: {', '.join(vals[:max_entities_per_category])} ... and {len(vals) - max_entities_per_category} more")
                        else:
                            report_lines.append(f"      • {p}: {', '.join(vals)}")
    else: 
        report_lines.append("\nNo new individual types or property values inferred.")

    # 5. Recommendations
    add_section("RECOMMENDATIONS")
    recommendations = []
    if inconsistent_classes:
        recommendations.append("❗ HIGH PRIORITY: Resolve inconsistencies listed above.")
    if not inconsistent_classes and not inferences_made and use_reasoner:
        recommendations.append("⚠️ No inferences made - Ontology is consistent but may lack richness or reasoner configuration issue. Consider adding more specific axioms or reviewing reasoner setup.")
        # Don't flag as issue if reasoner wasn't run
        if use_reasoner: 
            has_issues = True
    if class_diff == 0 and prop_diff == 0 and ind_diff == 0 and use_reasoner:
       recommendations.append("ℹ️ No structural changes after reasoning - verify if this is expected.")
    if recommendations:
        report_lines.extend(["\n" + rec for rec in recommendations])
    else: 
        report_lines.append("\nNo critical issues or major inference gaps found.")

    return "\n".join(report_lines), has_issues


===========================================
FILE: ontology_generator/analysis/sequence_analysis.py
===========================================

"""
Sequence Analysis Module for the Ontology Generator.

This module provides functions for analyzing equipment sequences in the ontology.
"""
from typing import List, Optional, Dict, Any, Tuple
from owlready2 import Thing, Ontology

from ontology_generator.utils.logging import analysis_logger

def _safe_sort_by_attribute(items, attr_name, default_value="Unknown"):
    """
    Safely sorts items by an attribute, handling None values gracefully.
    
    Args:
        items: List of objects to sort
        attr_name: Name of attribute to sort by
        default_value: Default value to use for None attributes
        
    Returns:
        Sorted list of items
    """
    def get_safe_attribute(item):
        value = getattr(item, attr_name, None)
        if value is None:
            value = getattr(item, "name", default_value)
            if value is None:
                analysis_logger.warning(f"Item has neither {attr_name} nor name attribute, using default value for sorting")
                return default_value
        return value
        
    return sorted(items, key=get_safe_attribute)

def get_equipment_sequence_for_line(onto: Ontology, line_individual: Thing) -> List[Thing]:
    """
    Retrieves the equipment sequence for a specific production line.
    
    Args:
        onto: The ontology object
        line_individual: The ProductionLine individual
        
    Returns:
        A list of equipment individuals in sequence order
    """
    # Get required properties
    equipment_is_upstream_of = None
    for prop in onto.object_properties():
        if prop.name == "equipmentIsUpstreamOf":
            equipment_is_upstream_of = prop
            break
    
    if not equipment_is_upstream_of:
        analysis_logger.warning("Property 'equipmentIsUpstreamOf' not found in ontology")
        return []
    
    # Get all equipment on this line
    equipment_on_line = []
    for ind in onto.individuals():
        if hasattr(ind, "isPartOfProductionLine"):
            line_list = ind.isPartOfProductionLine
            if not isinstance(line_list, list):
                line_list = [line_list] if line_list else []
            
            if line_individual in line_list:
                equipment_on_line.append(ind)
    
    if not equipment_on_line:
        analysis_logger.info(f"No equipment found for line {line_individual.name}")
        return []
        
    analysis_logger.info(f"Found {len(equipment_on_line)} equipment instances on line {line_individual.name}")
    
    # Find equipment with no upstream equipment (start of sequence)
    start_equipment = []
    for eq in equipment_on_line:
        has_upstream = False
        for other_eq in equipment_on_line:
            upstream_list = getattr(other_eq, equipment_is_upstream_of.python_name, [])
            if not isinstance(upstream_list, list):
                upstream_list = [upstream_list] if upstream_list else []
                
            if eq in upstream_list:
                has_upstream = True
                break
        if not has_upstream:
            start_equipment.append(eq)
    
    analysis_logger.info(f"Found {len(start_equipment)} starting equipment (no upstream) for line {line_individual.name}")
    
    # Build sequence by following relationships
    sequence = []
    visited = set()
    
    def follow_sequence(eq):
        if eq in visited:
            return
        visited.add(eq)
        sequence.append(eq)
        
        downstream_list = getattr(eq, equipment_is_upstream_of.python_name, [])
        if not isinstance(downstream_list, list):
            downstream_list = [downstream_list] if downstream_list else []
        
        # Filter to equipment on this line only
        downstream_list = [d for d in downstream_list if d in equipment_on_line]
        
        # Sort by equipment ID if multiple downstream (unlikely but possible)
        if len(downstream_list) > 1:
            downstream_list.sort(key=lambda e: getattr(e, "equipmentId", e.name))
        
        for downstream in downstream_list:
            follow_sequence(downstream)
    
    # Start from each entry point
    for eq in start_equipment:
        follow_sequence(eq)
    
    analysis_logger.info(f"Determined sequence with {len(sequence)} equipment for line {line_individual.name}")
    return sequence

def generate_equipment_sequence_report(onto: Ontology) -> str:
    """
    Generates a report of equipment sequences for all lines in the ontology.
    
    Args:
        onto: The ontology object
        
    Returns:
        A string report of all equipment sequences
    """
    analysis_logger.info("Generating equipment sequence report for all lines")
    
    # Find all production lines
    lines = []
    
    # Get the ProductionLine class - search by name
    production_line_class = None
    for cls in onto.classes():
        if cls.name == "ProductionLine":
            production_line_class = cls
            break
    
    if not production_line_class:
        analysis_logger.warning("ProductionLine class not found in ontology - trying to find lines by lineId property")
        # Fallback to property-based detection with warnings about duplicates
        line_ids_seen = set()
        for ind in onto.individuals():
            if hasattr(ind, "lineId"):
                line_id = getattr(ind, "lineId")
                if line_id in line_ids_seen:
                    analysis_logger.warning(f"Duplicate lineId found: {line_id} - possible data quality issue")
                else:
                    line_ids_seen.add(line_id)
                    lines.append(ind)
    else:
        # Use proper class-based detection
        for ind in onto.individuals():
            if isinstance(ind, production_line_class):
                lines.append(ind)
    
    if not lines:
        analysis_logger.warning("No production lines found in ontology")
        return "No production lines found in ontology"
    
    analysis_logger.info(f"Found {len(lines)} production lines")
    
    # Log some line IDs for verification
    sample_size = min(5, len(lines))
    sample_lines = lines[:sample_size]
    sample_ids = [getattr(line, "lineId", line.name) for line in sample_lines]
    analysis_logger.info(f"Sample line IDs: {', '.join(map(str, sample_ids))}")
    
    report_lines = []
    report_lines.append("\n=== EQUIPMENT SEQUENCE REPORT ===")
    
    # Use safe sort for lines to avoid None comparison errors
    try:
        sorted_lines = _safe_sort_by_attribute(lines, "lineId")
    except Exception as e:
        analysis_logger.error(f"Error sorting lines: {e} - using unsorted lines")
        sorted_lines = lines
    
    for line in sorted_lines:
        line_id = getattr(line, "lineId", line.name)
        report_lines.append(f"\nLine: {line_id}")
        
        # Get sequence for this line
        sequence = get_equipment_sequence_for_line(onto, line)
        
        if not sequence:
            report_lines.append("  No equipment sequence found")
            continue
        
        # List equipment in sequence
        for i, eq in enumerate(sequence, 1):
            eq_id = getattr(eq, "equipmentId", "Unknown")
            eq_name = getattr(eq, "equipmentName", eq.name)
            eq_class = "Unknown"
            
            # Get class information
            if hasattr(eq, "memberOfClass") and eq.memberOfClass:
                if hasattr(eq.memberOfClass, "equipmentClassId"):
                    eq_class = eq.memberOfClass.equipmentClassId
            
            report_lines.append(f"  {i}. {eq_id} ({eq_name}) - Class: {eq_class}")
    
    return "\n".join(report_lines)

def analyze_equipment_sequences(onto: Ontology) -> Tuple[Dict[str, List[Thing]], Dict[str, Dict[str, Any]]]:
    """
    Analyzes all equipment sequences in the ontology and returns detailed information.
    
    Args:
        onto: The ontology object
        
    Returns:
        Tuple containing:
        - Dictionary mapping line IDs to equipment sequences
        - Dictionary with sequence statistics and diagnostics
    """
    analysis_logger.info("Analyzing equipment sequences in ontology")
    
    # Find all production lines using class-based detection
    lines = []
    
    # Get the ProductionLine class
    production_line_class = None
    for cls in onto.classes():
        if cls.name == "ProductionLine":
            production_line_class = cls
            break
    
    if production_line_class:
        for ind in onto.individuals():
            if isinstance(ind, production_line_class):
                lines.append(ind)
    else:
        # Fallback to property-based detection
        analysis_logger.warning("ProductionLine class not found in ontology - using lineId property")
        for ind in onto.individuals():
            if hasattr(ind, "lineId"):
                lines.append(ind)
    
    if not lines:
        analysis_logger.warning("No production lines found in ontology")
        return {}, {"error": "No production lines found"}
    
    # Get essential ontology elements
    equipment_class = None
    for cls in onto.classes():
        if cls.name == "Equipment":
            equipment_class = cls
            break
            
    if not equipment_class:
        analysis_logger.warning("Equipment class not found in ontology")
        return {}, {"error": "Equipment class not found"}
        
    # Get the "equipmentIsUpstreamOf" property
    equipment_is_upstream_of = None
    for prop in onto.object_properties():
        if prop.name == "equipmentIsUpstreamOf":
            equipment_is_upstream_of = prop
            break
            
    # Generate sequences for each line
    sequences = {}
    stats = {
        "total_lines": len(lines), 
        "lines_with_sequence": 0,
        "lines_without_sequence": 0,
        "total_equipment": 0, 
        "class_counts": {},
        "lines_with_equipment_but_no_sequence": [],
        "classes_without_sequence_position": set(),
        "equipment_without_sequence_by_line": {}
    }
    
    # Get all equipment individuals
    all_equipment = list(onto.search(type=equipment_class))
    stats["total_equipment"] = len(all_equipment)
    
    # Track equipment classes and their sequence positions
    class_sequence_positions = {}
    for equip in all_equipment:
        if hasattr(equip, "memberOfClass") and equip.memberOfClass:
            class_ind = equip.memberOfClass
            class_id = getattr(class_ind, "equipmentClassId", class_ind.name)
            
            # Count equipment by class
            if class_id not in stats["class_counts"]:
                stats["class_counts"][class_id] = 0
            stats["class_counts"][class_id] += 1
            
            # Check sequence position
            if class_id not in class_sequence_positions:
                position = getattr(class_ind, "defaultSequencePosition", None)
                class_sequence_positions[class_id] = position
                if position is None:
                    stats["classes_without_sequence_position"].add(class_id)
    
    # Process each line
    for line in lines:
        line_id = getattr(line, "lineId", line.name)
        line_id_str = str(line_id[0]) if isinstance(line_id, list) and line_id else str(line_id)
        sequence = get_equipment_sequence_for_line(onto, line)
        
        # Map line ID to its equipment sequence
        sequences[line_id_str] = sequence
        
        # Track equipment on this line that don't have a sequence
        equipment_on_line = []
        for equip in all_equipment:
            if hasattr(equip, "isPartOfProductionLine"):
                lines_list = equip.isPartOfProductionLine
                if not isinstance(lines_list, list):
                    lines_list = [lines_list] if lines_list else []
                    
                if line in lines_list:
                    equipment_on_line.append(equip)
        
        # Check if this line has equipment but no sequence
        if equipment_on_line and not sequence:
            stats["lines_without_sequence"] += 1
            stats["lines_with_equipment_but_no_sequence"].append(line_id_str)
            
            # Track equipment on this line without a sequence link
            stats["equipment_without_sequence_by_line"][line_id_str] = []
            for equip in equipment_on_line:
                # Get class information
                class_name = "Unknown"
                if hasattr(equip, "memberOfClass") and equip.memberOfClass:
                    class_name = getattr(equip.memberOfClass, "equipmentClassId", equip.memberOfClass.name)
                
                # Add to the list
                equip_id = getattr(equip, "equipmentId", equip.name)
                stats["equipment_without_sequence_by_line"][line_id_str].append({
                    "id": equip_id,
                    "name": getattr(equip, "equipmentName", equip.name),
                    "class": class_name,
                    "class_has_position": class_name in class_sequence_positions and class_sequence_positions[class_name] is not None
                })
        
        # Add to statistics
        if sequence:
            stats["lines_with_sequence"] += 1
            
    # Add sequence position information to statistics
    stats["classes_with_position"] = sum(1 for pos in class_sequence_positions.values() if pos is not None)
    stats["classes_without_position"] = sum(1 for pos in class_sequence_positions.values() if pos is None)
    stats["class_positions"] = {cls: pos for cls, pos in class_sequence_positions.items() if pos is not None}
    
    # Convert set to list for JSON serialization
    stats["classes_without_sequence_position"] = list(stats["classes_without_sequence_position"])
    
    analysis_logger.info(f"Sequence analysis complete: {stats['lines_with_sequence']} lines with sequences, "
                          f"{stats['lines_without_sequence']} lines without sequences, "
                          f"{len(stats['classes_without_sequence_position'])} classes without positions")
    
    return sequences, stats

def generate_enhanced_sequence_report(onto: Ontology) -> str:
    """
    Generates an enhanced report of equipment sequences and related issues.
    
    Args:
        onto: The ontology object
        
    Returns:
        A string with the enhanced sequence report
    """
    # Get sequence data and statistics
    sequences, stats = analyze_equipment_sequences(onto)
    
    # Build report
    report_lines = []
    report_lines.append("\n=== ENHANCED EQUIPMENT SEQUENCE REPORT ===")
    
    # Summary statistics
    report_lines.append(f"\nSUMMARY STATISTICS:")
    report_lines.append(f"  Total production lines: {stats['total_lines']}")
    report_lines.append(f"  Lines with equipment sequences: {stats['lines_with_sequence']}")
    report_lines.append(f"  Lines with equipment but no sequence: {len(stats.get('lines_with_equipment_but_no_sequence', []))}")
    report_lines.append(f"  Total equipment instances: {stats['total_equipment']}")
    report_lines.append(f"  Equipment classes without sequence positions: {len(stats.get('classes_without_sequence_position', []))}")
    
    # Class sequence positions
    report_lines.append(f"\nEQUIPMENT CLASS SEQUENCE POSITIONS:")
    for cls, pos in sorted(stats.get('class_positions', {}).items(), key=lambda x: x[1]):
        report_lines.append(f"  {cls}: Position {pos}")
    
    # Classes without positions
    if stats.get('classes_without_sequence_position'):
        report_lines.append(f"\nEQUIPMENT CLASSES WITHOUT SEQUENCE POSITIONS:")
        for cls in sorted(stats.get('classes_without_sequence_position', [])):
            report_lines.append(f"  {cls}")
    
    # Lines with equipment but no sequence
    if stats.get('lines_with_equipment_but_no_sequence'):
        report_lines.append(f"\nLINES WITH EQUIPMENT BUT NO SEQUENCE ESTABLISHED:")
        for line_id in sorted(stats.get('lines_with_equipment_but_no_sequence', [])):
            report_lines.append(f"  Line {line_id}:")
            equip_list = stats.get('equipment_without_sequence_by_line', {}).get(line_id, [])
            for eq in equip_list:
                class_status = "No position" if not eq['class_has_position'] else "Has position"
                report_lines.append(f"    {eq['id']} ({eq['name']}) - Class: {eq['class']} ({class_status})")
    
    # Equipment sequences by line
    report_lines.append(f"\nEQUIPMENT SEQUENCES BY LINE:")
    for line_id, sequence in sorted(sequences.items()):
        report_lines.append(f"\nLine: {line_id}")
        
        if not sequence:
            report_lines.append("  No equipment sequence found")
            continue
        
        # List equipment in sequence
        for i, eq in enumerate(sequence, 1):
            eq_id = getattr(eq, "equipmentId", "Unknown")
            eq_name = getattr(eq, "equipmentName", eq.name)
            eq_class = "Unknown"
            
            # Get class information
            if hasattr(eq, "memberOfClass") and eq.memberOfClass:
                if hasattr(eq.memberOfClass, "equipmentClassId"):
                    eq_class = eq.memberOfClass.equipmentClassId
            
            report_lines.append(f"  {i}. {eq_id} ({eq_name}) - Class: {eq_class}")
    
    return "\n".join(report_lines) 

===========================================
FILE: ontology_generator/config.py
===========================================

"""
Ontology Generator Configuration

This module contains constants, mappings, and configuration settings for the ontology generator.
It defines column names, equipment sequences, language mappings, and various other configuration
parameters used throughout the ontology generation process.
"""
from typing import Dict, Any, Type, Optional
from datetime import datetime, date, time
import logging

# -----------------------------------------------------------------------------
# GENERAL CONFIGURATION
# -----------------------------------------------------------------------------
DEFAULT_ONTOLOGY_IRI = "http://example.com/manufacturing_ontology.owl"
LOG_FORMAT = '%(asctime)s - %(levelname)s - %(name)s - %(message)s'
SPEC_PARENT_CLASS_COLUMN = 'Parent Class'  # Column name for hierarchy definition

# -----------------------------------------------------------------------------
# SPECIFICATION COLUMN NAMES
# -----------------------------------------------------------------------------
# Entity and property identification
SPEC_COL_ENTITY = "Proposed OWL Entity"
SPEC_COL_PROPERTY = "Proposed OWL Property"
SPEC_COL_PROP_TYPE = "OWL Property Type"

# Property details
SPEC_COL_RAW_DATA = "Raw Data Column Name"
SPEC_COL_TARGET_RANGE = "Target/Range (xsd:) / Target Class"
SPEC_COL_PROP_CHARACTERISTICS = "OWL Property Characteristics"
SPEC_COL_INVERSE_PROPERTY = "Inverse Property"
SPEC_COL_DOMAIN = "Domain"
SPEC_COL_TARGET_LINK_CONTEXT = "Target Link Context"
SPEC_COL_PROGRAMMATIC = "Programmatic"

# Classification and documentation
SPEC_COL_LOGICAL_GROUP = "Logical Group"
SPEC_COL_NOTES = "Notes/Considerations"
SPEC_COL_ISA95_CONCEPT = "ISA-95 Concept"

# -----------------------------------------------------------------------------
# LOGGING CONFIGURATION
# -----------------------------------------------------------------------------
# Warning messages to suppress in logs
SUPPRESSED_WARNINGS = [
    "Equipment.actualSequencePosition is missing 'column'",
    "EquipmentClass.defaultSequencePosition is missing 'column'",
    "No equipment instance relationships were created or verified",
    "Context entity 'EquipmentCapability' required for Equipment.hasCapability",
    "Context entity 'EventRecord' required for Material.materialUsedIn",
    "Context entity 'EventRecord' required for OperationalReason.reasonForEvent",
    "Context entity 'EventRecord' required for OperationalState.stateOfEvent",
    "Context entity 'EventRecord' required for ProductionRequest.hasAssociatedEvent",
    "Context entity 'EventRecord' required for Shift.includesEvent",
    "Context entity 'Person' required for EventRecord.performedBy",
    "Created new individual",
    "Context entity 'Material' required for EventRecord.consumedMaterial not found",
    "Context entity 'Material' required for EventRecord.producedMaterial not found",
    "Context entity 'ProductionRequest' required for EventRecord.associatedRequest not found",
    "Successfully linked EventRecord",
    "Successfully linking Equipment",
    "Linked (Start-Time Containment):"
]

class MessageFilter(logging.Filter):
    """
    A logging filter that suppresses specific log messages at any level
    """
    def __init__(self, suppressed_messages):
        super().__init__()
        self.suppressed_messages = suppressed_messages
        
    def filter(self, record):
        # Return False to suppress the message
        for msg in self.suppressed_messages:
            if msg in record.getMessage():
                return False
        return True

def setup_logging_filters():
    """
    Set up logging filters for all configured levels.
    This should be called during initial setup.
    """
    # Create the message filter with our suppressed warnings
    message_filter = MessageFilter(SUPPRESSED_WARNINGS)
    
    # Get the root logger and add the filter
    root_logger = logging.getLogger()
    root_logger.addFilter(message_filter)
    
    # Add filters to specific logger instances
    loggers = [
        "ontology_generator.population",
        "ontology_generator.population.row_processor",
        "ontology_generator.population.core",
        "event_linking"
    ]
    
    for logger_name in loggers:
        logger = logging.getLogger(logger_name)
        logger.addFilter(message_filter)

# -----------------------------------------------------------------------------
# LANGUAGE CONFIGURATION
# -----------------------------------------------------------------------------
# Mapping from country descriptions to BCP 47 language tags
COUNTRY_TO_LANGUAGE: Dict[str, str] = {
    "Mexico": "es",
    "United States": "en",
    "Brazil": "pt",
    "France": "fr",
    "Germany": "de",
    "Italy": "it",
    "Spain": "es",
    "Japan": "ja",
    "China": "zh"
}
DEFAULT_LANGUAGE = "en"  # Default language if country not found in mapping

# -----------------------------------------------------------------------------
# EQUIPMENT CONFIGURATION
# -----------------------------------------------------------------------------
# Default equipment sequence represents typical physical order on manufacturing line
DEFAULT_EQUIPMENT_SEQUENCE: Dict[str, int] = {
    "Filler": 1,        # First in typical sequence
    "Cartoner": 2,      # Second in typical sequence
    "Bundler": 3,       # Third in typical sequence
    "CaseFormer": 4,    # Fourth in typical sequence
    "CasePacker": 5,    # Fifth in typical sequence
    "CaseSealer": 6,    # Sixth in typical sequence
    "Palletizer": 7     # Last in typical sequence
}

# Known equipment classes for identification and matching
KNOWN_EQUIPMENT_CLASSES = list(DEFAULT_EQUIPMENT_SEQUENCE.keys())

# Maps specific patterns in equipment names to their classes
EQUIPMENT_NAME_TO_CLASS_MAP = {
    "_Filler": "Filler",
    "_Cartoner": "Cartoner",
    "_Bundler": "Bundler",
    "_CaseFormer": "CaseFormer",
    "_CasePacker": "CasePacker",
    "_CaseSealer": "CaseSealer",
    "_Palletizer": "Palletizer"
}

# Line-specific equipment sequences that override the default
LINE_SPECIFIC_EQUIPMENT_SEQUENCE: Dict[str, Dict[str, int]] = {
    "Line1": {
        "Filler": 1,        # First position
        "Cartoner": 2,      # Second position
        "Bundler": 3,       # Third position
        "CaseFormer": 4,    # Fourth position
        "CasePacker": 5,    # Fifth position
        "CaseSealer": 6,    # Sixth position
        "Palletizer": 7     # Seventh position
    }
}

# -----------------------------------------------------------------------------
# XSD TYPE MAPPING
# -----------------------------------------------------------------------------
# Initialized when importing required modules to avoid circular imports
XSD_TYPE_MAP: Dict[str, Type] = {}

def init_xsd_type_map(locstr_type: Any) -> None:
    """
    Initialize the XSD type mapping with the owlready2 locstr type.
    This should be called after owlready2 is imported.
    
    Args:
        locstr_type: The owlready2 locstr type
    """
    global XSD_TYPE_MAP
    
    XSD_TYPE_MAP.update({
        "xsd:string": str,
        "xsd:decimal": float,
        "xsd:double": float,
        "xsd:float": float,
        "xsd:integer": int,
        "xsd:int": int,
        "xsd:long": int,
        "xsd:short": int,
        "xsd:byte": int,
        "xsd:nonNegativeInteger": int,
        "xsd:positiveInteger": int,
        "xsd:negativeInteger": int,
        "xsd:nonPositiveInteger": int,
        "xsd:unsignedLong": int,
        "xsd:unsignedInt": int,
        "xsd:unsignedShort": int,
        "xsd:unsignedByte": int,
        "xsd:dateTime": datetime,
        "xsd:date": date,
        "xsd:time": time,
        "xsd:boolean": bool,
        "xsd:anyURI": str,
        "xsd:string (with lang tag)": locstr_type
    })


===========================================
FILE: ontology_generator/definition/__init__.py
===========================================

from .parser import (
    parse_specification, parse_property_mappings, validate_property_mappings,
    read_data
)
from .structure import define_ontology_structure, create_selective_classes


===========================================
FILE: ontology_generator/definition/ontology_comp
===========================================



===========================================
FILE: ontology_generator/definition/parser.py
===========================================

"""
Specification parsing module for the ontology generator.

This module provides functions for parsing the ontology specification file.
"""
import csv
from collections import defaultdict
from typing import List, Dict, Any, Optional

from ontology_generator.utils.logging import logger
from ontology_generator.config import (
    SPEC_COL_ENTITY, SPEC_COL_PROPERTY, SPEC_COL_PROP_TYPE,
    SPEC_COL_RAW_DATA, SPEC_COL_TARGET_RANGE, SPEC_COL_PROP_CHARACTERISTICS,
    SPEC_COL_INVERSE_PROPERTY, SPEC_COL_DOMAIN, SPEC_COL_TARGET_LINK_CONTEXT,
    SPEC_COL_PROGRAMMATIC, SPEC_COL_NOTES
)

def parse_specification(spec_file_path: str) -> List[Dict[str, str]]:
    """
    Parses the ontology specification CSV file.
    
    Args:
        spec_file_path: Path to the specification CSV file
        
    Returns:
        A list of dictionaries representing the specification rows
    """
    logger.info(f"Parsing specification file: {spec_file_path}")
    spec_list: List[Dict[str, str]] = []
    try:
        with open(spec_file_path, mode='r', encoding='utf-8-sig') as infile:  # Use utf-8-sig to handle potential BOM
            reader = csv.DictReader(infile)
            # Basic check for expected columns (optional but recommended)
            # expected_cols = {'Proposed OWL Entity', 'Proposed OWL Property', 'Parent Class', ...}
            # if not expected_cols.issubset(reader.fieldnames):
            #     logger.warning(f"Specification file might be missing expected columns. Found: {reader.fieldnames}")
            spec_list = list(reader)
            logger.info(f"Successfully parsed {len(spec_list)} rows from specification.")
            return spec_list
    except FileNotFoundError:
        logger.error(f"Specification file not found: {spec_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error parsing specification file {spec_file_path}: {e}")
        raise
    return []  # Return empty list on error if not raising

def parse_property_mappings(specification: List[Dict[str, str]]) -> Dict[str, Dict[str, Dict[str, Any]]]:
    """
    Parses the ontology specification to extract property-to-column mappings.
    
    Args:
        specification: The parsed specification list of dictionaries
        
    Returns:
        A nested dictionary with the structure:
        {
            'EntityName': {
                'data_properties': {
                    'propertyName': {
                        'column': 'RAW_DATA_COLUMN',
                        'data_type': 'xsd:type',
                        'functional': True/False,
                        'programmatic': True/False
                    }
                },
                'object_properties': {
                    'propertyName': {
                        'column': 'RAW_DATA_COLUMN',
                        'target_class': 'TargetClassName',
                        'functional': True/False,
                        'programmatic': True/False
                    }
                }
            }
        }
    """
    logger.info("Parsing property mappings from specification")
    mappings = defaultdict(lambda: {'data_properties': {}, 'object_properties': {}})
    
    # Get fieldnames to check for the new optional column
    fieldnames = []
    try:
        with open(specification[0]['_source_file_path_'], mode='r', encoding='utf-8-sig') as infile: # Assuming spec is not empty and comes from a file
             reader = csv.DictReader(infile)
             fieldnames = reader.fieldnames or []
    except Exception:
        # Fallback: Check the first row keys if reading file fails or spec is not from file
        if specification:
             fieldnames = list(specification[0].keys())
             
    has_target_link_context_col = SPEC_COL_TARGET_LINK_CONTEXT in fieldnames
    if not has_target_link_context_col:
        logger.warning(f"Specification file does not contain the '{SPEC_COL_TARGET_LINK_CONTEXT}' column. Context-based object property links may not be parsed.")
        
    has_programmatic_col = SPEC_COL_PROGRAMMATIC in fieldnames
    if not has_programmatic_col:
        logger.warning(f"Specification file does not contain the '{SPEC_COL_PROGRAMMATIC}' column. Programmatically-populated properties may not validate correctly.")

    for row_num, row in enumerate(specification):
        entity = row.get(SPEC_COL_ENTITY, '').strip()
        property_name = row.get(SPEC_COL_PROPERTY, '').strip()
        property_type = row.get(SPEC_COL_PROP_TYPE, '').strip()
        raw_data_col = row.get(SPEC_COL_RAW_DATA, '').strip()
        
        # Add source file path if not already present (useful for validation/debugging)
        if '_source_file_path_' not in row and hasattr(specification, '_source_file_path_'): 
             row['_source_file_path_'] = specification._source_file_path_ # Propagate if available
             
        # Skip if any essential fields are missing
        if not entity or not property_name:
            continue
            
        # Skip if property type is missing or invalid
        if property_type not in ['DatatypeProperty', 'ObjectProperty']:
            logger.warning(f"Skipping row {row_num+1}: Invalid or missing OWL Property Type '{property_type}' for {entity}.{property_name}")
            continue
            
        # Raw data col is optional for object properties if target link context is provided
        raw_data_col_is_na = not raw_data_col or raw_data_col.upper() == 'N/A'
        
        # Determine if the property is functional
        is_functional = 'Functional' in row.get(SPEC_COL_PROP_CHARACTERISTICS, '')
        
        # Determine if the property is populated programmatically - Fix for None issue
        programmatic_value = row.get(SPEC_COL_PROGRAMMATIC, '')
        is_programmatic = False
        if programmatic_value is not None and str(programmatic_value).strip().lower() == 'true':
            is_programmatic = True
        
        # Process data properties
        if property_type == 'DatatypeProperty':
            data_type = row.get(SPEC_COL_TARGET_RANGE, '').strip()
            # Create mapping info dictionary
            map_info = {
                'data_type': data_type,
                'functional': is_functional,
                'programmatic': is_programmatic
            }
            # Conditionally add the 'column' key
            if not raw_data_col_is_na:
                map_info['column'] = raw_data_col
                logger.debug(f"Mapped {entity}.{property_name} (DatatypeProperty) to column '{raw_data_col}', type '{data_type}'")
            else:
                # Log definition without mapping
                logger.debug(f"Defined {entity}.{property_name} (DatatypeProperty) type '{data_type}' but no data column mapping.")
            
            # Add to mappings regardless of column presence
            mappings[entity]['data_properties'][property_name] = map_info
            
        # Process object properties
        elif property_type == 'ObjectProperty':
            target_class = row.get(SPEC_COL_TARGET_RANGE, '').strip()
            target_link_context = row.get(SPEC_COL_TARGET_LINK_CONTEXT, '').strip() if has_target_link_context_col else ''
            
            # Initialize mapping info
            map_info = {
                'target_class': target_class,
                'functional': is_functional,
                'programmatic': is_programmatic
            }

            # Check if there's a way to populate/link this property later
            can_populate = False
            if not raw_data_col_is_na:
                # Prefer column mapping if available
                map_info['column'] = raw_data_col
                can_populate = True
                logger.debug(f"Mapped {entity}.{property_name} (ObjectProperty) to column '{raw_data_col}', target '{target_class}'")
                # Warn if context is also provided but will be ignored
                if target_link_context:
                    logger.warning(f"Row {row_num+1}: Both '{SPEC_COL_RAW_DATA}' ('{raw_data_col}') and '{SPEC_COL_TARGET_LINK_CONTEXT}' ('{target_link_context}') provided for {entity}.{property_name}. Prioritizing column lookup.")
            elif target_link_context:
                # Use context mapping if column is not available
                map_info['target_link_context'] = target_link_context
                can_populate = True
                logger.debug(f"Mapped {entity}.{property_name} (ObjectProperty) via context '{target_link_context}', target '{target_class}'")
            elif is_programmatic:
                # Property is populated programmatically
                can_populate = True
                logger.debug(f"Defined {entity}.{property_name} (ObjectProperty) target '{target_class}' to be populated programmatically.")
            else:
                 # Defined but cannot be populated from data/context
                 logger.debug(f"Defined {entity}.{property_name} (ObjectProperty) target '{target_class}' but no column or context for mapping.")

            # Add to mappings regardless of populatability, including mapping info if available
            mappings[entity]['object_properties'][property_name] = map_info
    
    # Convert defaultdict to regular dict for return
    return {k: {'data_properties': dict(v['data_properties']), 
                'object_properties': dict(v['object_properties'])} 
            for k, v in mappings.items()}

def validate_property_mappings(property_mappings: Dict[str, Dict[str, Dict[str, Any]]]) -> bool:
    """
    Validates property mappings and logs information for debugging.
    
    Args:
        property_mappings: Property mapping dictionary from parse_property_mappings
        
    Returns:
        bool: True if validation passed, False otherwise
    """
    logger.info("Validating property mappings...")
    
    if not property_mappings:
        logger.error("Property mappings dictionary is empty!")
        return False
    
    validation_passed = True
    entity_count = 0
    data_prop_count = 0
    object_prop_count = 0
    
    # Log summary
    logger.info(f"Found mappings for {len(property_mappings)} entities")
    
    # Check each entity
    for entity_name, entity_props in sorted(property_mappings.items()):
        entity_count += 1
        data_properties = entity_props.get('data_properties', {})
        object_properties = entity_props.get('object_properties', {})
        
        # Count properties
        data_prop_count += len(data_properties)
        object_prop_count += len(object_properties)
        
        # Log entity details
        logger.info(f"Entity: {entity_name} - {len(data_properties)} data properties, {len(object_properties)} object properties")
        
        # Log data properties
        if data_properties:
            logger.debug(f"  Data Properties for {entity_name}:")
            for prop_name, details in sorted(data_properties.items()):
                column = details.get('column', 'MISSING_COLUMN')
                data_type = details.get('data_type', 'MISSING_TYPE')
                functional = details.get('functional', False)
                programmatic = details.get('programmatic', False)
                
                logger.debug(f"    {prop_name}: column='{column}', type='{data_type}', functional={functional}, programmatic={programmatic}")
                
                # Validate required fields
                if not column and not programmatic and not data_type:
                    logger.warning(f"Missing required field for {entity_name}.{prop_name}: column='{column}', type='{data_type}'")
                    validation_passed = False
        
        # Log object properties
        if object_properties:
            logger.debug(f"  Object Properties for {entity_name}:")
            for prop_name, details in sorted(object_properties.items()):
                column = details.get('column', None) # Changed default to None
                target = details.get('target_class', 'MISSING_TARGET')
                functional = details.get('functional', False)
                link_context = details.get('target_link_context', None) # Added context check
                programmatic = details.get('programmatic', False) # Check for programmatic flag

                log_msg = f"    {prop_name}: target='{target}', functional={functional}"
                if column:
                     log_msg += f", column='{column}'"
                if link_context:
                     log_msg += f", context='{link_context}'"
                if programmatic:
                     log_msg += f", programmatic=True"
                logger.debug(log_msg)
                
                # Validate required fields
                if not target:
                    logger.warning(f"Missing required field target_class for {entity_name}.{prop_name}")
                    validation_passed = False
                # Must have either column or context or be programmatic
                if not column and not link_context and not programmatic:
                    logger.warning(f"Missing required field: Needs 'column', 'target_link_context', or 'Programmatic=True' for {entity_name}.{prop_name}")
                    validation_passed = False
    
    # Check for EventRecord specifically
    if 'EventRecord' not in property_mappings:
        logger.warning("No mappings found for 'EventRecord' entity (the main focus of this change)")
        validation_passed = False
    else:
        # Check for common EventRecord properties
        event_props = property_mappings['EventRecord'].get('data_properties', {})
        # TKT-006: Expanded check for AE model metrics
        expected_props = [
            'downtimeMinutes', 
            'runTimeMinutes', 
            'effectiveRuntimeMinutes', 
            'reportedDurationMinutes',
            'goodProductionQuantity',
            'rejectProductionQuantity',
            'allMaintenanceTimeMinutes'
        ]
        missing_props = [p for p in expected_props if p not in event_props]
        
        if missing_props:
            logger.warning(f"Some expected EventRecord AE model properties are missing from mappings: {missing_props}")
            # Don't fail validation for this, but log the warning
        
        # TKT-006: Verify that xsd:double is used for time metrics and xsd:integer for quantities
        time_metrics = [
            'downtimeMinutes', 
            'runTimeMinutes', 
            'effectiveRuntimeMinutes', 
            'reportedDurationMinutes',
            'allMaintenanceTimeMinutes'
        ]
        quantity_metrics = [
            'goodProductionQuantity',
            'rejectProductionQuantity'
        ]
        
        for prop in time_metrics:
            if prop in event_props:
                data_type = event_props[prop].get('data_type')
                if data_type not in ['xsd:double', 'xsd:decimal', 'xsd:float']:
                    logger.warning(f"EventRecord time metric '{prop}' should use 'xsd:double' data type, found '{data_type}'")
        
        for prop in quantity_metrics:
            if prop in event_props:
                data_type = event_props[prop].get('data_type')
                if data_type not in ['xsd:integer', 'xsd:int']:
                    logger.warning(f"EventRecord quantity metric '{prop}' should use 'xsd:integer' data type, found '{data_type}'")

    # Log summary stats
    logger.info(f"Property mapping validation complete. Found {entity_count} entities, {data_prop_count} data properties, {object_prop_count} object properties.")
    logger.info(f"Validation {'PASSED' if validation_passed else 'FAILED'}")
    
    return validation_passed

def read_data(data_file_path: str) -> List[Dict[str, str]]:
    """
    Reads the operational data CSV file.
    
    Args:
        data_file_path: Path to the data CSV file
        
    Returns:
        A list of dictionaries representing the data rows
    """
    logger.info(f"Reading data file: {data_file_path}")
    data_rows: List[Dict[str, str]] = []
    try:
        with open(data_file_path, mode='r', encoding='utf-8-sig') as infile:
            reader = csv.DictReader(infile)
            data_rows = list(reader)
            logger.info(f"Successfully read {len(data_rows)} data rows.")
            return data_rows
    except FileNotFoundError:
        logger.error(f"Data file not found: {data_file_path}")
        raise
    except Exception as e:
        logger.error(f"Error reading data file {data_file_path}: {e}")
        raise
    return []  # Return empty list on error if not raising


===========================================
FILE: ontology_generator/definition/structure.py
===========================================

"""
Ontology structure definition module for the ontology generator.

This module provides functions for defining the ontology structure.
"""
import re
import types
from typing import Dict, List, Tuple, Set, Optional, Any

from owlready2 import (
    Ontology, Thing, Nothing, ThingClass, PropertyClass,
    FunctionalProperty, InverseFunctionalProperty, TransitiveProperty,
    SymmetricProperty, AsymmetricProperty, ReflexiveProperty, IrreflexiveProperty,
    ObjectProperty, DataProperty
)

from ontology_generator.utils.logging import logger
from ontology_generator.config import (
    SPEC_PARENT_CLASS_COLUMN, XSD_TYPE_MAP,
    SPEC_COL_ENTITY, SPEC_COL_PROPERTY, SPEC_COL_PROP_TYPE,
    SPEC_COL_RAW_DATA, SPEC_COL_TARGET_RANGE, SPEC_COL_PROP_CHARACTERISTICS,
    SPEC_COL_INVERSE_PROPERTY, SPEC_COL_DOMAIN, SPEC_COL_TARGET_LINK_CONTEXT,
    SPEC_COL_PROGRAMMATIC, SPEC_COL_NOTES, SPEC_COL_ISA95_CONCEPT
)

def define_ontology_structure(onto: Ontology, specification: List[Dict[str, str]]) -> Tuple[Dict[str, ThingClass], Dict[str, PropertyClass], Dict[str, bool]]:
    """
    Defines OWL classes and properties based on the parsed specification.

    Args:
        onto: The ontology to define the structure in
        specification: The parsed specification
        
    Returns:
        tuple: (defined_classes, defined_properties, property_is_functional)
            - defined_classes: Dict mapping class name to owlready2 class object.
            - defined_properties: Dict mapping property name to owlready2 property object.
            - property_is_functional: Dict mapping property name to boolean indicating functionality.
    """
    logger.info(f"Defining ontology structure in: {onto.base_iri}")
    defined_classes: Dict[str, ThingClass] = {}
    defined_properties: Dict[str, PropertyClass] = {}
    property_is_functional: Dict[str, bool] = {}  # Track which properties are functional based on spec
    class_metadata: Dict[str, Dict[str, Any]] = {} # Store metadata like notes per class

    # --- Pre-process Spec for Class Metadata and Hierarchy ---
    logger.debug("--- Pre-processing specification for class details ---")
    all_class_names: Set[str] = set()
    class_parents: Dict[str, str] = {} # {child_name: parent_name}
    for i, row in enumerate(specification):
        class_name = row.get(SPEC_COL_ENTITY, '').strip()
        if class_name:
            all_class_names.add(class_name)
            # Store metadata (using first encountered row for simplicity, could collect all)
            if class_name not in class_metadata:
                    class_metadata[class_name] = {
                        'notes': row.get(SPEC_COL_NOTES, ''),
                        'isa95': row.get(SPEC_COL_ISA95_CONCEPT, ''),
                        'row_index': i # For reference if needed
                    }
            # Store parent class info if column exists
            parent_name = row.get(SPEC_PARENT_CLASS_COLUMN, '').strip()
            if parent_name and parent_name != class_name: # Avoid self-parenting
                class_parents[class_name] = parent_name
                all_class_names.add(parent_name) # Ensure parent is also considered a class


    # --- Pass 1: Define Classes with Hierarchy ---
    logger.debug("--- Defining Classes ---")
    with onto:
        # Ensure Thing is available if not explicitly listed
        if "Thing" not in all_class_names and "owl:Thing" not in all_class_names:
            pass # Thing is implicitly available via owlready2

        defined_order: List[str] = [] # Track definition order for hierarchy
        definition_attempts = 0
        max_attempts = len(all_class_names) + 5 # Allow some leeway for complex hierarchies

        classes_to_define: Set[str] = set(cn for cn in all_class_names if cn.lower() != "owl:thing") # Exclude Thing variants

        while classes_to_define and definition_attempts < max_attempts:
            defined_in_pass: Set[str] = set()
            for class_name in sorted(list(classes_to_define)): # Sort for somewhat deterministic order
                parent_name = class_parents.get(class_name)
                parent_class_obj: ThingClass = Thing # Default parent is Thing

                if parent_name:
                    if parent_name == "Thing" or parent_name.lower() == "owl:thing": # Handle case variation
                        parent_class_obj = Thing
                    elif parent_name in defined_classes:
                        parent_class_obj = defined_classes[parent_name]
                    else:
                        # Parent not defined yet, skip this class for now
                        logger.debug(f"Deferring class '{class_name}', parent '{parent_name}' not defined yet.")
                        continue

                # Define the class
                try:
                    if class_name not in defined_classes:
                        logger.debug(f"Attempting to define Class: {class_name} with Parent: {parent_class_obj.name}")
                        # Ensure class name is valid Python identifier if needed by backend
                        safe_class_name = re.sub(r'\W|^(?=\d)', '_', class_name)
                        if safe_class_name != class_name:
                            logger.warning(f"Class name '{class_name}' sanitized to '{safe_class_name}' for internal use. Using original name for IRI.")
                            # Sticking with original name as owlready2 often handles non-standard chars in IRIs

                        # Revert to types.new_class
                        new_class: ThingClass = types.new_class(class_name, (parent_class_obj,))

                        defined_classes[class_name] = new_class
                        defined_order.append(class_name)
                        defined_in_pass.add(class_name)
                        logger.debug(f"Defined Class: {new_class.iri} (Parent: {parent_class_obj.iri})") # Removed the extra type check log

                        # Add annotations like comments/labels from pre-processed metadata
                        meta = class_metadata.get(class_name)
                        if meta:
                            comments = []
                            if meta['notes']: comments.append(f"Notes: {meta['notes']}")
                            if meta['isa95']: comments.append(f"ISA-95 Concept: {meta['isa95']}")
                            if comments:
                                new_class.comment = comments
                                logger.debug(f"Added comments to class {class_name}")

                except Exception as e:
                    logger.error(f"Error defining class '{class_name}' with parent '{getattr(parent_class_obj,'name','N/A')}': {e}")
                    # Let it retry, might be a transient issue or solvable in later pass

            classes_to_define -= defined_in_pass
            definition_attempts += 1
            if not defined_in_pass and classes_to_define:
                logger.error(f"Could not define remaining classes (possible circular dependency or missing parents): {classes_to_define}")
                break # Avoid infinite loop

        if classes_to_define:
            logger.warning(f"Failed to define the following classes: {classes_to_define}")

    # --- Pass 2: Define Properties ---
    logger.debug("--- Defining Properties ---")
    properties_to_process = [row for row in specification if row.get(SPEC_COL_PROPERTY)]
    temp_inverse_map: Dict[str, str] = {} # Stores {prop_name: inverse_name}

    # Define instance-level equipment sequence properties if not in specification
    with onto:
        # Define instance-level sequence properties for Equipment
        if defined_classes.get("Equipment") and "sequencePosition" not in defined_properties:
            logger.info("Adding instance-level equipment sequence properties")
            
            # Get Equipment class
            cls_Equipment = defined_classes.get("Equipment")
            cls_ProductionLine = defined_classes.get("ProductionLine")
            cls_EquipmentClass = defined_classes.get("EquipmentClass")
            
            if not cls_Equipment:
                logger.error("Equipment class not found. Cannot define instance-level sequence properties.")
            else:
                # 1. sequencePosition (DataProperty)
                prop_sequencePosition = types.new_class("sequencePosition", (DataProperty, FunctionalProperty))
                prop_sequencePosition.domain = [cls_Equipment]
                prop_sequencePosition.range = [int]
                prop_sequencePosition.comment = ["Position of equipment instance in production sequence"]
                defined_properties["sequencePosition"] = prop_sequencePosition
                property_is_functional["sequencePosition"] = True
                logger.info("Defined property: sequencePosition")
                
                # 2. isImmediatelyUpstreamOf (ObjectProperty) with its inverse
                if cls_Equipment:
                    prop_isImmediatelyUpstreamOf = types.new_class("isImmediatelyUpstreamOf", (ObjectProperty,))
                    prop_isImmediatelyUpstreamOf.domain = [cls_Equipment]
                    prop_isImmediatelyUpstreamOf.range = [cls_Equipment]
                    prop_isImmediatelyUpstreamOf.comment = ["Links to the immediate downstream equipment in sequence"]
                    defined_properties["isImmediatelyUpstreamOf"] = prop_isImmediatelyUpstreamOf
                    property_is_functional["isImmediatelyUpstreamOf"] = False
                    logger.info("Defined property: isImmediatelyUpstreamOf")
                    
                    # Define inverse
                    prop_isImmediatelyDownstreamOf = types.new_class("isImmediatelyDownstreamOf", (ObjectProperty,))
                    prop_isImmediatelyDownstreamOf.domain = [cls_Equipment]
                    prop_isImmediatelyDownstreamOf.range = [cls_Equipment]
                    prop_isImmediatelyDownstreamOf.comment = ["Links to the immediate upstream equipment in sequence"]
                    defined_properties["isImmediatelyDownstreamOf"] = prop_isImmediatelyDownstreamOf
                    property_is_functional["isImmediatelyDownstreamOf"] = False
                    logger.info("Defined property: isImmediatelyDownstreamOf")
                    
                    # Set inverses
                    prop_isImmediatelyUpstreamOf.inverse_property = prop_isImmediatelyDownstreamOf
                    prop_isImmediatelyDownstreamOf.inverse_property = prop_isImmediatelyUpstreamOf
                
                # 3. isPartOfProductionLine (ObjectProperty)
                if cls_Equipment and cls_ProductionLine:
                    prop_isPartOfProductionLine = types.new_class("isPartOfProductionLine", (ObjectProperty,))
                    prop_isPartOfProductionLine.domain = [cls_Equipment]
                    prop_isPartOfProductionLine.range = [cls_ProductionLine]
                    prop_isPartOfProductionLine.comment = ["Links equipment to its production line"]
                    defined_properties["isPartOfProductionLine"] = prop_isPartOfProductionLine
                    property_is_functional["isPartOfProductionLine"] = False
                    logger.info("Defined property: isPartOfProductionLine")
                
                # 4. memberOfClass (ObjectProperty)
                if cls_Equipment and cls_EquipmentClass:
                    prop_memberOfClass = types.new_class("memberOfClass", (ObjectProperty, FunctionalProperty))
                    prop_memberOfClass.domain = [cls_Equipment]
                    prop_memberOfClass.range = [cls_EquipmentClass]
                    prop_memberOfClass.comment = ["Links equipment instance to its equipment class"]
                    defined_properties["memberOfClass"] = prop_memberOfClass
                    property_is_functional["memberOfClass"] = True
                    logger.info("Defined property: memberOfClass")

    with onto:
        # Define properties first without inverse, handle inverse in a second pass
        for row in properties_to_process:
            prop_name = row.get(SPEC_COL_PROPERTY,'').strip()
            if not prop_name or prop_name in defined_properties:
                continue # Skip empty or already defined properties
                
            # Skip deprecated class-level sequence properties
            if prop_name in ["classIsUpstreamOf", "classIsDownstreamOf", "defaultSequencePosition"]:
                logger.info(f"Skipping deprecated class-level property: {prop_name}")
                continue
            
            # Ensure instance-level equipment sequence properties are properly created
            if prop_name in ["sequencePosition", "isImmediatelyUpstreamOf", "isImmediatelyDownstreamOf", "isParallelWith"]:
                logger.info(f"Creating instance-level equipment property: {prop_name}")
                
            prop_type_str = row.get(SPEC_COL_PROP_TYPE, '').strip()
            domain_str = row.get(SPEC_COL_DOMAIN, '').strip()
            range_str = row.get(SPEC_COL_TARGET_RANGE, '').strip()
            characteristics_str = row.get(SPEC_COL_PROP_CHARACTERISTICS, '').strip().lower() # Normalize
            inverse_prop_name = row.get(SPEC_COL_INVERSE_PROPERTY, '').strip()

            if not prop_type_str or not domain_str or not range_str:
                logger.warning(f"Skipping property '{prop_name}' due to missing type, domain, or range in spec.")
                continue

            # Determine parent classes for the property
            parent_classes: List[type] = []
            base_prop_type: Optional[type] = None
            if prop_type_str == 'ObjectProperty':
                base_prop_type = ObjectProperty
            elif prop_type_str == 'DatatypeProperty':
                base_prop_type = DataProperty
            else:
                logger.warning(f"Unknown property type '{prop_type_str}' for property '{prop_name}'. Skipping.")
                continue

            parent_classes.append(base_prop_type)

            # Add characteristics
            is_functional = 'functional' in characteristics_str
            property_is_functional[prop_name] = is_functional # Track functionality status
            if is_functional: parent_classes.append(FunctionalProperty)
            if 'inversefunctional' in characteristics_str: parent_classes.append(InverseFunctionalProperty)
            if 'transitive' in characteristics_str: parent_classes.append(TransitiveProperty)
            if 'symmetric' in characteristics_str: parent_classes.append(SymmetricProperty)
            if 'asymmetric' in characteristics_str: parent_classes.append(AsymmetricProperty)
            if 'reflexive' in characteristics_str: parent_classes.append(ReflexiveProperty)
            if 'irreflexive' in characteristics_str: parent_classes.append(IrreflexiveProperty)

            try:
                # Define the property
                new_prop: PropertyClass = types.new_class(prop_name, tuple(parent_classes))

                # Set Domain
                domain_class_names = [dc.strip() for dc in domain_str.split('|')]
                prop_domain: List[ThingClass] = []
                valid_domain_found = False
                for dc_name in domain_class_names:
                    domain_class = defined_classes.get(dc_name)
                    if domain_class:
                        prop_domain.append(domain_class)
                        valid_domain_found = True
                    elif dc_name == "Thing" or dc_name.lower() == "owl:thing": # Allow Thing as domain
                        prop_domain.append(Thing)
                        valid_domain_found = True
                    else:
                        logger.warning(f"Domain class '{dc_name}' not found for property '{prop_name}'.")

                if prop_domain:
                    new_prop.domain = prop_domain # Assign list directly for union domain
                    logger.debug(f"Set domain for {prop_name} to {[dc.name for dc in prop_domain]}")
                elif not valid_domain_found:
                    logger.warning(f"No valid domain classes found for property '{prop_name}'. Skipping domain assignment.")

                # Set Range
                if base_prop_type is ObjectProperty:
                    range_class_names = [rc.strip() for rc in range_str.split('|')]
                    prop_range: List[ThingClass] = []
                    valid_range_found = False
                    for rc_name in range_class_names:
                        range_class = defined_classes.get(rc_name)
                        if range_class:
                            prop_range.append(range_class)
                            valid_range_found = True
                        elif rc_name == "Thing" or rc_name.lower() == "owl:thing": # Allow Thing as range
                            prop_range.append(Thing)
                            valid_range_found = True
                        else:
                            logger.warning(f"Range class '{rc_name}' not found for object property '{prop_name}'.")
                    if prop_range:
                        new_prop.range = prop_range # Assign list directly for union range
                        logger.debug(f"Set range for {prop_name} to {[rc.name for rc in prop_range]}")
                    elif not valid_range_found:
                        logger.warning(f"Could not set any valid range for object property '{prop_name}'.")

                elif base_prop_type is DataProperty:
                    target_type = XSD_TYPE_MAP.get(range_str)
                    if target_type:
                        new_prop.range = [target_type]
                        logger.debug(f"Set range for {prop_name} to {target_type.__name__ if hasattr(target_type, '__name__') else target_type}")
                    else:
                        logger.warning(f"Unknown XSD type '{range_str}' for property '{prop_name}'. Skipping range assignment.")

                # Add annotations
                notes = row.get(SPEC_COL_NOTES, '')
                isa95 = row.get(SPEC_COL_ISA95_CONCEPT, '')
                comments = []
                if notes: comments.append(f"Notes: {notes}")
                if isa95: comments.append(f"ISA-95 Concept: {isa95}")
                if comments:
                    new_prop.comment = comments

                defined_properties[prop_name] = new_prop
                logger.debug(f"Defined Property: {new_prop.iri} of type {prop_type_str} with characteristics {' '.join([p.__name__ for p in parent_classes[1:]]) if len(parent_classes) > 1 else 'None'}")

                # Store inverse relationship for later processing
                if inverse_prop_name and base_prop_type is ObjectProperty:
                    temp_inverse_map[prop_name] = inverse_prop_name

            except Exception as e:
                logger.error(f"Error defining property '{prop_name}': {e}")

    # --- Pass 3: Set Inverse Properties ---
    logger.debug("--- Setting Inverse Properties ---")
    with onto: # Ensure changes are applied within the ontology context
        for prop_name, inverse_name in temp_inverse_map.items():
            prop = defined_properties.get(prop_name)
            inverse_prop = defined_properties.get(inverse_name)

            if prop and inverse_prop:
                try:
                    # Check if already set to the desired value to avoid unnecessary writes/warnings if possible
                    current_inverse = getattr(prop, "inverse_property", None)
                    if current_inverse != inverse_prop:
                        prop.inverse_property = inverse_prop
                        logger.debug(f"Set inverse_property for {prop.name} to {inverse_prop.name}")
                    # Also explicitly set the inverse's inverse property back
                    current_inverse_of_inverse = getattr(inverse_prop, "inverse_property", None)
                    if current_inverse_of_inverse != prop:
                        inverse_prop.inverse_property = prop
                        logger.debug(f"Set inverse_property for {inverse_prop.name} back to {prop.name}")
                except Exception as e:
                    logger.error(f"Error setting inverse property between '{prop_name}' and '{inverse_name}': {e}")
            elif not prop:
                logger.warning(f"Property '{prop_name}' not found while trying to set inverse '{inverse_name}'.")
            elif not inverse_prop:
                logger.warning(f"Inverse property '{inverse_name}' not found for property '{prop_name}'.")

    logger.info("Ontology structure definition complete.")
    return defined_classes, defined_properties, property_is_functional

def create_selective_classes(onto: Ontology, 
                          specification: List[Dict[str, str]], 
                          skip_classes: List[str] = None,
                          strict_adherence: bool = False) -> Dict[str, ThingClass]:
    """
    Creates only the necessary classes from the specification, 
    optionally skipping specified classes or enforcing strict spec adherence.
    
    Args:
        onto: The ontology object
        specification: Parsed specification
        skip_classes: List of class names to skip (won't be created)
        strict_adherence: If True, only create classes explicitly defined in spec
        
    Returns:
        Dict mapping class name to class object
    """
    logger.info(f"Creating classes selectively from specification")
    
    skip_classes = set(skip_classes or [])
    defined_classes = {}
    
    # Pre-process spec to find essential classes
    spec_classes = set()
    spec_parents = {}
    property_domains = set()
    property_ranges = set()
    
    for row in specification:
        # Get class names
        class_name = row.get(SPEC_COL_ENTITY, '').strip()
        if class_name:
            spec_classes.add(class_name)
            parent_name = row.get(SPEC_PARENT_CLASS_COLUMN, '').strip()
            if parent_name and parent_name != class_name:
                spec_parents[class_name] = parent_name
                spec_classes.add(parent_name)  # Ensure parent is in spec classes
        
        # Get property domains and ranges
        prop_name = row.get(SPEC_COL_PROPERTY, '').strip()
        if prop_name:
            # Get domains
            domain_str = row.get(SPEC_COL_DOMAIN, '').strip()
            if domain_str:
                domains = [d.strip() for d in domain_str.split('|')]
                property_domains.update(domains)
            
            # Get ranges for object properties
            prop_type = row.get(SPEC_COL_PROP_TYPE, '').strip()
            if prop_type == 'ObjectProperty':
                range_str = row.get(SPEC_COL_TARGET_RANGE, '').strip()
                if range_str:
                    ranges = [r.strip() for r in range_str.split('|')]
                    property_ranges.update(ranges)
    
    # Determine which classes to create
    classes_to_create = set()
    
    if strict_adherence:
        # Only create classes explicitly defined in spec
        classes_to_create = spec_classes
    else:
        # Create spec classes plus any referenced in properties
        classes_to_create = spec_classes | property_domains | property_ranges
    
    # Remove classes to skip
    classes_to_create -= skip_classes
    
    # Create classes with proper hierarchy
    with onto:
        # First pass: create all classes as direct subclasses of Thing
        for class_name in classes_to_create:
            if class_name == "Thing" or class_name.lower() == "owl:thing":
                continue  # Skip Thing
            
            try:
                # Create as subclass of Thing initially
                new_class = types.new_class(class_name, (Thing,))
                defined_classes[class_name] = new_class
                logger.debug(f"Created class {class_name} (temp parent: Thing)")
            except Exception as e:
                logger.error(f"Error creating class {class_name}: {e}")
        
        # Second pass: set proper parent classes
        for class_name, class_obj in defined_classes.items():
            parent_name = spec_parents.get(class_name)
            if parent_name and parent_name in defined_classes:
                parent_class = defined_classes[parent_name]
                # Reset parent
                class_obj.is_a = [parent_class]
                logger.debug(f"Set parent of {class_name} to {parent_name}")
    
    classes_skipped = spec_classes - set(defined_classes.keys())
    if classes_skipped:
        logger.info(f"Skipped {len(classes_skipped)} classes: {', '.join(sorted(classes_skipped))}")
    
    logger.info(f"Selectively created {len(defined_classes)} classes from specification")
    return defined_classes


===========================================
FILE: ontology_generator/main.py
===========================================

"""
Main module for the ontology generator.

This module provides the main entry point for the ontology generator.
"""
import argparse
import csv
import logging
import os
import sys
import time as timing
from datetime import datetime, date, time
from typing import List, Dict, Any, Optional, Tuple

from owlready2 import (
    World, Ontology, sync_reasoner, Thing,
    OwlReadyInconsistentOntologyError, locstr, default_world,
    ThingClass, FunctionalProperty, InverseFunctionalProperty, TransitiveProperty, SymmetricProperty, AsymmetricProperty, ReflexiveProperty, IrreflexiveProperty, Nothing
)

from ontology_generator.config import DEFAULT_ONTOLOGY_IRI, init_xsd_type_map, DEFAULT_EQUIPMENT_SEQUENCE
from ontology_generator.utils.logging import (
    main_logger, configure_logging, analysis_logger
)
from ontology_generator.definition import (
    parse_specification, define_ontology_structure, create_selective_classes,
    parse_property_mappings, validate_property_mappings, read_data
)
from ontology_generator.population import (
    setup_equipment_sequence_relationships, # Need these post-population steps
    setup_equipment_instance_relationships,
    link_equipment_events_to_line_events
)
from ontology_generator.analysis import (
    analyze_ontology_population, generate_population_report,
    generate_optimization_recommendations, generate_reasoning_report,
    generate_equipment_sequence_report, analyze_equipment_sequences
)
from ontology_generator.utils import safe_cast # Import directly from utils now

# Initialize XSD type map and datetime types
init_xsd_type_map(locstr)

def populate_ontology_from_data(onto: Ontology,
                                data_rows: List[Dict[str, Any]],
                                defined_classes: Dict[str, object],
                                defined_properties: Dict[str, object],
                                property_is_functional: Dict[str, bool],
                                specification: List[Dict[str, str]],
                                property_mappings: Dict[str, Dict[str, Dict[str, Any]]] = None
                              ) -> Tuple[int, Dict[str, object], Dict[str, int], List[Tuple[object, object, object, object]], Dict]:
    """
    Populates the ontology with individuals and relations from data rows using a two-pass approach.
    Pass 1: Creates individuals and sets data properties.
    Pass 2: Creates object property relationships between individuals.

    Args:
        onto: The ontology to populate
        data_rows: The data rows from the data CSV file
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        property_is_functional: Dictionary indicating functionality of properties
        specification: The parsed specification
        property_mappings: Optional property mappings dictionary
        
    Returns:
        tuple: (failed_rows_count, created_equipment_class_inds, equipment_class_positions, created_events_context, all_created_individuals_by_uid)
    """
    # Ensure imports required *within* this function are present
    from ontology_generator.population.core import PopulationContext
    from ontology_generator.population.row_processor import process_single_data_row_pass1, process_single_data_row_pass2 # Import needed here

    main_logger.info(f"Starting ontology population with {len(data_rows)} data rows (Two-Pass Strategy).")

    # Create population context
    context = PopulationContext(onto, defined_classes, defined_properties, property_is_functional)

    # --- Pre-checks (Essential Classes and Properties) ---
    essential_classes_names = [
        "Plant", "Area", "ProcessCell", "ProductionLine", "Equipment",
        "EquipmentClass", "Material", "ProductionRequest", "EventRecord",
        "TimeInterval", "Shift", "OperationalState", "OperationalReason"
    ]
    missing_classes = [name for name in essential_classes_names if not context.get_class(name)]
    if missing_classes:
        # get_class already logged errors, just return failure
        main_logger.error(f"Cannot proceed. Missing essential classes definitions: {missing_classes}")
        return len(data_rows), {}, {}, [], {}  # Empty context

    essential_prop_names = { # Focus on IDs and core structure for initial checks
        "plantId", "areaId", "processCellId", "lineId", "equipmentId", "equipmentName",
        "equipmentClassId", "materialId", "requestId", "shiftId", "startTime", "endTime"
        # Object properties checked implicitly later
    }
    missing_essential_props = [name for name in essential_prop_names if not context.get_prop(name)]
    if missing_essential_props:
        main_logger.error(f"Cannot reliably proceed. Missing essential data properties definitions: {missing_essential_props}")
        return len(data_rows), {}, {}, [], {}  # Empty context

    # Warn about other missing properties defined in spec but not found
    all_spec_prop_names = {row.get('Proposed OWL Property','').strip() for row in specification if row.get('Proposed OWL Property')}
    for spec_prop in all_spec_prop_names:
        if spec_prop and not context.get_prop(spec_prop):
            main_logger.warning(f"Property '{spec_prop}' (from spec) not found in defined_properties. Population using this property will be skipped.")

    # --- Pass 1: Create Individuals and Apply Data Properties ---
    main_logger.info("--- Population Pass 1: Creating Individuals and Data Properties ---")
    all_created_individuals_by_uid = {} # {(entity_type, unique_id): individual_obj}
    individuals_by_row = {} # {row_index: {entity_type: individual_obj, ...}}
    created_equipment_class_inds = {}  # {eq_class_name_str: eq_class_ind_obj}
    equipment_class_positions = {}  # {eq_class_name_str: position_int}
    created_events_context = []  # List to store tuples for later linking: (event_ind, resource_ind, resource_type)
    pass1_successful_rows = 0
    pass1_failed_rows = 0

    with onto:  # Use the ontology context for creating individuals
        for i, row in enumerate(data_rows):
            row_num = i + 2  # 1-based index + header row = line number in CSV

            # Call the dedicated row processing function for Pass 1
            success, created_inds_in_row, event_context, eq_class_info = process_single_data_row_pass1(
                row, row_num, context, property_mappings, all_created_individuals_by_uid # Pass registry for get_or_create logic
            )

            if success:
                pass1_successful_rows += 1
                individuals_by_row[i] = created_inds_in_row # Store individuals created from this row
                # Update the global registry (used by get_or_create and Pass 2 context)
                # Note: process_single_data_row_pass1 should already populate all_created_individuals_by_uid via get_or_create calls
                
                # Store event context if returned
                if event_context:
                    created_events_context.append(event_context)

                # Process equipment class info if returned
                if eq_class_info:
                    eq_class_name, eq_class_ind, eq_class_pos = eq_class_info
                    if eq_class_name not in created_equipment_class_inds:
                        created_equipment_class_inds[eq_class_name] = eq_class_ind
                    # Update position map if a position is defined and potentially different
                    if eq_class_pos is not None:
                        if eq_class_name in equipment_class_positions and equipment_class_positions[eq_class_name] != eq_class_pos:
                             main_logger.warning(f"Sequence position conflict for class '{eq_class_name}' during population. Existing: {equipment_class_positions[eq_class_name]}, New: {eq_class_pos}. Using new value: {eq_class_pos}")
                        equipment_class_positions[eq_class_name] = eq_class_pos
                        # main_logger.debug(f"Tracked position {eq_class_pos} for class '{eq_class_name}'.") # Can be noisy

            else:
                pass1_failed_rows += 1
                individuals_by_row[i] = {} # Ensure entry exists even if row failed
                # Error logging handled within process_single_data_row_pass1

    main_logger.info(f"Pass 1 Complete. Successful rows: {pass1_successful_rows}, Failed rows: {pass1_failed_rows}.")
    main_logger.info(f"Total unique individuals created (approx): {len(all_created_individuals_by_uid)}")

    # Log Equipment Class Summary (collected during pass 1)
    main_logger.info("--- Unique Equipment Classes Found/Created (Pass 1) ---")
    if created_equipment_class_inds:
        sorted_class_names = sorted(created_equipment_class_inds.keys())
        main_logger.info(f"Total unique equipment classes: {len(sorted_class_names)}")
        for class_name in sorted_class_names:
            main_logger.info(f"  • {class_name} (Position: {equipment_class_positions.get(class_name, 'Not Set')})")
        
        # Log information about default sequence positions from config
        defaults_used = [name for name in sorted_class_names if name in DEFAULT_EQUIPMENT_SEQUENCE]
        if defaults_used:
            main_logger.info(f"Using default sequence positions from config for {len(defaults_used)} equipment classes: {', '.join(defaults_used)}")
    else:
        main_logger.warning("No EquipmentClass individuals were created or tracked during population!")

    # --- Pass 2: Apply Object Property Mappings ---
    main_logger.info("--- Population Pass 2: Linking Individuals (Object Properties) ---")
    pass2_successful_rows = 0
    pass2_failed_rows = 0
    # Prepare the full context dictionary for linking (use the values from the UID map)
    full_context_individuals = {k[0]: v for k, v in all_created_individuals_by_uid.items()} # Simple context {type_name: ind_obj} - May need refinement based on linking needs
    main_logger.info(f"Prepared context for Pass 2 with {len(full_context_individuals)} potential link targets.")


    # Refine context based on actual needs - Needs careful thought on how apply_object_property_mappings uses context_individuals
    # The warning "Context entity 'Equipment' required for Equipment.isParallelWith not found in provided context_individuals dictionary"
    # suggests the key should be the *type* ('Equipment') and the value the *target* individual.
    # However, apply_property_mappings seems to look up `context_individuals[link_context_entity]`.
    # For linking Equipment to Equipment via 'isParallelWith', link_context_entity would be 'Equipment'.
    # The current `apply_property_mappings` expects ONE individual for that key. This is flawed for many-to-many or one-to-many via context.

    # Let's assume apply_object_property_mappings will handle lookup within all_created_individuals_by_uid.
    # We pass the full registry instead of a simplified context.
    linking_context = all_created_individuals_by_uid

    with onto: # Context manager might not be strictly needed here if only setting properties
        for i, row in enumerate(data_rows):
            row_num = i + 2
            # Skip rows that failed significantly in Pass 1 (e.g., couldn't create core individuals)
            if i not in individuals_by_row or not individuals_by_row[i]:
                 main_logger.debug(f"Skipping Pass 2 linking for row {row_num} as no individuals were successfully created in Pass 1.")
                 pass2_failed_rows += 1 # Count as failed for Pass 2
                 continue

            created_inds_this_row = individuals_by_row[i]

            # Call the dedicated row processing function for Pass 2
            success = process_single_data_row_pass2(
                row, row_num, context, property_mappings, created_inds_this_row, linking_context
            )

            if success:
                pass2_successful_rows += 1
            else:
                pass2_failed_rows += 1
                # Logging handled within process_single_data_row_pass2

    main_logger.info(f"Pass 2 Complete. Rows successfully linked: {pass2_successful_rows}, Rows failed/skipped linking: {pass2_failed_rows}.")

    # Determine overall failed count (consider a row failed if it failed either pass?)
    # For now, return Pass 1 failure count as it indicates primary data issues.
    # The warnings originally reported were link failures (Pass 2 type issues).
    final_failed_rows = pass1_failed_rows # Or potentially max(pass1_failed_rows, pass2_failed_rows) or other logic

    main_logger.info(f"Ontology population complete. Final failed row count (based on Pass 1): {final_failed_rows}.")
    # Return collected contexts from Pass 1 and the registry
    return final_failed_rows, created_equipment_class_inds, equipment_class_positions, created_events_context, all_created_individuals_by_uid


def _log_initial_parameters(args, logger):
    logger.info("--- Starting Ontology Generation ---")
    logger.info(f"Specification file: {args.spec_file}")
    logger.info(f"Data file: {args.data_file}")
    logger.info(f"Output OWL file: {args.output_file}")
    logger.info(f"Ontology IRI: {args.iri}")
    logger.info(f"Save format: {args.format}")
    logger.info(f"Run reasoner: {args.reasoner}")
    if args.worlddb:
        logger.info(f"Using persistent world DB: {args.worlddb}")
    logger.info(f"Reasoner report max entities: {args.max_report_entities}")
    logger.info(f"Reasoner report verbose: {args.full_report}")
    logger.info(f"Analyze population: {args.analyze_population}")
    logger.info(f"Strict adherence: {args.strict_adherence}")
    logger.info(f"Skip classes: {args.skip_classes}")
    logger.info(f"Optimize ontology: {args.optimize_ontology}")

def _parse_spec_and_mappings(spec_file_path, logger):
    logger.info(f"Parsing specification file: {spec_file_path}")
    specification = parse_specification(spec_file_path)
    if not specification:
        logger.error("Specification parsing failed or resulted in empty spec. Aborting.")
        return None, None # Indicate failure

    logger.info("Parsing property mappings from specification...")
    property_mappings = parse_property_mappings(specification)
    logger.info(f"Parsed property mappings for {len(property_mappings)} entities")

    logger.info("Validating property mappings...")
    validation_result = validate_property_mappings(property_mappings)
    if not validation_result:
        logger.warning("Property mapping validation had issues. Population may be incomplete.")
    else:
        logger.info("Property mapping validation passed.")
    return specification, property_mappings

def _setup_world_and_ontology(ontology_iri, world_db_path, logger):
    world = None
    onto = None
    if world_db_path:
        logger.info(f"Initializing persistent World at: {world_db_path}")
        db_dir = os.path.dirname(world_db_path)
        if db_dir and not os.path.exists(db_dir):
            try:
                os.makedirs(db_dir, exist_ok=True)
                logger.info(f"Created directory for world DB: {db_dir}")
            except OSError as e:
                 logger.error(f"Failed to create directory for world DB {db_dir}: {e}")
                 return None, None # Indicate failure
        try:
            world = World(filename=world_db_path)
            onto = world.get_ontology(ontology_iri).load()
            logger.info(f"Ontology object obtained from persistent world: {onto}")
        except Exception as db_err:
             logger.error(f"Failed to initialize or load from persistent world DB {world_db_path}: {db_err}", exc_info=True)
             return None, None # Indicate failure
    else:
        logger.info("Initializing in-memory World.")
        world = World()  # Create a fresh world
        onto = world.get_ontology(ontology_iri)
        logger.info(f"Ontology object created in memory: {onto}")
    return world, onto

def _define_tbox(onto, specification, strict_adherence, skip_classes, logger):
    logger.info("Defining ontology structure (TBox)...")
    if strict_adherence or skip_classes:
        logger.info("Using selective class creation based on config.")
        defined_classes = create_selective_classes(onto, specification,
                                                  skip_classes=skip_classes,
                                                  strict_adherence=strict_adherence)
        # Define properties separately when using selective classes
        _, defined_properties, property_is_functional = define_ontology_structure(onto, specification)
    else:
        defined_classes, defined_properties, property_is_functional = define_ontology_structure(onto, specification)

    if not defined_classes:
        logger.warning("Ontology structure definition resulted in no classes. Population might be empty.")
    logger.info("TBox definition complete.")
    return defined_classes, defined_properties, property_is_functional

def _read_operational_data(data_file_path, logger):
    logger.info(f"Reading operational data from: {data_file_path}")
    try:
        data_rows = read_data(data_file_path)
        logger.info(f"Read {len(data_rows)} data rows.")
        if not data_rows:
            logger.warning("No data rows read. Ontology population will be skipped.")
        return data_rows
    except Exception as read_err:
        logger.error(f"Failed to read data file {data_file_path}: {read_err}", exc_info=True)
        return None # Indicate failure

def _populate_abox(onto, data_rows, defined_classes, defined_properties, prop_is_functional, specification, property_mappings, logger):
    logger.info("Starting ontology population (ABox)...")
    population_successful = True
    failed_rows_count = 0
    created_eq_classes = {}
    eq_class_positions = {}
    created_events_context = []
    all_created_individuals_by_uid = {}

    if not data_rows:
        logger.warning("Skipping population as no data rows were provided.")
        # Return success=True but with zero counts/empty contexts
        return True, 0, {}, {}, [], {}

    try:
        failed_rows_count, created_eq_classes, eq_class_positions, created_events_context, all_created_individuals_by_uid = populate_ontology_from_data(
            onto, data_rows, defined_classes, defined_properties, prop_is_functional,
            specification, property_mappings
        )
        if failed_rows_count == len(data_rows) and len(data_rows) > 0:
            logger.error(f"Population failed for all {len(data_rows)} data rows.")
            population_successful = False
        elif failed_rows_count > 0:
            logger.warning(f"Population completed with {failed_rows_count} out of {len(data_rows)} failed rows.")
        else:
            logger.info(f"Population completed successfully for all {len(data_rows)} rows.")

    except Exception as pop_exc:
        logger.error(f"Critical error during population: {pop_exc}", exc_info=True)
        population_successful = False

    logger.info("ABox population phase finished.")
    return population_successful, failed_rows_count, created_eq_classes, eq_class_positions, created_events_context, all_created_individuals_by_uid

def _run_analysis_and_optimization(onto, defined_classes, specification, optimize_ontology, output_owl_path, logger):
    logger.info("Analyzing ontology population status...")
    try:
        population_counts, empty_classes, class_instances, class_usage_info = analyze_ontology_population(onto, defined_classes, specification)
        population_report = generate_population_report(population_counts, empty_classes, class_instances, defined_classes, class_usage_info)
        logger.info("Ontology Population Analysis Complete")
        print(population_report) # Print to console

        if optimize_ontology:
            logger.info("Generating detailed optimization recommendations...")
            optimization_recs = generate_optimization_recommendations(class_usage_info, defined_classes)
            print("\n=== DETAILED OPTIMIZATION RECOMMENDATIONS ===")
            if optimization_recs.get('classes_to_remove'):
                print(f"\nClasses that could be safely removed ({len(optimization_recs['classes_to_remove'])}):")
                for class_name in optimization_recs['classes_to_remove']:
                    print(f"  • {class_name}")
            if optimization_recs.get('configuration_options'):
                print("\nSuggested configuration for future runs:")
                for option in optimization_recs['configuration_options']:
                    print(f"  • {option}")
            # Save recommendations to file
            try:
                base_dir = os.path.dirname(output_owl_path)
                recs_file = os.path.join(base_dir, "ontology_optimization.txt")
                with open(recs_file, 'w') as f:
                    f.write("# Ontology Optimization Recommendations\n\n")
                    f.write("## Classes to Remove\n")
                    for cls in optimization_recs.get('classes_to_remove', []):
                        f.write(f"- {cls}\n")
                    f.write("\n## Configuration Options\n")
                    for opt in optimization_recs.get('configuration_options', []):
                        f.write(f"- {opt}\n")
                logger.info(f"Saved optimization recommendations to {recs_file}")
            except Exception as e:
                logger.error(f"Failed to save optimization recommendations: {e}")

    except Exception as analysis_exc:
        logger.error(f"Error analyzing ontology population: {analysis_exc}", exc_info=False)
        # Continue despite analysis failure

def _setup_sequence_relationships(onto, created_eq_classes, eq_class_positions, defined_classes, defined_properties, property_is_functional, logger):
    logger.info("Setting up equipment instance relationships...")
    try:
        # Remove class-level sequence relationship setup
        setup_equipment_sequence_relationships(onto, eq_class_positions, defined_classes, defined_properties, created_eq_classes)
        
        # Only set up instance-level relationships
        setup_equipment_instance_relationships(onto, defined_classes, defined_properties, property_is_functional, eq_class_positions)
        logger.info("Equipment instance sequence relationship setup complete.")
        
        # Add equipment sequence report generation
        sequence_report = generate_equipment_sequence_report(onto)
        logger.info("Equipment sequence report generated.")
        print(sequence_report)  # Print to console for immediate visibility
        
    except Exception as seq_exc:
        logger.error(f"Error during sequence relationship setup: {seq_exc}", exc_info=True)
        # Log error but continue

def _link_equipment_events(onto, created_events_context, defined_classes, defined_properties, logger):
    logger.info("Linking equipment events to line events...")
    try:
        links_made = link_equipment_events_to_line_events(
            onto, created_events_context, defined_classes, defined_properties
        )
        logger.info(f"Event linking pass created {links_made} links.")
    except Exception as link_exc:
        logger.error(f"Error during event linking pass: {link_exc}", exc_info=True)
        # Log error but continue

def _process_structural_relationships(onto, data_rows, defined_classes, defined_properties, property_is_functional, property_mappings, all_created_individuals_by_uid, logger):
    """
    Process structural relationships between entities after all individuals have been created.
    This is specifically for relationships that can't be established during row-by-row processing.
    
    Args:
        onto: The ontology being populated
        data_rows: The data rows (used for referencing column information)
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties 
        property_is_functional: Dictionary indicating whether properties are functional
        property_mappings: Parsed property mappings
        all_created_individuals_by_uid: Registry of all created individuals
        logger: Logger to use
    
    Returns:
        int: Number of structural links created
    """
    logger.info("Processing structural relationships between entities...")
    try:
        # Import the function from row_processor
        from ontology_generator.population.row_processor import process_structural_relationships
        from ontology_generator.population.core import PopulationContext
        
        # Create the context
        context = PopulationContext(onto, defined_classes, defined_properties, property_is_functional)
        
        # Call the structural relationship processor
        links_created = process_structural_relationships(
            context, property_mappings, all_created_individuals_by_uid, logger
        )
        
        logger.info(f"Structural relationship processing complete. Created {links_created} links.")
        return links_created
    except Exception as e:
        logger.error(f"Error processing structural relationships: {e}", exc_info=True)
        return 0  # Indicate no links were created due to error

def _run_reasoning_phase(onto, world, world_db_path, reasoner_report_max_entities, reasoner_report_verbose, logger):
    logger.info("Applying reasoner (ensure HermiT or compatible reasoner is installed)...")
    reasoning_successful = True
    try:
        active_world = world if world_db_path else default_world
        with onto:
            pre_stats = {
                'classes': len(list(onto.classes())), 'object_properties': len(list(onto.object_properties())),
                'data_properties': len(list(onto.data_properties())), 'individuals': len(list(onto.individuals()))
            }
            logger.info("Starting reasoning process...")
            reasoning_start_time = timing.time()
            sync_reasoner(infer_property_values=True, debug=0) # Pass world implicitly via onto context?
            reasoning_end_time = timing.time()
            logger.info(f"Reasoning finished in {reasoning_end_time - reasoning_start_time:.2f} seconds.")

            # Post-reasoning analysis and report generation
            inconsistent = list(active_world.inconsistent_classes())
            inferred_hierarchy = {}
            inferred_properties = {}
            inferred_individuals = {}
            for cls in onto.classes():
                current_subclasses = set(cls.subclasses())
                inferred_subs = [sub.name for sub in current_subclasses if sub != cls and sub != Nothing] 
                equivalent_classes = [eq.name for eq in cls.equivalent_to if eq != cls and isinstance(eq, ThingClass)]
                if inferred_subs or equivalent_classes:
                    inferred_hierarchy[cls.name] = {'subclasses': inferred_subs, 'equivalent': equivalent_classes}

            inferrable_chars = {
                'FunctionalProperty': FunctionalProperty, 'InverseFunctionalProperty': InverseFunctionalProperty,
                'TransitiveProperty': TransitiveProperty, 'SymmetricProperty': SymmetricProperty,
                'AsymmetricProperty': AsymmetricProperty, 'ReflexiveProperty': ReflexiveProperty,
                'IrreflexiveProperty': IrreflexiveProperty,
            }
            for prop in list(onto.object_properties()) + list(onto.data_properties()):
                inferred_chars_for_prop = [char_name for char_name, char_class in inferrable_chars.items() if char_class in prop.is_a]
                if inferred_chars_for_prop: inferred_properties[prop.name] = inferred_chars_for_prop

            logger.info("Collecting simplified individual inferences (post-reasoning state).")
            for ind in onto.individuals():
                current_types = [c.name for c in ind.is_a if c is not Thing]
                current_props = {}
                for prop in list(onto.object_properties()) + list(onto.data_properties()):
                    try:
                        values = prop[ind]
                        if not isinstance(values, list): values = [values] if values is not None else []
                        if values:
                            formatted_values = []
                            for v in values:
                                if isinstance(v, Thing): formatted_values.append(v.name)
                                elif isinstance(v, locstr): formatted_values.append(f'"{v}"@{v.lang}')
                                else: formatted_values.append(repr(v))
                            if formatted_values: current_props[prop.name] = formatted_values
                    except Exception: continue
                if current_types or current_props:
                    inferred_individuals[ind.name] = {'types': current_types, 'properties': current_props}

            post_stats = {
                'classes': len(list(onto.classes())), 'object_properties': len(list(onto.object_properties())),
                'data_properties': len(list(onto.data_properties())), 'individuals': len(list(onto.individuals()))
            }
            report, has_issues = generate_reasoning_report(
                onto, pre_stats, post_stats, inconsistent, inferred_hierarchy,
                inferred_properties, inferred_individuals, True, # Assuming reasoner ran
                max_entities_per_category=reasoner_report_max_entities,
                verbose=reasoner_report_verbose
            )
            logger.info("\nReasoning Report:\n" + report)

            if has_issues or inconsistent:
                logger.warning("Reasoning completed but potential issues or inconsistencies were identified.")
                if inconsistent: reasoning_successful = False
            else: logger.info("Reasoning completed successfully.")

    except OwlReadyInconsistentOntologyError:
        logger.error("REASONING FAILED: Ontology is inconsistent!")
        reasoning_successful = False
        try:
            active_world = world if world_db_path else default_world
            inconsistent = list(active_world.inconsistent_classes())
            logger.error(f"Inconsistent classes detected: {[c.name for c in inconsistent]}")
        except Exception as e_inc: logger.error(f"Could not retrieve inconsistent classes: {e_inc}")
    except NameError as ne:
        if "sync_reasoner" in str(ne): logger.error("Reasoning failed: Reasoner (sync_reasoner) function not found.")
        else: logger.error(f"Unexpected NameError during reasoning: {ne}")
        reasoning_successful = False
    except Exception as e:
        logger.error(f"An error occurred during reasoning: {e}", exc_info=True)
        reasoning_successful = False

    logger.info("Reasoning phase finished.")
    return reasoning_successful

def _save_ontology_file(onto, world, output_owl_path, save_format, world_db_path, population_successful, reasoning_successful, logger):
    should_save_primary = population_successful and reasoning_successful
    final_output_path = output_owl_path
    save_failed = False

    if not should_save_primary:
        logger.error("Ontology generation had issues (population/reasoning failure/inconsistency). Saving to debug file instead.")
        base, ext = os.path.splitext(output_owl_path)
        debug_output_path = f"{base}_debug{ext}"
        if debug_output_path == output_owl_path:
            debug_output_path = output_owl_path + "_debug"
        final_output_path = debug_output_path
        logger.info(f"Attempting to save potentially problematic ontology to: {final_output_path}")
    else:
        logger.info(f"Attempting to save final ontology to: {final_output_path}")

    logger.info(f"Saving ontology in '{save_format}' format...")
    try:
        # Use the world associated with the ontology for saving, especially if persistent
        # If world is None (in-memory case after setup failure?), this will likely fail, which is ok.
        onto.save(file=final_output_path, format=save_format)
        logger.info("Ontology saved successfully.")
    except Exception as save_err:
        logger.error(f"Failed to save ontology to {final_output_path}: {save_err}", exc_info=True)
        save_failed = True # Indicate saving failed

    return save_failed

def main_ontology_generation(spec_file_path: str,
                             data_file_path: str,
                             output_owl_path: str,
                             ontology_iri: str = DEFAULT_ONTOLOGY_IRI,
                             save_format: str = "rdfxml",
                             use_reasoner: bool = False,
                             world_db_path: Optional[str] = None,
                             reasoner_report_max_entities: int = 10,
                             reasoner_report_verbose: bool = False,
                             analyze_population: bool = True,
                             strict_adherence: bool = False,
                             skip_classes: List[str] = None,
                             optimize_ontology: bool = False
                            ) -> bool:
    """
    Main function to generate the ontology by orchestrating helper functions.
    (Args documentation remains the same)
    Returns:
        bool: True on overall success, False on failure
    """
    start_time = timing.time()
    main_logger.info("--- Ontology Generation Process Started ---")

    # Use a dummy args object for logging if needed, or adapt helpers
    # For simplicity, let's create a temporary Namespace-like object
    class Args: pass
    args = Args()
    args.spec_file = spec_file_path
    args.data_file = data_file_path
    args.output_file = output_owl_path
    args.iri = ontology_iri
    args.format = save_format
    args.reasoner = use_reasoner
    args.worlddb = world_db_path
    args.max_report_entities = reasoner_report_max_entities
    args.full_report = reasoner_report_verbose
    args.analyze_population = analyze_population
    args.strict_adherence = strict_adherence
    args.skip_classes = skip_classes
    args.optimize_ontology = optimize_ontology

    world = None
    onto = None
    population_successful = False
    reasoning_successful = True # Assume success unless reasoner runs and fails
    save_failed = False

    try:
        # 1. Log Initial Parameters
        _log_initial_parameters(args, main_logger)

        # 2. Parse Specification and Mappings
        specification, property_mappings = _parse_spec_and_mappings(args.spec_file, main_logger)
        if specification is None: return False

        # 3. Setup World and Ontology
        world, onto = _setup_world_and_ontology(args.iri, args.worlddb, main_logger)
        if onto is None: return False

        # 4. Define Ontology Structure (TBox)
        defined_classes, defined_properties, property_is_functional = _define_tbox(
            onto, specification, args.strict_adherence, args.skip_classes, main_logger
        )
        # Handle case where TBox definition might yield nothing critical?
        # Current _define_tbox logs warning, main flow continues.

        # 5. Read Operational Data
        data_rows = _read_operational_data(args.data_file, main_logger)
        if data_rows is None: return False # Indicate failure if reading failed

        # 6. Populate Ontology (ABox)
        population_successful, failed_rows_count, created_eq_classes, eq_class_positions, created_events_context, all_created_individuals_by_uid = _populate_abox(
            onto, data_rows, defined_classes, defined_properties, property_is_functional,
            specification, property_mappings, main_logger
        )
        
        # 7. Process Structural Relationships (NEW STEP)
        if population_successful:
            # Process structural relationships with the registry from population
            _process_structural_relationships(
                onto, data_rows, defined_classes, defined_properties, property_is_functional,
                property_mappings, all_created_individuals_by_uid, main_logger
            )
            main_logger.info("Structural relationship processing complete.")

        # 8. Analyze Population & Optimize (Optional)
        if population_successful and args.analyze_population:
            _run_analysis_and_optimization(onto, defined_classes, specification, args.optimize_ontology, args.output_file, main_logger)
        elif not args.analyze_population:
            main_logger.warning("Skipping ontology population analysis as requested.")

        # 9. Setup Sequence Relationships (Optional)
        if population_successful and created_eq_classes and eq_class_positions:
             _setup_sequence_relationships(onto, created_eq_classes, eq_class_positions, defined_classes, defined_properties, property_is_functional, main_logger)
        elif population_successful:
             main_logger.warning("Skipping sequence relationship setup: No EquipmentClass info available.")
        # No action needed if population failed, handled by checks below

        # 10. Link Events (Optional)
        if population_successful and created_events_context:
            _link_equipment_events(onto, created_events_context, defined_classes, defined_properties, main_logger)
        elif population_successful:
             main_logger.warning("Skipping event linking: No event context available.")
        # No action needed if population failed

        # 11. Apply Reasoning (Optional)
        if args.reasoner and population_successful:
            reasoning_successful = _run_reasoning_phase(onto, world, args.worlddb, args.max_report_entities, args.full_report, main_logger)
        elif args.reasoner and not population_successful:
            main_logger.warning("Skipping reasoning due to prior population failure.")
            reasoning_successful = False # Ensure overall success reflects this skipped step
        # If reasoner not used, reasoning_successful remains True

        # 12. Save Ontology
        # Saving logic depends on population and reasoning success
        # The helper returns True if saving *failed*
        save_failed = _save_ontology_file(onto, world, args.output_file, args.format, args.worlddb, population_successful, reasoning_successful, main_logger)
        if save_failed:
            return False # Saving failed, overall process is unsuccessful

        # 13. Determine overall success
        overall_success = population_successful and reasoning_successful and not save_failed
        return overall_success

    except Exception as e:
        main_logger.exception("A critical error occurred during the overall ontology generation process.")
        return False

    finally:
        end_time = timing.time()
        main_logger.info(f"--- Ontology Generation Finished --- Total time: {end_time - start_time:.2f} seconds")
        
        # Log suppressed message counts
        from ontology_generator.utils.logging import log_suppressed_message_counts
        log_suppressed_message_counts()


def test_property_mappings(spec_file_path: str):
    """
    Test function to verify property mapping functionality.
    
    This can be called manually for testing and provides detailed debug output
    about the parsed property mappings for all entity types.
    
    Args:
        spec_file_path: Path to the specification CSV file
    """
    # Configure more verbose logging for testing
    configure_logging(level=logging.DEBUG)
    
    test_logger = logging.getLogger("property_mapping_test")
    test_logger.info("=== Starting Property Mapping Test ===")
    
    try:
        # Parse spec file
        test_logger.info(f"Parsing specification file: {spec_file_path}")
        spec = parse_specification(spec_file_path)
        test_logger.info(f"Parsed {len(spec)} rows from specification file")
        
        # Parse and validate property mappings
        test_logger.info("Generating property mappings from specification")
        mappings = parse_property_mappings(spec)
        validation_passed = validate_property_mappings(mappings)
        test_logger.info(f"Validation result: {'PASSED' if validation_passed else 'FAILED'}")
        
        # Group entities by logical group for organization
        from collections import defaultdict
        entity_groups = defaultdict(list)
        for row in spec:
            entity = row.get('Proposed OWL Entity', '').strip()
            group = row.get('Logical Group', '').strip()
            if entity and group:
                if entity not in entity_groups[group]:
                    entity_groups[group].append(entity)
                    
        # Print summary by group
        test_logger.info("\n=== Entity Coverage by Logical Group ===")
        for group, entities in sorted(entity_groups.items()):
            mapped_entities = [e for e in entities if e in mappings]
            test_logger.info(f"{group}: {len(mapped_entities)}/{len(entities)} entities mapped")
            
            if mapped_entities:
                for entity in sorted(mapped_entities):
                    data_props = len(mappings[entity].get('data_properties', {}))
                    obj_props = len(mappings[entity].get('object_properties', {}))
                    test_logger.info(f"  ✓ {entity}: {data_props} data properties, {obj_props} object properties")
            
            missing = [e for e in entities if e not in mappings]
            if missing:
                for entity in sorted(missing):
                    test_logger.warning(f"  ✗ {entity}: No property mappings found")
        
        # Detailed analysis of common entities
        key_entities = [
            'EventRecord', 
            'Material', 
            'OperationalReason', 
            'OperationalState',
            'ProductionLine',
            'Equipment',
            'EquipmentClass',
            'Plant',
            'Area',
            'ProcessCell',
            'Shift',
            'TimeInterval',
            'ProductionRequest'
        ]
        
        for entity in key_entities:
            if entity in mappings:
                entity_map = mappings[entity]
                
                test_logger.info(f"\n=== {entity} Property Mappings ===")
                
                # Data properties
                data_props = entity_map.get('data_properties', {})
                test_logger.info(f"Found {len(data_props)} data properties for {entity}")
                
                if data_props:
                    for prop_name, details in sorted(data_props.items()):
                        test_logger.info(f"  ✓ {prop_name}: column='{details.get('column')}', type='{details.get('data_type')}', functional={details.get('functional')}")
                
                # Object properties
                obj_props = entity_map.get('object_properties', {})
                if obj_props:
                    test_logger.info(f"Found {len(obj_props)} object properties for {entity}")
                    for prop_name, details in sorted(obj_props.items()):
                        test_logger.info(f"  ✓ {prop_name}: column='{details.get('column')}', target='{details.get('target_class')}', functional={details.get('functional')}")
            else:
                test_logger.warning(f"\n=== {entity} Property Mappings ===")
                test_logger.warning(f"  ✗ {entity} entity not found in mappings!")
        
        # Summary stats        
        total_data_props = sum(len(entity_map.get('data_properties', {})) for entity_map in mappings.values())
        total_obj_props = sum(len(entity_map.get('object_properties', {})) for entity_map in mappings.values())
        
        test_logger.info("\n=== Property Mapping Summary ===")
        test_logger.info(f"Total entities mapped: {len(mappings)}")
        test_logger.info(f"Total data properties mapped: {total_data_props}")
        test_logger.info(f"Total object properties mapped: {total_obj_props}")
        test_logger.info(f"Total properties mapped: {total_data_props + total_obj_props}")
        
        test_logger.info("=== Property Mapping Test Complete ===")
        
    except Exception as e:
        test_logger.error(f"Error during property mapping test: {e}", exc_info=True)


def analyze_equipment_sequence_in_ontology(owl_file_path: str, verbose: bool = False) -> bool:
    """
    Standalone function to analyze equipment sequences in an existing ontology file.
    
    Args:
        owl_file_path: Path to the OWL file to analyze
        verbose: Enable verbose output
        
    Returns:
        True if analysis was successful, False otherwise
    """
    # Configure logging
    configure_logging(logging.DEBUG if verbose else logging.INFO)
    logger = logging.getLogger(__name__)
    
    try:
        # Import here to avoid circular imports
        from ontology_generator.analysis.sequence_analysis import (
            generate_equipment_sequence_report, 
            analyze_equipment_sequences,
            generate_enhanced_sequence_report
        )
        from owlready2 import get_ontology, IRIS
        
        logger.info(f"Loading ontology from {owl_file_path} for sequence analysis...")
        # Initialize owlready2 world and load ontology
        from owlready2 import World
        world = World()
        onto = world.get_ontology(owl_file_path).load()
        
        # Get namespace
        IRIS.prefixes[""] = onto.base_iri
        
        logger.info(f"Loaded ontology: {onto.base_iri}")
        
        # Generate and print the standard equipment sequence report
        sequence_report = generate_equipment_sequence_report(onto)
        print(sequence_report)
        
        # Generate and print the enhanced sequence report 
        enhanced_report = generate_enhanced_sequence_report(onto)
        print(enhanced_report)
        
        # Run deeper analysis if verbose
        if verbose:
            sequences, stats = analyze_equipment_sequences(onto)
            print("\n=== EQUIPMENT SEQUENCE STATISTICS ===")
            print(f"Total Lines: {stats['total_lines']}")
            print(f"Lines with Equipment Sequence: {stats['lines_with_sequence']}")
            print(f"Total Equipment in Sequences: {stats['total_equipment']}")
            print("\nEquipment Classes:")
            for cls, count in sorted(stats.get('class_counts', {}).items(), key=lambda x: x[1], reverse=True):
                print(f"  {cls}: {count}")
        
        return True
    except Exception as e:
        logger.error(f"Error analyzing equipment sequences: {e}", exc_info=True)
        return False

def main():
    """Main entry point for the ontology generator."""
    parser = argparse.ArgumentParser(description="Generate an OWL ontology from specification and data CSV files.")
    parser.add_argument("spec_file", help="Path to the ontology specification CSV file (e.g., opera_spec.csv).")
    parser.add_argument("data_file", help="Path to the operational data CSV file (e.g., sample_data.csv).")
    parser.add_argument("output_file", help="Path to save the generated OWL ontology file (e.g., manufacturing.owl).")
    parser.add_argument("--iri", default=DEFAULT_ONTOLOGY_IRI, help=f"Base IRI for the ontology (default: {DEFAULT_ONTOLOGY_IRI}).")
    parser.add_argument("--format", default="rdfxml", choices=["rdfxml", "ntriples", "nquads", "owlxml"], help="Format for saving the ontology (default: rdfxml).")
    parser.add_argument("--reasoner", action="store_true", help="Run the reasoner after population.")
    parser.add_argument("--worlddb", default=None, help="Path to use/create a persistent SQLite world database (e.g., my_ontology.sqlite3).")
    parser.add_argument("--max-report-entities", type=int, default=10, help="Maximum number of entities to show per category in the reasoner report (default: 10).")
    parser.add_argument("--full-report", action="store_true", help="Show full details in the reasoner report (all entities).")
    parser.add_argument("--no-analyze-population", action="store_false", dest="analyze_population", help="Skip analysis and reporting of ontology population (analysis is on by default).")
    parser.add_argument("--strict-adherence", action="store_true", help="Only create classes explicitly defined in the specification.")
    parser.add_argument("--skip-classes", type=str, nargs='+', help="List of class names to skip during ontology creation.")
    parser.add_argument("--optimize", action="store_true", dest="optimize_ontology", help="Generate detailed optimization recommendations.")
    parser.add_argument("--test-mappings", action="store_true", help="Test the property mapping functionality only, without generating the ontology.")
    parser.add_argument("--analyze-sequences", metavar="OWL_FILE", help="Analyze equipment sequences in an existing ontology file.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Enable verbose (DEBUG level) logging.")
    parser.add_argument("-q", "--quiet", action="store_true", help="Suppress INFO level logging.")

    args = parser.parse_args()

    # If analyze-sequences mode is requested, just run the analysis and exit
    if hasattr(args, 'analyze_sequences') and args.analyze_sequences:
        success = analyze_equipment_sequence_in_ontology(args.analyze_sequences, args.verbose)
        sys.exit(0 if success else 1)

    # If test mode is requested, just run the test and exit
    if hasattr(args, 'test_mappings') and args.test_mappings:
        test_property_mappings(args.spec_file)
        sys.exit(0)

    # Setup Logging Level
    log_level = logging.INFO
    if args.verbose: 
        log_level = logging.DEBUG
    elif args.quiet: 
        log_level = logging.WARNING

    # Configure logging
    configure_logging(log_level=log_level)

    # Execute main function
    success = main_ontology_generation(
        args.spec_file, args.data_file, args.output_file,
        args.iri, args.format, args.reasoner, args.worlddb,
        reasoner_report_max_entities=args.max_report_entities,
        reasoner_report_verbose=args.full_report,
        analyze_population=args.analyze_population,
        strict_adherence=args.strict_adherence,
        skip_classes=args.skip_classes,
        optimize_ontology=args.optimize_ontology
    )
    
    # Exit with appropriate code
    if success:
        main_logger.info("Ontology generation process completed.")
        sys.exit(0)
    else:
        main_logger.error("Ontology generation process failed or encountered errors.")
        sys.exit(1)


if __name__ == "__main__":
    main()


===========================================
FILE: ontology_generator/population/__init__.py
===========================================

from .sequence import setup_equipment_sequence_relationships, setup_equipment_instance_relationships
from .linking import link_equipment_events_to_line_events
from .row_processor import process_single_data_row_pass1, process_single_data_row_pass2

# List functions/classes to expose at the package level
__all__ = [
    # 'populate_ontology_from_data', # Removed - Defined in main.py
    'setup_equipment_sequence_relationships',
    'setup_equipment_instance_relationships',
    'link_equipment_events_to_line_events',
    'process_single_data_row_pass1',
    'process_single_data_row_pass2',
    # Add other core components if needed, e.g., PopulationContext? No, likely used internally.
]


===========================================
FILE: ontology_generator/population/asset.py
===========================================

"""
Asset population module for the ontology generator.

This module provides functions for processing asset hierarchy data.
"""
from typing import Dict, Any, Optional, Tuple

from owlready2 import Thing

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, apply_data_property_mappings
)

# Type Alias for registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object

def process_asset_hierarchy(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
) -> Tuple[Optional[Thing], Optional[Thing], Optional[Thing], Optional[Thing]]:
    """
    Processes the asset hierarchy (Plant, Area, ProcessCell, ProductionLine)
    from a single data row for a specific population pass.

    Pass 1: Creates individuals, applies data properties, adds to registry.
    Pass 2: (Currently not handled here, linking done by apply_object_property_mappings)

    Args:
        row: The data row.
        context: Population context.
        property_mappings: Property mappings dictionary.
        all_created_individuals_by_uid: The central registry of individuals.
        pass_num: The current population pass (1 or 2).

    Returns:
        A tuple containing the Plant, Area, ProcessCell, and ProductionLine
        individuals created or retrieved for this row.
    """
    if not property_mappings:
        pop_logger.warning("Property mappings not provided to process_asset_hierarchy. Skipping.")
        return None, None, None, None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_asset_hierarchy. Skipping.")
         return None, None, None, None

    # Get classes from context
    cls_Plant = context.get_class("Plant")
    cls_Area = context.get_class("Area")
    cls_ProcessCell = context.get_class("ProcessCell")
    cls_ProductionLine = context.get_class("ProductionLine")

    if not all([cls_Plant, cls_Area, cls_ProcessCell, cls_ProductionLine]):
        pop_logger.error("One or more essential asset classes (Plant, Area, ProcessCell, ProductionLine) not found. Cannot process hierarchy.")
        return None, None, None, None

    plant_ind: Optional[Thing] = None
    area_ind: Optional[Thing] = None
    pcell_ind: Optional[Thing] = None
    line_ind: Optional[Thing] = None

    # --- Plant ---
    # Check for plantId property mapping
    plant_id_map = property_mappings.get('Plant', {}).get('data_properties', {}).get('plantId')
    if not plant_id_map or not plant_id_map.get('column'):
        pop_logger.error("Cannot determine the column for Plant.plantId from property mappings. Skipping Plant creation.")
        return None, None, None, None
    plant_id_col = plant_id_map['column']
    plant_id = safe_cast(row.get(plant_id_col), str)
    if not plant_id:
        pop_logger.error(f"Missing or invalid Plant ID in column '{plant_id_col}'. Skipping Plant creation.")
        return None, None, None, None  # Plant is essential

    # Create descriptive labels for Plant
    plant_labels = []
    
    # Primary label: Plant ID (required)
    plant_labels.append(plant_id)
    
    # Add descriptive label if available - with property existence check
    plant_name_map = property_mappings.get('Plant', {}).get('data_properties', {}).get('plantName')
    if plant_name_map and plant_name_map.get('column'):
        plant_name = safe_cast(row.get(plant_name_map.get('column')), str)
        if plant_name and plant_name != plant_id:
            plant_labels.append(f"{plant_name}")
    
    # Create or retrieve Plant individual using plant_id as the base identifier
    plant_ind = get_or_create_individual(cls_Plant, plant_id, context.onto, all_created_individuals_by_uid, add_labels=plant_labels)

    if plant_ind and pass_num == 1 and "Plant" in property_mappings:
        apply_data_property_mappings(plant_ind, property_mappings["Plant"], row, context, "Plant", pop_logger)
    elif not plant_ind:
         pop_logger.error(f"Failed to create/retrieve Plant individual for ID '{plant_id}'. Cannot proceed with hierarchy.")
         return None, None, None, None

    # --- Area ---
    # Check for areaId property mapping
    area_id_map = property_mappings.get('Area', {}).get('data_properties', {}).get('areaId')
    if not area_id_map or not area_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for Area.areaId from property mappings. Skipping Area/ProcessCell/Line creation.")
        return plant_ind, None, None, None
    area_id_col = area_id_map['column']

    raw_area_id = row.get(area_id_col)
    if not raw_area_id:
        pop_logger.warning(f"Missing or invalid Area ID in column '{area_id_col}'. Skipping Area/ProcessCell/Line creation.")
        return plant_ind, None, None, None
    area_id = safe_cast(raw_area_id, str)

    # Create descriptive labels for Area
    area_labels = []
    
    # Primary label: Area ID (required)
    area_labels.append(area_id)
    
    # Add descriptive label with hierarchy information
    if plant_id:
        area_labels.append(f"Area {area_id} in Plant {plant_id}")
    
    # Create or retrieve Area individual using area_id as the base identifier
    area_ind = get_or_create_individual(cls_Area, area_id, context.onto, all_created_individuals_by_uid, add_labels=area_labels)

    if area_ind and pass_num == 1 and "Area" in property_mappings:
        # Linking to Plant (locatedInPlant) happens in Pass 2 via apply_object_property_mappings
        apply_data_property_mappings(area_ind, property_mappings["Area"], row, context, "Area", pop_logger)
    elif not area_ind:
         pop_logger.warning(f"Failed to create/retrieve Area individual for ID '{area_id}'. Skipping ProcessCell/Line creation.")
         # Return what we have so far
         return plant_ind, None, None, None

    # --- ProcessCell ---
    # Check for processCellId property mapping
    pcell_id_map = property_mappings.get('ProcessCell', {}).get('data_properties', {}).get('processCellId')
    if not pcell_id_map or not pcell_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for ProcessCell.processCellId from property mappings. Skipping ProcessCell/Line creation.")
        return plant_ind, area_ind, None, None
    pcell_id_col = pcell_id_map['column']

    pcell_id = safe_cast(row.get(pcell_id_col), str)
    if not pcell_id:
        pop_logger.warning(f"Missing or invalid ProcessCell ID in column '{pcell_id_col}'. Skipping ProcessCell/Line creation.")
        return plant_ind, area_ind, None, None

    # Create descriptive labels for ProcessCell
    pcell_labels = []
    
    # Primary label: ProcessCell ID (required)
    pcell_labels.append(pcell_id)
    
    # Add descriptive label with hierarchy information
    if plant_id and area_id:
        pcell_labels.append(f"Process Cell {pcell_id} in Area {area_id}, Plant {plant_id}")
    elif area_id:
        pcell_labels.append(f"Process Cell {pcell_id} in Area {area_id}")
    
    # Create or retrieve ProcessCell individual using pcell_id as the base identifier
    pcell_ind = get_or_create_individual(cls_ProcessCell, pcell_id, context.onto, all_created_individuals_by_uid, add_labels=pcell_labels)

    if pcell_ind and pass_num == 1 and "ProcessCell" in property_mappings:
        # Linking to Area (partOfArea) happens in Pass 2
        apply_data_property_mappings(pcell_ind, property_mappings["ProcessCell"], row, context, "ProcessCell", pop_logger)
    elif not pcell_ind:
        pop_logger.warning(f"Failed to create/retrieve ProcessCell individual for ID '{pcell_id}'. Skipping Line creation.")
        return plant_ind, area_ind, None, None

    # --- ProductionLine ---
    # Check for lineId property mapping
    line_id_map = property_mappings.get('ProductionLine', {}).get('data_properties', {}).get('lineId')
    if not line_id_map or not line_id_map.get('column'):
        pop_logger.warning("Cannot determine the column for ProductionLine.lineId from property mappings. Skipping Line creation.")
        return plant_ind, area_ind, pcell_ind, None
    line_id_col = line_id_map['column']

    raw_line_id = row.get(line_id_col)
    if not raw_line_id:
        pop_logger.warning(f"Missing or invalid Line ID in column '{line_id_col}'. Skipping Line creation.")
        return plant_ind, area_ind, pcell_ind, None # Return created individuals up to this point
    else:
        line_id = safe_cast(raw_line_id, str)
        
        # Use the line_id directly as the unique identifier for ProductionLine
        # This follows the naming convention: #ProductionLine_{LINE_NAME}
        line_unique_base = line_id
        
        # Create descriptive labels for the ProductionLine
        line_labels = []
        
        # Primary label: Line ID (required)
        line_labels.append(line_id)
        
        # Add a more descriptive label if we have plant/area information
        if plant_id and area_id and pcell_id:
            descriptive_label = f"Production Line {line_id} in {pcell_id}, {area_id}, {plant_id}"
            line_labels.append(descriptive_label)
        elif plant_id and pcell_id:
            descriptive_label = f"Production Line {line_id} in {pcell_id}, {plant_id}"
            line_labels.append(descriptive_label)
        
        # Create or retrieve the ProductionLine individual
        line_ind = get_or_create_individual(cls_ProductionLine, line_unique_base, context.onto, all_created_individuals_by_uid, add_labels=line_labels)

        if line_ind and pass_num == 1 and "ProductionLine" in property_mappings:
            # Linking to ProcessCell (locatedInProcessCell) happens in Pass 2
            apply_data_property_mappings(line_ind, property_mappings["ProductionLine"], row, context, "ProductionLine", pop_logger)
        elif not line_ind:
             pop_logger.warning(f"Failed to create/retrieve ProductionLine individual for base '{line_unique_base}'.")
             # Return what we have (line_ind will be None)
             return plant_ind, area_ind, pcell_ind, None

    # Return all individuals created/retrieved in this hierarchy for this row
    return plant_ind, area_ind, pcell_ind, line_ind


def process_material(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
    ) -> Optional[Thing]:
    """
    Processes Material from a row using property mappings (Pass 1: Create/Data Props).

    Args:
        row: The data row
        context: The population context
        property_mappings: Property mappings dictionary
        all_created_individuals_by_uid: Central individual registry.
        pass_num: Current pass number.

    Returns:
        The Material individual or None
    """
    if not property_mappings or "Material" not in property_mappings:
        pop_logger.warning("Property mappings for 'Material' not provided or empty. Skipping material processing.")
        return None
    if all_created_individuals_by_uid is None:
         pop_logger.error("Individual registry not provided to process_material. Skipping.")
         return None

    cls_Material = context.get_class("Material")
    if not cls_Material:
        pop_logger.error("Material class not found in ontology. Skipping material processing.")
        return None

    # Check for materialId property mapping
    material_id_map = property_mappings["Material"].get("data_properties", {}).get("materialId")
    if not material_id_map or not material_id_map.get("column"):
        pop_logger.warning("Required property mapping 'materialId' not found. Skipping material creation.")
        return None

    material_id_col = material_id_map["column"]
    material_id = safe_cast(row.get(material_id_col), str)
    if not material_id:
        pop_logger.warning(f"Missing or invalid Material ID in column '{material_id_col}'. Skipping material creation.")
        return None

    # Create descriptive labels for Material
    material_labels = []
    
    # Primary label: Material ID (required)
    material_labels.append(material_id)
    
    # Add descriptive name label if available - with property existence check
    material_name_map = property_mappings["Material"].get("data_properties", {}).get("materialName")
    if material_name_map and material_name_map.get("column"):
        material_name_col = material_name_map["column"]
        material_name = safe_cast(row.get(material_name_col), str)
        if material_name and material_name != material_id:
            material_labels.append(f"{material_name}")
    
    # Create or retrieve Material individual using material_id as the base identifier
    material_ind = get_or_create_individual(cls_Material, material_id, context.onto, all_created_individuals_by_uid, add_labels=material_labels)

    if material_ind and pass_num == 1:
        apply_data_property_mappings(material_ind, property_mappings["Material"], row, context, "Material", pop_logger)
    
    return material_ind


def process_production_request(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None, # Add registry
    pass_num: int = 1 # Add pass number
    ) -> Optional[Thing]:
    """
    Processes ProductionRequest from a row using property mappings (Pass 1: Create/Data Props).

    Args:
        row: The data row
        context: The population context
        property_mappings: Property mappings dictionary
        all_created_individuals_by_uid: Central individual registry.
        pass_num: Current pass number.

    Returns:
        The ProductionRequest individual or None
    """
    if not property_mappings or "ProductionRequest" not in property_mappings:
        pop_logger.debug("Property mappings for 'ProductionRequest' not provided. Skipping request processing.")
        return None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_production_request. Skipping.")
        return None

    cls_Request = context.get_class("ProductionRequest")
    if not cls_Request:
        pop_logger.error("ProductionRequest class not found in ontology. Skipping request processing.")
        return None

    # Check for requestId property mapping - critical for creating the request
    request_id_map = property_mappings["ProductionRequest"].get("data_properties", {}).get("requestId")
    if not request_id_map or not request_id_map.get("column"):
        pop_logger.warning("Required property mapping 'requestId' not found. Skipping production request creation.")
        return None

    request_id_col = request_id_map["column"]
    request_id = safe_cast(row.get(request_id_col), str)
    if not request_id:
        pop_logger.debug(f"Missing or invalid Request ID in column '{request_id_col}'. Skipping request creation.")
        return None

    # Check for batch property mapping - may be optional but useful for identification
    batch_id_map = property_mappings["ProductionRequest"].get("data_properties", {}).get("batchId")
    batch_id = None
    if batch_id_map and batch_id_map.get("column"):
        batch_id_col = batch_id_map["column"]
        batch_id = safe_cast(row.get(batch_id_col), str)

    # Create a unique base name, potentially incorporating batch ID
    request_unique_base = request_id
    if batch_id:
        request_unique_base = f"{request_id}_{batch_id}"

    # Create descriptive labels for request
    request_labels = []
    
    # Primary label: Request ID (required)
    if batch_id:
        request_labels.append(f"Request {request_id} (Batch {batch_id})")
    else:
        request_labels.append(f"Request {request_id}")
    
    # Create or retrieve the individual
    request_ind = get_or_create_individual(cls_Request, request_unique_base, context.onto, all_created_individuals_by_uid, add_labels=request_labels)

    if request_ind and pass_num == 1:
        apply_data_property_mappings(request_ind, property_mappings["ProductionRequest"], row, context, "ProductionRequest", pop_logger)
    
    return request_ind


===========================================
FILE: ontology_generator/population/core.py
===========================================

"""
Core population module for the ontology generator.

This module provides the base functionality for ontology population, including the
PopulationContext class and property application functions.
"""
from typing import Dict, Any, Optional, List, Set, Tuple, Union, Callable
import logging
import pandas as pd

from owlready2 import (
    Ontology, Thing, ThingClass, PropertyClass,
    locstr, FunctionalProperty, ObjectProperty, DataProperty, ObjectPropertyClass, DataPropertyClass
)

from ontology_generator.utils.logging import pop_logger
from ontology_generator.config import XSD_TYPE_MAP
from ontology_generator.utils.types import safe_cast, sanitize_name

# Type Alias for registry used in linking
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object

class PopulationContext:
    """
    Holds references to ontology elements needed during population.
    
    Attributes:
        onto: The ontology being populated
        classes: Dictionary of defined classes
        props: Dictionary of defined properties
        is_functional: Dictionary indicating whether properties are functional
    """
    def __init__(self, 
                 onto: Ontology, 
                 defined_classes: Dict[str, ThingClass], 
                 defined_properties: Dict[str, PropertyClass], 
                 property_is_functional: Dict[str, bool]):
        """
        Initialize the population context.
        
        Args:
            onto: The ontology being populated
            defined_classes: Dictionary mapping class names to class objects
            defined_properties: Dictionary mapping property names to property objects
            property_is_functional: Dictionary mapping property names to boolean functionality flags
        """
        self.onto = onto
        self.defined_classes = defined_classes
        self.defined_properties = defined_properties
        self.property_is_functional = property_is_functional
        self._property_cache = {} # Cache for faster property lookup
        self._class_cache = {} # Cache for faster class lookup

    def get_class(self, name: str) -> Optional[ThingClass]:
        """
        Get a class by name.
        
        Args:
            name: The name of the class
            
        Returns:
            The class object or None if not found
        """
        if name in self._class_cache:
            return self._class_cache[name]

        cls = self.defined_classes.get(name)
        if not cls: 
            pop_logger.error(f"Essential class '{name}' not found in defined_classes.")
            return None
        # Basic validation (could add more specific checks if needed)
        if not isinstance(cls, ThingClass):
             pop_logger.error(f"Item '{name}' found but is not a ThingClass (checked via isinstance).")
             return None

        self._class_cache[name] = cls
        return cls

    def get_prop(self, name: str) -> Optional[PropertyClass]:
        """
        Get a property by name.
        
        Args:
            name: The name of the property
            
        Returns:
            The property object or None if not found
        """
        if name in self._property_cache:
            return self._property_cache[name]

        prop = self.defined_properties.get(name)
        if not prop:
            pop_logger.warning(f"Property '{name}' not found in defined properties.")
            return None
        # Basic validation (could add more specific checks if needed)
        if not isinstance(prop, (ObjectPropertyClass, DataPropertyClass)):
            pop_logger.error(f"Item '{name}' found but is not a PropertyClass (Object or Data).")
            return None

        self._property_cache[name] = prop
        return prop

    def set_prop(self, individual: Thing, prop_name: str, value: Any) -> None:
        """
        Safely sets a property value using the context.
        
        Args:
            individual: The individual to set the property on
            prop_name: The name of the property to set
            value: The value to set
        """
        prop = self.get_prop(prop_name)
        if not prop:
            # Error logged by get_prop
            return
        is_functional = self.property_is_functional.get(prop_name, False) # Assume non-functional if not specified

        try:
            _set_property_value(individual, prop, value, is_functional)
        except Exception as e:
            pop_logger.error(f"Error setting property '{prop_name}' on individual '{individual.name}' with value '{value}': {e}", exc_info=True)


def _set_property_value(individual: Thing, prop: PropertyClass, value: Any, is_functional: bool) -> None:
    """
    Helper to set functional or non-functional properties, checking existence first.
    
    Args:
        individual: The individual to set the property on
        prop: The property to set
        value: The value to set
        is_functional: Whether the property is functional
    """
    if value is None: 
        return  # Don't set None values

    prop_name = prop.python_name  # Use Python name for attribute access

    try:
        if is_functional:
            # Functional: Use setattr, potentially overwriting. Check if different first.
            current_value = getattr(individual, prop_name, None)
            # Handle comparison carefully, especially for complex types like lists/individuals
            # Simple direct comparison works for primitives and owlready individuals/locstr
            if current_value != value:
                setattr(individual, prop_name, value)
                pop_logger.debug(f"Set functional property {individual.name}.{prop.name} = {repr(value)}")
        else:
            # Non-Functional: Use append, check if value already exists.
            # Initialize the attribute if it doesn't exist yet
            if not hasattr(individual, prop_name) or getattr(individual, prop_name) is None:
                # For non-functional properties, initialize with an empty list
                setattr(individual, prop_name, [])
            
            # Now safely append the value to the list
            current_values = getattr(individual, prop_name)
            # Check if value already exists to avoid duplicates
            if value not in current_values:
                current_values.append(value)
                pop_logger.debug(f"Appended non-functional property {individual.name}.{prop.name} = {repr(value)}")

    except Exception as e:
        pop_logger.error(f"Error setting property '{prop.name}' on individual '{individual.name}' with value '{repr(value)}': {e}", exc_info=False)


def set_prop_if_col_exists(
    context: PopulationContext,
    individual: Thing,
    prop_name: str,
    col_name: str,
    row: Dict[str, Any],
    cast_func: Callable,
    target_type: type,
    logger
) -> bool:
    """Helper function to check if column exists, cast value, and set property if value exists."""
    # Check if column exists in the row
    if col_name not in row:
        logger.error(f"Missing required column '{col_name}' for property '{prop_name}' on individual '{individual.name}' in row: {truncate_row_repr(row)}")
        return False

    # Column exists but might be empty/None/NaN
    raw_value = row.get(col_name)
    if pd.isna(raw_value) or raw_value == '' or raw_value is None:
        logger.debug(f"Column '{col_name}' exists but has null/empty value for property '{prop_name}' on individual '{individual.name}'")
        return False

    # Cast value to target type
    value = cast_func(raw_value, target_type)
    if value is None:  # Cast failed
        logger.warning(f"Failed to cast value '{raw_value}' from column '{col_name}' to type {target_type.__name__} for property '{prop_name}' on individual '{individual.name}'")
        return False

    # Set the property
    context.set_prop(individual, prop_name, value)
    
    # TKT-006: Add specific debug logging for AE model metrics
    ae_metrics = [
        'downtimeMinutes', 
        'runTimeMinutes', 
        'effectiveRuntimeMinutes',
        'goodProductionQuantity',
        'rejectProductionQuantity',
        'allMaintenanceTimeMinutes'
    ]
    
    if prop_name in ae_metrics:
        logger.debug(f"TKT-006: Successfully set AE model metric {prop_name} = {value} (from column {col_name}) on {individual.name}")
    
    return True

def truncate_row_repr(row: Dict[str, Any], max_length: int = 100) -> str:
    """Create a truncated string representation of a row for logging."""
    row_str = str(row)
    if len(row_str) > max_length:
        return row_str[:max_length] + "..."
    return row_str


def get_or_create_individual(
    onto_class: ThingClass,
    individual_name_base: Any,
    onto: Ontology,
    registry: IndividualRegistry, # Use the defined type alias
    add_labels: Optional[List[str]] = None
) -> Optional[Thing]:
    """
    Gets an individual from the registry or creates a new one if it doesn't exist.
    Uses a combination of class name and a base ID/name for the registry key.
    Adds labels if provided.

    Args:
        onto_class: The owlready2 class of the individual.
        individual_name_base: The base name or ID (will be sanitized) used for the individual's name and registry key.
        onto: The ontology instance.
        registry: The dictionary acting as the central registry.
        add_labels: Optional list of labels to add to the individual (if created or found).

    Returns:
        The existing or newly created individual, or None if creation fails.
    """
    if not onto_class or not individual_name_base:
        pop_logger.error(f"Missing onto_class ({onto_class}) or individual_name_base ({individual_name_base}) for get_or_create.")
        return None

    # Sanitize the base name for use in IRI and registry key
    sanitized_name_base = sanitize_name(str(individual_name_base))
    if not sanitized_name_base:
        pop_logger.error(f"Could not sanitize base name '{individual_name_base}' for individual of class '{onto_class.name}'.")
        return None

    class_name_str = onto_class.name
    
    # Use the provided base identifier for the registry key
    # This follows the naming rules specified in the ticket
    registry_key = (class_name_str, sanitized_name_base)

    # Check registry first
    if registry_key in registry:
        existing_individual = registry[registry_key]
        pop_logger.debug(f"Found existing individual '{existing_individual.name}' (Key: {registry_key}) in registry.")
        # Add labels if found and labels provided
        if add_labels:
            for label in add_labels:
                if label and label not in existing_individual.label:
                    existing_individual.label.append(str(label))
        return existing_individual

    # --- If not found, create ---
    # Generate standardized IRI names based on class type
    individual_name = f"{class_name_str}_{sanitized_name_base}"

    try:
        with onto: # Ensure operation within ontology context
             # Check if an individual with this *exact* name already exists in owlready's cache
             # This can happen if safe_name produces the same result for different inputs,
             # or if an individual was created outside the registry mechanism.
            existing_by_name = onto.search_one(iri=f"*{individual_name}")
            if existing_by_name and isinstance(existing_by_name, onto_class):
                pop_logger.warning(f"Individual with name '{individual_name}' already exists in ontology but not registry (Key: {registry_key}). Returning existing one and adding to registry.")
                new_individual = existing_by_name
            elif existing_by_name:
                 # Name collision with an individual of a DIFFERENT class - should be rare with prefixing
                 pop_logger.error(f"Cannot create individual '{individual_name}': Name collision with existing individual '{existing_by_name.name}' of different class ({type(existing_by_name).__name__})")
                 return None
            else:
                # Create the new individual
                new_individual = onto_class(individual_name)
                pop_logger.info(f"Created new individual '{individual_name}' (Class: {class_name_str}, Base: '{individual_name_base}')")

                # Add labels if provided
                if add_labels:
                    for label in add_labels:
                        if label: # Ensure label is not empty
                            new_individual.label.append(str(label)) # Ensure labels are strings

        # Add to registry *after* successful creation
        registry[registry_key] = new_individual
        return new_individual

    except Exception as e:
        pop_logger.error(f"Failed to create individual '{individual_name}' of class '{class_name_str}': {e}", exc_info=True)
        return None


# --- Mappings Application Functions ---

def apply_data_property_mappings(
    individual: Thing,
    mappings: Dict[str, Dict[str, Any]],
    row: Dict[str, Any],
    context: PopulationContext,
    entity_name: str, # Name of the entity type being processed (for logging)
    logger # Pass logger explicitly
) -> None:
    """Applies data property mappings defined in the configuration."""
    if not mappings or 'data_properties' not in mappings:
        return

    data_prop_mappings = mappings.get('data_properties', {})

    for prop_name, details in data_prop_mappings.items():
        col_name = details.get('column')
        # Get cast type from mapping, default to string
        data_type_str = details.get('data_type', 'xsd:string')
        target_type = XSD_TYPE_MAP.get(data_type_str, str) # Map XSD type to Python type
        cast_func = safe_cast # Use the safe_cast utility

        if not col_name:
            logger.warning(f"Data property mapping for {entity_name}.{prop_name} is missing 'column'. Skipping.")
            continue

        # Use helper function to handle casting, existence check, and setting
        set_prop_if_col_exists(
            context=context,
            individual=individual,
            prop_name=prop_name,
            col_name=col_name,
            row=row,
            cast_func=cast_func,
            target_type=target_type,
            logger=logger
        )

def apply_object_property_mappings(
    individual: Thing,
    mappings: Dict[str, Dict[str, Any]],
    row: Dict[str, Any],
    context: PopulationContext,
    entity_name: str, # Name of the entity type being processed (for logging)
    logger, # Pass logger explicitly
    linking_context: IndividualRegistry, # The GLOBAL registry of ALL individuals
    individuals_in_row: Dict[str, Thing], # Individuals created/found specifically for THIS row in Pass 1
    exclude_structural: bool = False
) -> None:
    """Applies ONLY object property mappings, using linking_context or individuals_in_row to find targets."""
    if not mappings or 'object_properties' not in mappings:
        return

    obj_prop_mappings = mappings.get('object_properties', {})
    links_applied_count = 0
    
    # Define known structural properties that should be handled in post-processing
    structural_properties = ["isPartOfProductionLine", "hasEquipmentPart", "memberOfClass"]
    
    # Track missing entities per row to log only once
    missing_context_entities = set()

    for prop_name, details in obj_prop_mappings.items():
        # Skip structural properties if requested
        if exclude_structural and prop_name in structural_properties:
            logger.debug(f"Skipping structural property {entity_name}.{prop_name} for post-processing")
            continue
            
        target_class_name = details.get('target_class')
        col_name = details.get('column') # For linking via ID lookup in GLOBAL registry
        link_context_key = details.get('target_link_context') # For linking via key lookup in CURRENT row context

        if not target_class_name:
            logger.warning(f"Object property mapping for {entity_name}.{prop_name} is missing 'target_class'. Skipping link.")
            continue

        prop = context.get_prop(prop_name)
        if not prop or not isinstance(prop, ObjectPropertyClass): # Ensure it's an ObjectProperty
            logger.warning(f"Object property '{prop_name}' not found or not an ObjectProperty. Skipping link for {entity_name} {individual.name}.")
            continue

        # Add debug for EventRecord.involvesResource specifically
        if entity_name == "EventRecord" and prop_name == "involvesResource":
            if hasattr(individual, "involvesResource") and individual.involvesResource:
                logger.debug(f"EventRecord {individual.name} already has involvesResource set to {individual.involvesResource.name if hasattr(individual.involvesResource, 'name') else individual.involvesResource}")
                # Skip this property if already set
                continue
            else:
                # If missing both column and target_link_context, skip with a more specific message
                if not col_name and not link_context_key:
                    logger.debug(f"Skipping {entity_name}.{prop_name} in Pass 2 since it's handled in Pass 1 directly and missing column/target_link_context")
                    continue

        # Find the target individual
        target_individual: Optional[Thing] = None
        lookup_method = "None"

        if col_name:
            # --- Link via Column Lookup (using GLOBAL registry) ---
            target_base_id = safe_cast(row.get(col_name), str)
            lookup_method = f"Column '{col_name}' (Registry Lookup)"
            if not target_base_id:
                logger.debug(f"Row {row.get('row_num', 'N/A')} - No target ID found in column '{col_name}' for link {entity_name}.{prop_name}. Skipping link.")
                continue

            # Find target in the GLOBAL registry
            registry_key = (target_class_name, target_base_id)
            target_individual = linking_context.get(registry_key)
            if not target_individual:
                 logger.warning(f"Link target {target_class_name} with ID '{target_base_id}' (from {lookup_method}) not found in global registry for relation {entity_name}.{prop_name}. Skipping link for {individual.name}.")
                 continue
            else:
                 logger.debug(f"Found link target {target_individual.name} for {entity_name}.{prop_name} via registry key {registry_key}.")

        elif link_context_key:
             # --- Link via Context Key (using CURRENT row's individuals) ---
             lookup_method = f"Context Key '{link_context_key}' (Row Lookup)"
             # Ensure individuals_in_row is provided and is a dictionary
             if not isinstance(individuals_in_row, dict):
                 logger.warning(f"Cannot link via context key '{link_context_key}' for {entity_name}.{prop_name}: individuals_in_row dictionary was not provided or invalid for row {row.get('row_num', 'N/A')}. Skipping link.")
                 continue

             # Added for TKT-002: Extra debugging for ProductionLine context lookups
             if entity_name == "Equipment" and prop_name == "isPartOfProductionLine":
                 logger.debug(f"Row {row.get('row_num', 'N/A')} - Equipment.isPartOfProductionLine context lookup - Available keys in individuals_in_row: {list(individuals_in_row.keys())}")
                 if "EQUIPMENT_TYPE" in row:
                     logger.debug(f"Row {row.get('row_num', 'N/A')} - EQUIPMENT_TYPE value in row: {row.get('EQUIPMENT_TYPE')}")

             target_individual = individuals_in_row.get(link_context_key)
             if not target_individual:
                 # Track missing context entity to log only once
                 missing_key = f"{link_context_key} for {entity_name}.{prop_name}"
                 if missing_key not in missing_context_entities:
                     missing_context_entities.add(missing_key)
                     logger.warning(f"Context entity '{link_context_key}' required for {entity_name}.{prop_name} not found in individuals_in_row dictionary for row {row.get('row_num', 'N/A')}. Skipping link.")
                 continue
             else:
                 logger.debug(f"Found link target {target_individual.name} for {entity_name}.{prop_name} via row context key '{link_context_key}'.")

        else:
            # Should not happen if parser validation is correct
            logger.error(f"Invalid mapping for object property {entity_name}.{prop_name}: Missing both 'column' and 'target_link_context'. Skipping.")
            continue

        # --- Type Check and Set Property ---
        if target_individual:
            target_cls = context.get_class(target_class_name)
            # Check if the found individual is an instance of the target class (or subclass)
            if not target_cls or not isinstance(target_individual, target_cls):
                 logger.error(f"Type mismatch for link {entity_name}.{prop_name}: Expected {target_class_name} but found target '{target_individual.name}' of type {type(target_individual).__name__} via {lookup_method}. Skipping link.")
                 continue

            # Set the property
            context.set_prop(individual, prop_name, target_individual)
            links_applied_count += 1
            
            # Add specific debug for important links
            if entity_name == "EventRecord":
                if prop_name == "involvesResource":
                    logger.debug(f"Successfully linked EventRecord {individual.name} to resource {target_individual.name} via {prop_name}")
                # TKT-004: Add specific logging for event context relationships
                elif prop_name == "duringShift":
                    logger.info(f"Successfully linked EventRecord {individual.name} to Shift {target_individual.name} via context key '{link_context_key}'")
                elif prop_name == "occursDuring":
                    logger.info(f"Successfully linked EventRecord {individual.name} to TimeInterval {target_individual.name} via context key '{link_context_key}'")
                elif prop_name == "eventHasState":
                    logger.info(f"Successfully linked EventRecord {individual.name} to OperationalState {target_individual.name} via context key '{link_context_key}'")
                elif prop_name == "eventHasReason":
                    logger.info(f"Successfully linked EventRecord {individual.name} to OperationalReason {target_individual.name} via context key '{link_context_key}'")
            
            # Added for TKT-002: Track Equipment-Line links specifically
            if entity_name == "Equipment" and prop_name == "isPartOfProductionLine":
                logger.info(f"Successfully linking Equipment {individual.name} to Line {target_individual.name} via context key '{link_context_key}'")

    # logger.debug(f"Applied {links_applied_count} object property links for {entity_name} individual {individual.name}. Row {row.get('row_num', 'N/A')}.")


# --- DEPRECATED - Combined function (keep for reference temporarily?) ---


===========================================
FILE: ontology_generator/population/equipment.py
===========================================

"""
Equipment population module for the ontology generator.

This module provides functions for processing equipment data.

Equipment Class Identification
-----------------------------
The ontology generator supports multiple strategies for identifying the correct equipment class 
from equipment data. Configuration options are available in config.py:

1. EQUIPMENT_NAME_TO_CLASS_MAP: A dictionary mapping patterns found in equipment names to their 
   corresponding equipment class. This is the highest priority matching method.
   Example: {"_Filler": "Filler", "TFS30": "TubeFillingSealer30"}

2. KNOWN_EQUIPMENT_CLASSES: A list of all known equipment classes used for fallback matching 
   when direct mapping fails.
   
The parse_equipment_class function uses the following priority order:
1. Direct pattern match using EQUIPMENT_NAME_TO_CLASS_MAP
2. Parse from underscore format (e.g., "FIPCO009_Filler" → "Filler")
3. Match against KNOWN_EQUIPMENT_CLASSES (exact match, prefix match, or substring)
4. Equipment model inspection (if available)
5. Generic string extraction (last resort)

When the function identifies a class, it logs which method was used for traceability.
"""
import re
from typing import Dict, Any, Optional, Tuple

from owlready2 import Thing

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, 
    apply_data_property_mappings, apply_object_property_mappings
)
from ontology_generator.config import DEFAULT_EQUIPMENT_SEQUENCE, KNOWN_EQUIPMENT_CLASSES, EQUIPMENT_NAME_TO_CLASS_MAP

def parse_equipment_class(equipment_name: Optional[str], equipment_type: Optional[str] = None, 
                      equipment_model: Optional[str] = None, model: Optional[str] = None,
                      complexity: Optional[str] = None) -> Optional[str]:
    """
    Parses the EquipmentClass from equipment name.
    
    Priority logic for determining equipment class:
    1. Check for match in EQUIPMENT_NAME_TO_CLASS_MAP
    2. Parse from EQUIPMENT_NAME if contains underscore (FIPCO009_Filler)
    3. Check if name matches or contains a known class name from KNOWN_EQUIPMENT_CLASSES
    4. Generic string parsing as fallback
    
    Args:
        equipment_name: The equipment name to parse (primary source)
        equipment_type: Used to validate if it's a Line or Equipment
        equipment_model: Can provide additional clues for equipment class if name is unclear
        model: Alias for equipment_model (maintained for backwards compatibility)
        complexity: Ignored (maintained for backwards compatibility)
        
    Returns:
        The parsed equipment class name or None
    
    Examples:
    - FIPCO009_Filler -> Filler
    - FIPCO009_Filler2 -> Filler (trailing numbers are removed)
    - CasePacker2 -> CasePacker (trailing numbers are removed)
    - FIPCO009_CaseFormer3 -> CaseFormer (trailing numbers are removed)
    - LINE_CLASS (FIPCO009_Filler) -> Filler
    - CLASS (Filler) -> Filler
    - LINE_CLASS# (FIPCO009_Filler2) -> Filler
    - CLASS# (Filler2) -> Filler
    """
    from ontology_generator.config import KNOWN_EQUIPMENT_CLASSES, EQUIPMENT_NAME_TO_CLASS_MAP
    
    # Skip processing immediately if equipment_type is 'Line'
    if equipment_type and equipment_type.lower() == 'line':
        pop_logger.warning(f"'{equipment_name}' is a Line type - not a valid equipment class")
        return None
        
    # Initialize match method tracking (for logging)
    match_method = "None"
    matched_class = None
    
    # --- Process inputs --- 
    if not equipment_name:
        pop_logger.warning("Equipment name is empty or None, cannot parse equipment class")
        return None
        
    # --- Method 1: Direct match from configuration map ---
    if equipment_name and isinstance(equipment_name, str):
        # Check for matches in the equipment name-to-class mapping
        for pattern, class_name in EQUIPMENT_NAME_TO_CLASS_MAP.items():
            if pattern in equipment_name:
                match_method = "Config Map"
                matched_class = class_name
                pop_logger.debug(f"Found equipment class '{matched_class}' via pattern '{pattern}' in config map")
                break
                
    # --- Method 2: Parse from EQUIPMENT_NAME with underscore ---
    if not matched_class and equipment_name and isinstance(equipment_name, str) and '_' in equipment_name:
        parts = equipment_name.split('_')
        class_part = parts[-1]  # Take the part after the last underscore

        # Try to extract base class name by removing trailing digits
        base_class = re.sub(r'\d+$', '', class_part)

        # Validate the base class name
        if base_class and re.search(r'[a-zA-Z]', base_class):
            # Further validate that this looks like an equipment class and not a line ID
            if not base_class.startswith("FIPCO"):
                # Check if this matches or is a substring of a known class
                for known_class in KNOWN_EQUIPMENT_CLASSES:
                    if base_class == known_class or known_class.startswith(base_class):
                        match_method = "Name Underscore Parsing"
                        matched_class = known_class
                        pop_logger.debug(f"Parsed equipment class '{matched_class}' from underscore format '{equipment_name}'")
                        break
                
                # If we didn't find a match in known classes but have a valid class name
                if not matched_class and len(base_class) >= 3:
                    match_method = "Name Underscore Parsing (New Class)"
                    matched_class = base_class
                    pop_logger.debug(f"Parsed potential new equipment class '{matched_class}' from '{equipment_name}'")
            else:
                pop_logger.debug(f"Part after underscore '{base_class}' looks like a line ID, not a valid equipment class")
    
    # --- Method 3: Known Class Matching ---
    if not matched_class and equipment_name and isinstance(equipment_name, str):
        # Attempt exact match with known classes (case-insensitive)
        for known_class in KNOWN_EQUIPMENT_CLASSES:
            # Check if the equipment name IS the class name (with optional trailing digits)
            class_pattern = re.compile(f"^{known_class}\\d*$", re.IGNORECASE)
            if class_pattern.match(equipment_name):
                match_method = "Known Class Exact Match"
                matched_class = known_class
                pop_logger.debug(f"Matched equipment name '{equipment_name}' to known class '{matched_class}'")
                break
            
            # Check if name starts with known class followed by numbers
            if equipment_name.startswith(known_class):
                # Check if what follows is just digits
                remainder = equipment_name[len(known_class):]
                if not remainder or remainder.isdigit() or remainder[0].isdigit():
                    match_method = "Known Class Prefix Match"
                    matched_class = known_class
                    pop_logger.debug(f"Extracted equipment class '{matched_class}' from '{equipment_name}' via prefix")
                    break
            
            # Check if a known class is embedded within the name
            if known_class in equipment_name:
                match_method = "Known Class Substring Match"
                matched_class = known_class
                pop_logger.debug(f"Found equipment class '{matched_class}' within '{equipment_name}'")
                break
    
    # --- Method 4: Equipment Model Inspection (if available) ---
    if not matched_class and equipment_model and isinstance(equipment_model, str):
        model_to_use = equipment_model
        
        # Look for known classes in the model information
        for known_class in KNOWN_EQUIPMENT_CLASSES:
            if known_class in model_to_use:
                match_method = "Model-Based Match"
                matched_class = known_class
                pop_logger.debug(f"Extracted equipment class '{matched_class}' from model '{model_to_use}'")
                break
    
    # --- Method 5: Generic Extraction (most permissive, last resort) ---
    if not matched_class and equipment_name and isinstance(equipment_name, str):
        # Try to extract a class-like string if it starts with capital letter and has no spaces
        words = re.findall(r'[A-Z][a-zA-Z]*', equipment_name)
        if words:
            for word in words:
                # Look for words that might be equipment classes (proper noun-like)
                if len(word) > 3 and word not in ['LINE', 'FIPCO']:
                    base_class = re.sub(r'\d+$', '', word)
                    match_method = "Generic String Extraction"
                    matched_class = base_class
                    pop_logger.debug(f"Extracted potential equipment class '{matched_class}' via generic parsing")
                    break
    
    # Log the result
    if matched_class:
        pop_logger.info(f"Parsed equipment class '{matched_class}' from '{equipment_name}' using method: {match_method}")
        return matched_class
    else:
        pop_logger.warning(f"Could not extract valid equipment class from EQUIPMENT_NAME='{equipment_name}'")
        return None

def process_equipment_and_class(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: Dict[Tuple[str, str], Thing] = None,
    line_ind: Optional[Thing] = None,
    pass_num: int = 1
) -> Tuple[Optional[Thing], Optional[Thing], Optional[Tuple]]:
    """
    Process equipment data to create/retrieve Equipment and EquipmentClass individuals.
    
    Args:
        row: The data row/dict being processed.
        context: The population context with ontology, classes and properties.
        property_mappings: Property mappings dictionary for populating individuals.
        all_created_individuals_by_uid: Registry of created individuals for reuse.
        line_ind: Optional production line individual to link to (if available).
        pass_num: The current processing pass (1 or 2).
    
    Returns:
        Tuple: (equipment_ind, equipment_class_ind, equipment_class_info)
               - equipment_ind: Created/retrieved Equipment individual (or None)
               - equipment_class_ind: Created/retrieved EquipmentClass individual (or None)
               - equipment_class_info: (Optional) Tuple with class name, individual, and sequence info
    """
    # Validate essential inputs
    if not property_mappings:
        pop_logger.warning("Property mappings not provided to process_equipment_and_class. Skipping.")
        return None, None, None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_equipment_and_class. Skipping.")
        return None, None, None
    
    # Get required classes
    cls_Equipment = context.get_class("Equipment")
    cls_EquipmentClass = context.get_class("EquipmentClass")
    
    if not cls_Equipment or not cls_EquipmentClass:
        pop_logger.error("Required classes (Equipment, EquipmentClass) not found in ontology.")
        return None, None, None

    # Initialize result
    eq_class_ind, eq_ind, eq_class_info_out = None, None, None
    
    # Check for equipment name property - critical for identification
    eq_name = None
    eq_name_map = property_mappings.get('Equipment', {}).get('data_properties', {}).get('equipmentName')
    if eq_name_map and eq_name_map.get('column') and eq_name_map['column'] in row:
        eq_name = row.get(eq_name_map['column'], '').strip()
    else:
        # Fall back to direct column lookup
        eq_name = row.get('EQUIPMENT_NAME', '').strip() if 'EQUIPMENT_NAME' in row else None
    
    if not eq_name:
        pop_logger.warning("Missing equipment name. Cannot create equipment instance.")
        return None, None, None
    
    # Check EQUIPMENT_TYPE to determine if we should process equipment or skip
    eq_type = None
    eq_type_map = property_mappings.get('Equipment', {}).get('data_properties', {}).get('equipmentType')
    if eq_type_map and eq_type_map.get('column') and eq_type_map['column'] in row:
        eq_type = row.get(eq_type_map['column'], '').strip()
    else:
        # Fall back to direct column lookup
        eq_type = row.get('EQUIPMENT_TYPE', '').strip() if 'EQUIPMENT_TYPE' in row else 'Equipment'
    
    # If this is a Line type entry, don't create Equipment - it's handled by ProductionLine processing
    if eq_type.lower() == 'line':
        pop_logger.debug(f"EQUIPMENT_TYPE is 'Line' for '{eq_name}' - skipping Equipment instance creation.")
        return None, None, None
    
    # Get equipment ID - critical for uniquely identifying the equipment instance
    # Check for equipmentId property mapping
    eq_id = None
    eq_id_map = property_mappings.get('Equipment', {}).get('data_properties', {}).get('equipmentId')
    if eq_id_map and eq_id_map.get('column') and eq_id_map['column'] in row:
        eq_id = row.get(eq_id_map['column'], '').strip()
    else:
        # Fall back to direct column lookup
        eq_id = row.get('EQUIPMENT_ID', '').strip() if 'EQUIPMENT_ID' in row else None
    
    if not eq_id:
        pop_logger.warning(f"Missing equipment ID for equipment named '{eq_name}'. Cannot create unique Equipment instance.")
        return None, None, None

    # --- EQUIPMENT CLASS IDENTIFICATION ---
    
    # Create/retrieve equipment class from equipment name/type
    eq_class_id_map = property_mappings.get('EquipmentClass', {}).get('data_properties', {}).get('equipmentClassId')
    eq_class_col = eq_class_id_map.get('column') if eq_class_id_map else None
    eq_class_name_map = property_mappings.get('EquipmentClass', {}).get('data_properties', {}).get('equipmentClassName')
    
    eq_class_base_name = None
    eq_class_labels = []
    is_parsed_from_name = False
    
    # Method 1: Try to get directly from a mapped column for equipmentClassId
    if eq_class_col and eq_class_col in row and row[eq_class_col]:
        eq_class_id_value = row[eq_class_col].strip()
        if eq_class_id_value:
            eq_class_base_name = eq_class_id_value
            pop_logger.debug(f"Using equipment class '{eq_class_base_name}' from column '{eq_class_col}'")
    
    # Method 2: Try to parse from equipment name if not yet determined
    if not eq_class_base_name:
        parsed_class = parse_equipment_class(
            equipment_name=eq_name,
            equipment_type=eq_type
        )
        if parsed_class:
            eq_class_base_name = parsed_class
            eq_class_id_value = parsed_class  # Use parsed value as ID value
            is_parsed_from_name = True
            pop_logger.debug(f"Parsed equipment class '{eq_class_base_name}' from equipment name '{eq_name}'")
    
    # If still no class name, use a default or return None
    if not eq_class_base_name:
        pop_logger.warning(f"Could not determine equipment class for '{eq_name}'. Using 'GenericEquipment' as fallback.")
        eq_class_base_name = "GenericEquipment"
        eq_class_id_value = "GenericEquipment"  # Use generic value
    
    # Set proper label for equipment class - use the parsed/determined type name
    # This is the primary label for EquipmentClass
    eq_class_labels = [eq_class_base_name]
    
    # Optionally add the original EQUIPMENT_NAME as a supplementary label if we parsed
    if is_parsed_from_name and eq_class_id_value and eq_class_id_value != eq_class_base_name:
        eq_class_labels.append(f"Source Name: {eq_class_id_value}")
    
    # --- EQUIPMENT CLASS INDIVIDUAL CREATION ---
    
    # Create the EquipmentClass individual using a stable identifier
    try:
        eq_class_ind = get_or_create_individual(
            cls_EquipmentClass, 
            eq_class_base_name, 
            context.onto, 
            all_created_individuals_by_uid, 
            add_labels=eq_class_labels
        )
        
        # Set the sequence position property during population, if available
        eq_class_sequence_map = property_mappings.get('EquipmentClass', {}).get('data_properties', {}).get('classSequencePosition')
        if eq_class_ind and pass_num == 1 and eq_class_sequence_map:
            # Check if the sequence is from the DEFAULT_EQUIPMENT_SEQUENCE or from a column
            sequence_position = None
            
            # Try to get from config first
            if eq_class_base_name in KNOWN_EQUIPMENT_CLASSES:
                sequence_position = KNOWN_EQUIPMENT_CLASSES.index(eq_class_base_name) + 1
                pop_logger.debug(f"Retrieved sequence position {sequence_position} for {eq_class_base_name} from KNOWN_EQUIPMENT_CLASSES")
            
            # Apply data property mappings for EquipmentClass
            if "EquipmentClass" in property_mappings:
                apply_data_property_mappings(eq_class_ind, property_mappings["EquipmentClass"], row, context, "EquipmentClass", pop_logger)
            
            # Create tuple with equipment class info for return
            eq_class_info_out = (eq_class_base_name, eq_class_ind, sequence_position)
    except Exception as e:
        pop_logger.error(f"Error creating EquipmentClass '{eq_class_base_name}': {e}")
    
    # --- EQUIPMENT INDIVIDUAL CREATION ---
    
    # Only create equipment individual if we have a valid equipment id
    if eq_id:
        # Create descriptive labels for Equipment
        eq_labels = []
        
        # Primary label: Full equipment name if available (most descriptive)
        eq_labels.append(eq_name)
        
        # Ensure equipment ID is in labels if not the same as name
        if eq_id != eq_name:
            eq_labels.append(f"ID: {eq_id}")
        
        # Add class information if available
        if eq_class_base_name:
            eq_labels.append(f"Type: {eq_class_base_name}")
            
        # Create the Equipment individual
        try:
            eq_ind = get_or_create_individual(
                cls_Equipment,
                eq_id,  # Use equipment ID as base for stable identifier
                context.onto,
                all_created_individuals_by_uid,
                add_labels=eq_labels
            )
            
            if eq_ind and pass_num == 1:
                # Apply data properties for Equipment
                if "Equipment" in property_mappings:
                    apply_data_property_mappings(eq_ind, property_mappings["Equipment"], row, context, "Equipment", pop_logger)
                
                # --- MANUAL PROPERTY LINKING OUTSIDE OF OBJECT PROPERTY MAPPINGS ---
                
                # 1. Link equipment to its class - add memberOfClass property (handled by object_prop_maps in pass 2 as well)
                if eq_ind and eq_class_ind:
                    # Check if the memberOfClass property is available before attempting to set it
                    member_of_class_prop = context.get_prop("memberOfClass")
                    if member_of_class_prop:
                        context.set_prop(eq_ind, "memberOfClass", eq_class_ind)
                        pop_logger.debug(f"Linked equipment '{eq_name}' to its class '{eq_class_base_name}'")
                    else:
                        pop_logger.warning(f"Required property mapping 'memberOfClass' not found. Cannot link equipment to class.")
                
                # 2. Link equipment to its production line if available
                if eq_ind and line_ind:
                    # Check for existence of isPartOfProductionLine property before attempting to set it
                    part_of_line_prop = context.get_prop("isPartOfProductionLine")
                    if part_of_line_prop:
                        # Set bidirectional links
                        context.set_prop(eq_ind, "isPartOfProductionLine", line_ind)
                        
                        # Check for existence of hasEquipmentPart property before attempting to set it
                        has_part_prop = context.get_prop("hasEquipmentPart")
                        if has_part_prop:
                            context.set_prop(line_ind, "hasEquipmentPart", eq_ind)
                            pop_logger.debug(f"Linked equipment '{eq_name}' to production line")
                        else:
                            pop_logger.warning(f"Required property mapping 'hasEquipmentPart' not found. Cannot link line to equipment.")
                    else:
                        pop_logger.warning(f"Required property mapping 'isPartOfProductionLine' not found. Cannot link equipment to line.")
                    
        except Exception as e:
            pop_logger.error(f"Error creating Equipment '{eq_id}': {e}")
    
    return eq_ind, eq_class_ind, eq_class_info_out


===========================================
FILE: ontology_generator/population/events.py
===========================================

"""
Events population module for the ontology generator.

This module provides functions for processing event-related data.
"""
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List

from owlready2 import Thing, locstr

from ontology_generator.utils.logging import pop_logger
from ontology_generator.utils.types import safe_cast
from ontology_generator.population.core import (
    PopulationContext, get_or_create_individual, apply_data_property_mappings
)
from ontology_generator.config import COUNTRY_TO_LANGUAGE, DEFAULT_LANGUAGE
from ontology_generator.population.linking import link_equipment_events_to_line_events

# Type Alias for registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object
RowIndividuals = Dict[str, Thing] # Key: entity_type_str, Value: Individual Object for this row

def process_shift(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes Shift from a row (Pass 1: Create/Data Props).
    Requires shiftId, startTime, endTime mappings.
    """
    if not property_mappings or "Shift" not in property_mappings:
        pop_logger.debug("Property mappings for 'Shift' not provided. Skipping shift processing.")
        return None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_shift. Skipping.")
        return None

    cls_Shift = context.get_class("Shift")
    if not cls_Shift:
        pop_logger.error("Shift class not found in ontology. Skipping shift processing.")
        return None

    # Check for required property mappings
    shift_id_map = property_mappings['Shift'].get('data_properties', {}).get('shiftId')
    start_time_map = property_mappings['Shift'].get('data_properties', {}).get('shiftStartTime')
    end_time_map = property_mappings['Shift'].get('data_properties', {}).get('shiftEndTime')

    if not shift_id_map or not shift_id_map.get('column'):
        pop_logger.warning("Required property mapping 'shiftId' not found. Skipping shift creation.")
        return None
    # Start/end times are crucial for identification/labeling if ID isn't unique
    if not start_time_map or not start_time_map.get('column'):
         pop_logger.warning("Required property mapping 'shiftStartTime' not found. Skipping shift creation.")
         return None
    if not end_time_map or not end_time_map.get('column'):
         pop_logger.warning("Required property mapping 'shiftEndTime' not found. Skipping shift creation.")
         return None

    shift_id_col = shift_id_map['column']
    shift_id = safe_cast(row.get(shift_id_col), str)
    start_time_str = safe_cast(row.get(start_time_map['column']), str)
    end_time_str = safe_cast(row.get(end_time_map['column']), str)

    if not shift_id:
        pop_logger.debug(f"Missing shift ID in column '{shift_id_col}'. Skipping shift creation.")
        return None
    if not start_time_str:
        pop_logger.debug(f"Missing shift start time in column '{start_time_map['column']}'. Skipping shift creation.")
        return None

    # Create a unique base name, e.g., ShiftID_StartTime
    shift_unique_base = f"{shift_id}_{start_time_str}"
    shift_labels = [shift_id, f"{start_time_str} to {end_time_str or '?'}"]

    shift_ind = get_or_create_individual(cls_Shift, shift_unique_base, context.onto, all_created_individuals_by_uid, add_labels=shift_labels)

    if shift_ind and pass_num == 1:
        apply_data_property_mappings(shift_ind, property_mappings["Shift"], row, context, "Shift", pop_logger)

    return shift_ind

def process_state(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes OperationalState from a row (Pass 1: Create/Data Props).
    Requires stateDescription mapping.
    """
    if not property_mappings or "OperationalState" not in property_mappings:
        pop_logger.debug("Property mappings for 'OperationalState' not provided. Skipping state processing.")
        return None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_state. Skipping.")
        return None

    cls_State = context.get_class("OperationalState")
    if not cls_State:
        pop_logger.error("OperationalState class not found in ontology. Skipping state processing.")
        return None

    # Check for required property mapping: stateDescription
    state_desc_map = property_mappings['OperationalState'].get('data_properties', {}).get('stateDescription')
    if not state_desc_map or not state_desc_map.get('column'):
        pop_logger.warning("Required property mapping 'stateDescription' not found. Skipping state creation.")
        return None

    state_desc_col = state_desc_map['column']
    state_desc = safe_cast(row.get(state_desc_col), str)
    if not state_desc:
        pop_logger.debug(f"Missing state description in column '{state_desc_col}'. Skipping state creation.")
        return None

    # Use description as the base name (assuming descriptions are reasonably unique states)
    state_unique_base = state_desc
    state_labels = [state_desc]

    state_ind = get_or_create_individual(cls_State, state_unique_base, context.onto, all_created_individuals_by_uid, add_labels=state_labels)

    if state_ind and pass_num == 1:
        apply_data_property_mappings(state_ind, property_mappings["OperationalState"], row, context, "OperationalState", pop_logger)

    return state_ind

def process_reason(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1
) -> Optional[Thing]:
    """
    Processes OperationalReason from a row (Pass 1: Create/Data Props).
    Requires reasonDescription or altReasonDescription mapping.
    """
    if not property_mappings or "OperationalReason" not in property_mappings:
        pop_logger.debug("Property mappings for 'OperationalReason' not provided. Skipping reason processing.")
        return None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_reason. Skipping.")
        return None

    cls_Reason = context.get_class("OperationalReason")
    if not cls_Reason:
        pop_logger.error("OperationalReason class not found in ontology. Skipping reason processing.")
        return None

    # Check for required property mappings: reasonDescription or altReasonDescription
    reason_desc_map = property_mappings['OperationalReason'].get('data_properties', {}).get('reasonDescription')
    alt_reason_desc_map = property_mappings['OperationalReason'].get('data_properties', {}).get('altReasonDescription')

    reason_desc_col = None
    reason_desc = None

    if reason_desc_map and reason_desc_map.get('column'):
        reason_desc_col = reason_desc_map['column']
        reason_desc = safe_cast(row.get(reason_desc_col), str)
    elif alt_reason_desc_map and alt_reason_desc_map.get('column'):
        reason_desc_col = alt_reason_desc_map['column']
        reason_desc = safe_cast(row.get(reason_desc_col), str)
        pop_logger.debug(f"Using altReasonDescription column '{reason_desc_col}' for reason.")
    else:
        pop_logger.warning("Required property mapping for reason description (reasonDescription or altReasonDescription) not found. Skipping reason creation.")
        return None

    if not reason_desc:
        pop_logger.debug(f"Missing reason description in column '{reason_desc_col}'. Skipping reason creation.")
        return None

    # Use description as the base name
    reason_unique_base = reason_desc
    reason_labels = [reason_desc]

    reason_ind = get_or_create_individual(cls_Reason, reason_unique_base, context.onto, all_created_individuals_by_uid, add_labels=reason_labels)

    if reason_ind and pass_num == 1:
        apply_data_property_mappings(reason_ind, property_mappings["OperationalReason"], row, context, "OperationalReason", pop_logger)

    return reason_ind

def process_time_interval(
    row: Dict[str, Any],
    context: PopulationContext,
    resource_base_id: str,
    row_num: int,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    pass_num: int = 1,
    infer_missing_end_time: bool = False,  # New parameter to optionally infer missing end times
    default_duration_hours: int = 2  # Default duration for inferring end times
) -> Optional[Thing]:
    """
    Processes TimeInterval from a row (Pass 1: Create/Data Props).
    Requires startTime mapping. endTime mapping is needed for the property but not strictly for creation.
    Uses resource_base_id and row_num for unique naming.
    
    If startTime is missing, creates a robust fallback name using resource ID and row number.
    
    Args:
        row: The data row
        context: The population context
        resource_base_id: The base ID for the resource (equipment/line)
        row_num: The row number for unique naming
        property_mappings: The property mappings
        all_created_individuals_by_uid: The registry of created individuals
        pass_num: The current pass number
        infer_missing_end_time: If True, infer end times for intervals with missing end times
        default_duration_hours: Default duration in hours to use for inferring end times
        
    Returns:
        The created interval individual
    """
    if not property_mappings or "TimeInterval" not in property_mappings:
        pop_logger.debug("Property mappings for 'TimeInterval' not provided. Skipping interval processing.")
        return None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_time_interval. Skipping.")
        return None

    cls_Interval = context.get_class("TimeInterval")
    if not cls_Interval:
        pop_logger.error("TimeInterval class not found in ontology. Skipping interval processing.")
        return None

    # Check for required property mappings: startTime (required) and endTime (optional but important)
    start_map = property_mappings['TimeInterval'].get('data_properties', {}).get('startTime')
    end_map = property_mappings['TimeInterval'].get('data_properties', {}).get('endTime')

    if not start_map or not start_map.get('column'):
        pop_logger.warning("Required property mapping 'startTime' not found. Using fallback naming scheme.")
        has_start_mapping = False
    else:
        has_start_mapping = True

    # Extract available time data with validation
    start_col = start_map.get('column') if start_map else None
    end_col = end_map.get('column') if end_map else None
    
    start_time_str = None
    end_time_str = None
    valid_start_time = False
    valid_end_time = False
    
    # Try to extract startTime value (even if we'll use a fallback name, we want the data property)
    if has_start_mapping and start_col:
        start_time_str = safe_cast(row.get(start_col), str)
        if start_time_str:
            valid_start_time = True
        else:
            pop_logger.warning(f"Row {row_num}: Missing startTime value from column '{start_col}'.")
    
    # Try to extract endTime if available
    if end_col:
        end_time_str = safe_cast(row.get(end_col), str)
        if end_time_str:
            valid_end_time = True
        else:
            pop_logger.debug(f"Row {row_num}: No endTime value in column '{end_col}'.")
            if infer_missing_end_time and valid_start_time:
                # Logic to infer end time would go here if we needed it
                # For now, we're just focusing on property checks
                pass

    # Create a robust unique base name
    if valid_start_time:
        # Create a safe start time string for naming
        safe_start_time_str = start_time_str.replace(":", "").replace("+", "plus").replace(" ", "T")
        interval_unique_base = f"Interval_{resource_base_id}_{safe_start_time_str}_Row{row_num}"
        end_label_part = f"to {end_time_str}" if valid_end_time else "(No End Time)"
        interval_labels = [f"Interval for {resource_base_id} starting {start_time_str} {end_label_part}"]
    else:
        # Fallback when no valid start time, use row-based approach only
        interval_unique_base = f"Interval_{resource_base_id}_Row{row_num}"
        interval_labels = [f"Interval for {resource_base_id} (Row {row_num})"]
        pop_logger.warning(f"Using fallback naming for time interval '{interval_unique_base}' due to missing start time.")

    # Create or retrieve the interval individual
    interval_ind = get_or_create_individual(cls_Interval, interval_unique_base, context.onto, all_created_individuals_by_uid, add_labels=interval_labels)

    if interval_ind and pass_num == 1:
        apply_data_property_mappings(interval_ind, property_mappings["TimeInterval"], row, context, "TimeInterval", pop_logger)

    return interval_ind

def process_event_record(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    # Pass individuals created earlier in the row processing for context if needed
    time_interval_ind: Optional[Thing] = None,
    shift_ind: Optional[Thing] = None,
    state_ind: Optional[Thing] = None,
    reason_ind: Optional[Thing] = None,
    equipment_ind: Optional[Thing] = None, # The primary resource involved
    line_ind: Optional[Thing] = None, # The line context
    material_ind: Optional[Thing] = None, # Optional context
    request_ind: Optional[Thing] = None, # Optional context
    pass_num: int = 1,
    row_num: int = -1
) -> Tuple[Optional[Thing], Optional[Tuple]]:
    """
    Processes EventRecord from a row (Pass 1: Create/Data Props and critical links).
    
    Critical context for individual creation comes from relationships with other individuals:
    - time_interval_ind: When the event occurred
    - equipment_ind OR line_ind: Resource involved (at least one should be present)
    - state_ind: The operational state of the resource
    - reason_ind: Optional reason for the state

    Returns:
        Tuple: (event_ind, event_tuple)
               - event_ind: The created event individual
               - event_tuple: Context tuple for post-processing if needed
    """
    if not property_mappings or "EventRecord" not in property_mappings:
        pop_logger.debug("Property mappings for 'EventRecord' not provided. Skipping event processing.")
        return None, None
    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_event_record. Skipping.")
        return None, None

    cls_Event = context.get_class("EventRecord")
    if not cls_Event:
        pop_logger.error("EventRecord class not found in ontology. Skipping event processing.")
        return None, None

    # Validate required context individuals for creating meaningful events
    if not time_interval_ind:
        pop_logger.warning(f"Row {row_num}: Missing time interval individual for event. Skipping event creation.")
        return None, None
    if not state_ind:
        pop_logger.warning(f"Row {row_num}: Missing state individual for event. Skipping event creation.")
        return None, None
    if not (equipment_ind or line_ind):
        pop_logger.warning(f"Row {row_num}: Missing both equipment and line individuals for event. Need at least one resource. Skipping event creation.")
        return None, None

    # Determine the main resource involved (equipment or line)
    resource_ind = equipment_ind if equipment_ind else line_ind
    resource_type = "Equipment" if equipment_ind else "Line"
    resource_id = None
    
    # Extract resource ID with property existence check
    if resource_type == "Equipment":
        # Check for equipmentId property on the individual
        equipment_id_prop = context.get_prop("equipmentId")
        if equipment_id_prop and hasattr(resource_ind, "equipmentId"):
            resource_id = resource_ind.equipmentId
        else:
            # Fall back to name-based extraction
            if hasattr(resource_ind, "name"):
                resource_id = resource_ind.name.split("_")[-1]  # Extract from individual name
            else:
                resource_id = f"Unknown{row_num}"
                pop_logger.warning(f"Row {row_num}: Could not determine equipment ID for event. Using fallback ID '{resource_id}'.")
    else:  # Line
        # Check for lineId property on the individual
        line_id_prop = context.get_prop("lineId")
        if line_id_prop and hasattr(resource_ind, "lineId"):
            resource_id = resource_ind.lineId
        else:
            # Fall back to name-based extraction
            if hasattr(resource_ind, "name"):
                resource_id = resource_ind.name.split("_")[-1]  # Extract from individual name
            else:
                resource_id = f"UnknownLine{row_num}"
                pop_logger.warning(f"Row {row_num}: Could not determine line ID for event. Using fallback ID '{resource_id}'.")

    # Extract start time for uniqueness if available
    start_time_str = "Unknown"
    start_time_prop = context.get_prop("startTime")
    if start_time_prop and hasattr(time_interval_ind, "startTime"):
        start_time_val = time_interval_ind.startTime
        start_time_str = str(start_time_val).replace(":", "").replace(" ", "T").replace("+", "plus")
    
    # Extract state description for labeling if available
    state_desc = "Unknown State"
    state_desc_prop = context.get_prop("stateDescription")
    if state_desc_prop and hasattr(state_ind, "stateDescription"):
        state_desc = state_ind.stateDescription
    
    # Create a unique ID for the event
    event_unique_base = f"Event_{resource_id}_{start_time_str}_Row{row_num}"
    
    # Create descriptive labels
    event_labels = []
    # Primary user-friendly label based on resource and state
    event_labels.append(f"{resource_type} {resource_id} {state_desc} at {start_time_str}")
    
    # Add reason if available with property existence check
    if reason_ind:
        reason_desc_prop = context.get_prop("reasonDescription")
        reason_desc = None
        if reason_desc_prop and hasattr(reason_ind, "reasonDescription"):
            reason_desc = reason_ind.reasonDescription
            if reason_desc:
                event_labels.append(f"Reason: {reason_desc}")
        
        alt_reason_desc_prop = context.get_prop("altReasonDescription")
        if not reason_desc and alt_reason_desc_prop and hasattr(reason_ind, "altReasonDescription"):
            alt_reason_desc = reason_ind.altReasonDescription
            if alt_reason_desc:
                event_labels.append(f"Reason: {alt_reason_desc}")
    
    # Create or retrieve EventRecord individual
    event_ind = get_or_create_individual(cls_Event, event_unique_base, context.onto, all_created_individuals_by_uid, add_labels=event_labels)

    if event_ind and pass_num == 1:
        # Apply standard data property mappings
        apply_data_property_mappings(event_ind, property_mappings["EventRecord"], row, context, "EventRecord", pop_logger)
        
        # --- CRITICAL OBJECT PROPERTY LINKING FOR EVENT CONTEXT ---
        
        # 1. Link to TimeInterval (when the event occurred) - critical
        # Check for occursDuring property existence
        occurs_during_prop = context.get_prop("occursDuring")
        if occurs_during_prop:
            context.set_prop(event_ind, "occursDuring", time_interval_ind)
        else:
            pop_logger.warning(f"Row {row_num}: Required property 'occursDuring' not found. Cannot link event to time interval.")
        
        # 2. Link to OperationalState - critical
        # Check for hasOperationalState property existence
        has_state_prop = context.get_prop("hasOperationalState")
        if has_state_prop:
            context.set_prop(event_ind, "hasOperationalState", state_ind)
        else:
            pop_logger.warning(f"Row {row_num}: Required property 'hasOperationalState' not found. Cannot link event to state.")
        
        # 3. Link to OperationalReason if available
        if reason_ind:
            # Check for hasOperationalReason property existence
            has_reason_prop = context.get_prop("hasOperationalReason")
            if has_reason_prop:
                context.set_prop(event_ind, "hasOperationalReason", reason_ind)
            else:
                pop_logger.warning(f"Row {row_num}: Required property 'hasOperationalReason' not found. Cannot link event to reason.")
        
        # 4. Link to primary resource (Equipment or ProductionLine) - critical
        # Check for involvesResource property existence
        involves_resource_prop = context.get_prop("involvesResource")
        if involves_resource_prop:
            context.set_prop(event_ind, "involvesResource", resource_ind)
        else:
            pop_logger.warning(f"Row {row_num}: Required property 'involvesResource' not found. Cannot link event to resource.")
        
        # 5. Link to Shift if available
        if shift_ind:
            # Check for occursInShift property existence
            occurs_in_shift_prop = context.get_prop("occursInShift")
            if occurs_in_shift_prop:
                context.set_prop(event_ind, "occursInShift", shift_ind)
            else:
                pop_logger.warning(f"Row {row_num}: Property 'occursInShift' not found. Cannot link event to shift.")
        
        # 6. Link to Material if available and context equipment/line supports it
        if material_ind:
            # Check for involvesMaterial property existence
            involves_material_prop = context.get_prop("involvesMaterial")
            if involves_material_prop:
                context.set_prop(event_ind, "involvesMaterial", material_ind)
            else:
                pop_logger.warning(f"Row {row_num}: Property 'involvesMaterial' not found. Cannot link event to material.")
        
        # 7. Link to ProductionRequest if available
        if request_ind:
            # Check for involvesRequest property existence
            involves_request_prop = context.get_prop("involvesRequest")
            if involves_request_prop:
                context.set_prop(event_ind, "involvesRequest", request_ind)
            else:
                pop_logger.warning(f"Row {row_num}: Property 'involvesRequest' not found. Cannot link event to production request.")
    
    # Create event context tuple for post-processing
    event_context = (event_ind, resource_ind, resource_type) if event_ind else None
    
    return event_ind, event_context

def process_event_related(
    row: Dict[str, Any],
    context: PopulationContext,
    property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None,
    all_created_individuals_by_uid: IndividualRegistry = None,
    # Pass required context individuals identified earlier in the row
    equipment_ind: Optional[Thing] = None,
    line_ind: Optional[Thing] = None,
    material_ind: Optional[Thing] = None,
    request_ind: Optional[Thing] = None,
    pass_num: int = 1,
    row_num: int = -1,  # Add row_num parameter with default value
    infer_missing_end_times: bool = True,  # Control whether to infer missing end times
    default_duration_hours: int = 2  # Default duration for inferred end times
) -> Tuple[RowIndividuals, Optional[Tuple]]:
    """
    Orchestrates the processing of all event-related individuals for a row in a given pass.

    Pass 1: Creates Shift, TimeInterval, State, Reason, EventRecord. Applies data props.
            Returns created individuals and event context tuple.
    Pass 2: (No actions needed here - linking done externally via apply_object_property_mappings)

    Args:
        row: Data row.
        context: Population context.
        property_mappings: Property mappings.
        all_created_individuals_by_uid: Central individual registry.
        equipment_ind: Equipment individual for this event context.
        line_ind: Line individual for this event context.
        material_ind: Optional material individual for context.
        request_ind: Optional production request for context.
        pass_num: Current population pass.
        row_num: Original row number from the source data for robust naming.
        infer_missing_end_times: If True, infer missing end times for time intervals.
        default_duration_hours: Default duration in hours to use for inferring end times.

    Returns:
        Tuple: (created_individuals_dict, event_context_tuple)
                - created_individuals_dict: Dict of event-related individuals created/found.
                - event_context_tuple: Context for later linking steps (from process_event_record).
    """
    created_inds: RowIndividuals = {}
    event_context_out = None

    if all_created_individuals_by_uid is None:
        pop_logger.error("Individual registry not provided to process_event_related. Skipping.")
        return {}, None

    # Get the actual row number from the row dict if available
    actual_row_num = row.get('row_num', row_num)
    if actual_row_num == -1:
        pop_logger.warning(f"No valid row_num provided or found in row. Using fallback value. This may cause naming issues.")

    # Determine resource_base_id needed for interval processing
    # This duplicates some logic from process_event_record but is needed early for interval naming
    resource_base_id = None
    resource_type_hint = row.get('EQUIPMENT_TYPE', 'Equipment').strip()

    if resource_type_hint == 'Line':
        if line_ind:
            resource_base_id = line_ind.lineId[0] if hasattr(line_ind, 'lineId') and line_ind.lineId else line_ind.name
    else: # Default to Equipment
        if equipment_ind:
            resource_base_id = equipment_ind.equipmentId[0] if hasattr(equipment_ind, 'equipmentId') and equipment_ind.equipmentId else equipment_ind.name

    if not resource_base_id:
         pop_logger.warning(f"Row {actual_row_num}: Could not determine resource_base_id early for interval naming (TypeHint: {resource_type_hint}, Line: {line_ind}, Eq: {equipment_ind}). Using fallback.")
         resource_base_id = f"UnknownResource_{hash(str(row))}" # Example: Use row hash for fallback uniqueness

    # Process in dependency order (roughly)
    shift_ind = process_shift(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if shift_ind: created_inds["Shift"] = shift_ind

    # Pass the actual row number and inference parameters to process_time_interval
    time_interval_ind = process_time_interval(
        row, context, resource_base_id, actual_row_num, property_mappings, all_created_individuals_by_uid, pass_num,
        infer_missing_end_time=infer_missing_end_times, default_duration_hours=default_duration_hours
    )
    if time_interval_ind: created_inds["TimeInterval"] = time_interval_ind

    state_ind = process_state(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if state_ind: created_inds["OperationalState"] = state_ind

    reason_ind = process_reason(row, context, property_mappings, all_created_individuals_by_uid, pass_num)
    if reason_ind: created_inds["OperationalReason"] = reason_ind

    # --- Process the main EventRecord --- 
    # Pass the actual row number to process_event_record
    event_ind, event_context_tuple = process_event_record(
        row, context, property_mappings, all_created_individuals_by_uid,
        time_interval_ind=time_interval_ind,
        shift_ind=shift_ind,
        state_ind=state_ind,
        reason_ind=reason_ind,
        equipment_ind=equipment_ind,
        line_ind=line_ind,
        material_ind=material_ind,
        request_ind=request_ind,
        pass_num=pass_num,
        row_num=actual_row_num  # Pass the actual row number
    )
    if event_ind:
        created_inds["EventRecord"] = event_ind
        event_context_out = event_context_tuple # Capture context from successful event creation

    # Only return individuals created/found in this scope
    return created_inds, event_context_out


===========================================
FILE: ontology_generator/population/linking.py
===========================================

"""
Event linking module for the ontology generator.

This module provides functions for linking equipment events to line events.
"""
from collections import defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Set, Optional

from owlready2 import Thing, Ontology, ThingClass, PropertyClass

from ontology_generator.utils.logging import link_logger

def link_equipment_events_to_line_events(onto: Ontology,
                                        created_events_context: List[Tuple[Thing, Thing, str]],
                                        defined_classes: Dict[str, ThingClass],
                                        defined_properties: Dict[str, PropertyClass]) -> int:
    """
    Second pass function to link equipment EventRecords to their containing line EventRecords,
    using relaxed temporal containment logic.
    
    Handles cases where TimeInterval.startTime may be missing or invalid by skipping those events
    for linking rather than failing the entire process.
    
    Args:
        onto: The ontology
        created_events_context: List of tuples (event_ind, resource_ind, resource_type)
                                where resource_type is "Equipment" or "Line"
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        
    Returns:
        The number of links created
    """
    link_logger.info("Starting second pass: Linking equipment events to line events (Enhanced Relaxed Temporal Logic)...")

    # --- Get required classes and properties ---
    cls_EventRecord = defined_classes.get("EventRecord")
    cls_ProductionLine = defined_classes.get("ProductionLine")
    cls_Equipment = defined_classes.get("Equipment")
    prop_isPartOfLineEvent = defined_properties.get("isPartOfLineEvent")
    # Optional: Inverse property
    prop_hasDetailedEquipmentEvent = defined_properties.get("hasDetailedEquipmentEvent")
    prop_startTime = defined_properties.get("startTime")
    prop_endTime = defined_properties.get("endTime")
    prop_occursDuring = defined_properties.get("occursDuring")
    prop_involvesResource = defined_properties.get("involvesResource")
    prop_isPartOfProductionLine = defined_properties.get("isPartOfProductionLine")

    if not all([cls_EventRecord, cls_ProductionLine, cls_Equipment, prop_isPartOfLineEvent, prop_startTime, prop_endTime]):
        link_logger.error("Missing essential classes or properties (EventRecord, ProductionLine, Equipment, isPartOfLineEvent, startTime, endTime) for linking. Aborting.")
        return 0  # Return count of links created

    # --- Configure Linking Parameters ---
    # Time buffer for edge cases: allows equipment events that start slightly before/after line events
    # This helps account for minor clock sync issues or timing discrepancies
    TIME_BUFFER_MINUTES = 5
    time_buffer = timedelta(minutes=TIME_BUFFER_MINUTES)
    
    # Default duration to assume for events with missing end times
    DEFAULT_EVENT_DURATION_HOURS = 2
    default_duration = timedelta(hours=DEFAULT_EVENT_DURATION_HOURS)
    
    link_logger.info(f"Using temporal linking parameters: Buffer={TIME_BUFFER_MINUTES} minutes, Default Duration={DEFAULT_EVENT_DURATION_HOURS} hours")

    # --- Prepare Lookups ---
    line_events_by_line: Dict[Thing, List[Tuple[Thing, Optional[datetime], Optional[datetime]]]] = defaultdict(list)
    equipment_events_to_link: List[Tuple[Thing, Thing, Optional[datetime], Optional[datetime]]] = []  # (eq_event, line_ind, start, end)

    # Create tracking for statistics and diagnostics
    missing_start_count = 0
    missing_end_count = 0
    inferred_end_count = 0
    
    link_logger.debug("Indexing created events...")
    processed_intervals = 0
    skipped_intervals = 0
    for event_ind, resource_ind, resource_type in created_events_context:
        # Get time interval using the occursDuring property
        time_interval_ind = None
        if prop_occursDuring:
            time_interval_ind = getattr(event_ind, prop_occursDuring.python_name, None)
            # Handle potential lists (owlready2 might return a list for object properties)
            if isinstance(time_interval_ind, list) and time_interval_ind:
                time_interval_ind = time_interval_ind[0]
        
        # Skip if no interval exists
        if not time_interval_ind:
            link_logger.warning(f"Event {event_ind.name} has no associated TimeInterval. Cannot use for linking.")
            skipped_intervals += 1
            continue
        
        start_time = None
        end_time = None
        
        # Try to get start and end times safely
        try:
            start_time = getattr(time_interval_ind, prop_startTime.python_name, None)
            end_time = getattr(time_interval_ind, prop_endTime.python_name, None)
            processed_intervals += 1
            
            # Count missing times for diagnostics
            if not isinstance(start_time, datetime):
                missing_start_count += 1
            if not isinstance(end_time, datetime):
                missing_end_count += 1
                
        except Exception as e:
            link_logger.warning(f"Error retrieving time properties from interval {time_interval_ind.name}: {e}")
            skipped_intervals += 1
            continue
        
        # Basic validation: Need at least a start time for meaningful comparison
        # Explicitly check if it's a datetime to avoid type errors later
        if not isinstance(start_time, datetime):
            interval_name = getattr(time_interval_ind, 'name', 'UnnamedInterval')
            link_logger.warning(f"Event {event_ind.name} has invalid or missing start time in interval {interval_name}. Cannot use for linking.")
            skipped_intervals += 1
            continue  # Skip this event for linking if start time is bad

        # Check if it's a line event or equipment event
        if resource_type == "Line":
            # It's a line event
            line_events_by_line[resource_ind].append((event_ind, start_time, end_time))
            link_logger.debug(f"Indexed line event {event_ind.name} for line {resource_ind.name}")
        elif resource_type == "Equipment":
            # It's an equipment event, find its associated line
            associated_line_ind = None
            if prop_isPartOfProductionLine:
                associated_line_ind = getattr(resource_ind, prop_isPartOfProductionLine.python_name, None)
                # Handle potential lists (owlready2 might return a list for object properties)
                if isinstance(associated_line_ind, list) and associated_line_ind:
                    associated_line_ind = associated_line_ind[0]
            
            if associated_line_ind:
                equipment_events_to_link.append((event_ind, associated_line_ind, start_time, end_time))
                link_logger.debug(f"Found equipment event {event_ind.name} for equipment {resource_ind.name} with line {associated_line_ind.name}")
            else:
                link_logger.debug(f"Equipment event {event_ind.name} has no associated line. Cannot link.")

    link_logger.info(f"Indexed {len(line_events_by_line)} lines with line events.")
    link_logger.info(f"Found {len(equipment_events_to_link)} equipment events with context to potentially link.")
    link_logger.info(f"Processed {processed_intervals} valid intervals, skipped {skipped_intervals} invalid/incomplete intervals.")
    link_logger.info(f"Time data statistics: Missing start times: {missing_start_count}, Missing end times: {missing_end_count}")
    
    if processed_intervals == 0 and (len(line_events_by_line) > 0 or len(equipment_events_to_link) > 0):
        link_logger.warning("Processed events but found 0 valid time intervals. Linking will likely fail.")
    
    if missing_end_count > 0:
        link_logger.info(f"Found {missing_end_count} events with missing end times - will apply enhanced linking logic")

    # --- Perform Linking ---
    links_created = 0
    total_equipment_events = len(equipment_events_to_link)
    linked_events = 0
    failed_events = 0
    linking_methods_used = defaultdict(int)
    
    link_logger.info("Attempting to link equipment events to containing line events...")
    with onto:  # Use ontology context for modifications
        for eq_event_ind, line_ind, eq_start, eq_end in equipment_events_to_link:
            potential_parents = line_events_by_line.get(line_ind, [])
            parent_found = False

            # Equipment start time must be valid (already checked during indexing)
            if not isinstance(eq_start, datetime):
                continue

            # If equipment event has no end time, infer a reasonable end time for comparison purposes
            inferred_eq_end = None
            if not isinstance(eq_end, datetime):
                inferred_eq_end = eq_start + default_duration
                inferred_end_count += 1
            
            for line_event_ind, line_start, line_end in potential_parents:
                # Line event start time must be valid (defensive check)
                if not isinstance(line_start, datetime):
                    link_logger.debug(f"Skipping line event {line_event_ind.name} - invalid start time")
                    continue

                # --- Enhanced Temporal Containment Logic ---
                link = False
                link_method = "None"

                # Diagnostic information for logging interval comparison details
                eq_interval_str = f"{eq_start}" + (f" - {eq_end}" if isinstance(eq_end, datetime) else f" - (inferred: {inferred_eq_end})" if inferred_eq_end else " - NoEnd")
                line_interval_str = f"{line_start}" + (f" - {line_end}" if isinstance(line_end, datetime) else " - NoEnd")

                # 1. Check for Strict Containment (requires valid start/end for both)
                if isinstance(eq_end, datetime) and isinstance(line_end, datetime):
                    # Apply time buffer for start comparison (equipment can start slightly before line)
                    strict_cond1 = (line_start - time_buffer <= eq_start <= line_end)
                    # Apply time buffer for end comparison (equipment can end slightly after line)
                    strict_cond2 = (line_start <= eq_end <= line_end + time_buffer)
                    
                    if strict_cond1 and strict_cond2:
                        link = True
                        link_method = "Strict Containment"
                        link_logger.debug(f"Match via strict containment: Equipment event {eq_interval_str} within Line event {line_interval_str}")
                
                # 2. Check for Start Containment (if strict containment failed or missing end times)
                if not link:
                    # Equipment starts within line event timespan (with buffer)
                    # - Equipment starts after or at the same time as line (minus buffer)
                    # - Line has no end OR equipment starts before line ends (plus buffer)
                    start_cond1 = (line_start - time_buffer <= eq_start)
                    start_cond2 = (line_end is None or eq_start <= line_end + time_buffer)
                    
                    if start_cond1 and start_cond2:
                        link = True
                        link_method = "Start Containment"
                        link_logger.debug(f"Match via start containment: Equipment event {eq_interval_str} starts within Line event {line_interval_str}")
                
                # 3. Check for End Containment if we have a real or inferred equipment end time
                if not link and (isinstance(eq_end, datetime) or inferred_eq_end):
                    actual_eq_end = eq_end if isinstance(eq_end, datetime) else inferred_eq_end
                    
                    # Equipment ends within line event timespan (with buffer)
                    # - Line has no end OR equipment ends before line ends (plus buffer)
                    # - Equipment ends after or at the same time as line starts (minus buffer)
                    end_cond1 = (line_end is None or actual_eq_end <= line_end + time_buffer)
                    end_cond2 = (line_start - time_buffer <= actual_eq_end)
                    
                    if end_cond1 and end_cond2:
                        link = True
                        link_method = "End Containment" if isinstance(eq_end, datetime) else "Inferred End Containment"
                        link_logger.debug(f"Match via {link_method}: Equipment event {eq_interval_str} ends within Line event {line_interval_str}")
                
                # 4. Check for Temporal Overlap when both events have a start and at least one has an end time
                if not link and line_end is not None:
                    # Either the equipment event has a real end time or we use the inferred one
                    actual_eq_end = eq_end if isinstance(eq_end, datetime) else inferred_eq_end
                    
                    if actual_eq_end and ((eq_start <= line_end + time_buffer and actual_eq_end >= line_start - time_buffer)):
                        link = True
                        link_method = "Temporal Overlap" if isinstance(eq_end, datetime) else "Inferred Overlap"
                        link_logger.debug(f"Match via {link_method}: Equipment event {eq_interval_str} overlaps with Line event {line_interval_str}")

                # --- End of Enhanced Containment Logic ---

                if link:
                    try:
                        # Link: Equipment Event ---isPartOfLineEvent---> Line Event
                        current_parents = getattr(eq_event_ind, prop_isPartOfLineEvent.python_name, [])
                        if not isinstance(current_parents, list): 
                            current_parents = [current_parents] if current_parents is not None else []

                        if line_event_ind not in current_parents:
                            getattr(eq_event_ind, prop_isPartOfLineEvent.python_name).append(line_event_ind)
                            links_created += 1
                            linking_methods_used[link_method] += 1
                            link_logger.info(f"Linked ({link_method}): {eq_event_ind.name} isPartOfLineEvent {line_event_ind.name}")

                            # Optional: Link inverse if property exists
                            if prop_hasDetailedEquipmentEvent:
                                current_children = getattr(line_event_ind, prop_hasDetailedEquipmentEvent.python_name, [])
                                if not isinstance(current_children, list): 
                                    current_children = [current_children] if current_children is not None else []

                                if eq_event_ind not in current_children:
                                    getattr(line_event_ind, prop_hasDetailedEquipmentEvent.python_name).append(eq_event_ind)
                                    link_logger.debug(f"Linked Inverse: {line_event_ind.name} hasDetailedEquipmentEvent {eq_event_ind.name}")

                            parent_found = True
                            linked_events += 1
                            break  # Stop searching for parents for this equipment event
                        else:
                            # Log if the link already existed (useful for debugging duplicates/re-runs)
                            link_logger.debug(f"Link already exists: {eq_event_ind.name} isPartOfLineEvent {line_event_ind.name}. Skipping append.")
                            parent_found = True  # Treat existing link as success
                            linked_events += 1  # Count as linked since it's already linked
                            break
                    except Exception as e:
                        link_logger.error(f"Error linking equipment event {eq_event_ind.name} to line event {line_event_ind.name}: {e}")
                        continue
                    
            if not parent_found:
                failed_events += 1
                link_logger.debug(f"Could not find suitable parent line event for equipment event {eq_event_ind.name}")
    
    # --- Report Results ---
    link_logger.info(f"Equipment Event Linking Complete: Created {links_created} links between equipment and line events")
    link_logger.info(f"Linking stats: {linked_events}/{total_equipment_events} equipment events linked ({failed_events} failed)")
    
    # Report which linking methods were used - helps understand temporal matching patterns
    for method, count in linking_methods_used.items():
        link_logger.info(f"  - {method}: {count} links")
    
    # Report inferred end times usage
    if inferred_end_count > 0:
        link_logger.info(f"Used {inferred_end_count} inferred end times for linking")
    
    return links_created


===========================================
FILE: ontology_generator/population/processing.py
===========================================

"""
Module for processing individual data rows during ontology population.
"""
import logging
from typing import Any, Dict, Optional, Tuple

from owlready2 import Thing, ThingClass

# Assuming PopulationContext is defined elsewhere and imported appropriately
from .core import PopulationContext
# Assuming processing functions are available
from .asset import process_asset_hierarchy, process_material, process_production_request
from .equipment import process_equipment
from .events import process_shift, process_state_reason, process_time_interval, process_event_record

# Use a logger specific to this module
proc_logger = logging.getLogger(__name__)

# Define a return type structure for clarity
RowProcessingResult = Tuple[
    bool,  # Success status
    Optional[Tuple[Thing, Thing, Thing, Thing]], # event_context: (event_ind, resource_ind, time_interval_ind, line_ind_associated)
    Optional[Tuple[str, Thing, Optional[int]]] # eq_class_info: (eq_class_name, eq_class_ind, position)
]

def process_single_data_row(row: Dict[str, Any],
                            row_num: int,
                            context: PopulationContext,
                            property_mappings: Optional[Dict[str, Dict[str, Dict[str, Any]]]] = None) \
                            -> RowProcessingResult:
    """
    Processes a single data row to create ontology individuals and relationships.

    Args:
        row: The data dictionary for the current row.
        row_num: The original row number (for logging).
        context: The PopulationContext object.
        property_mappings: The parsed property mappings.

    Returns:
        A tuple containing:
        - bool: True if processing was successful, False otherwise.
        - Optional[Tuple]: Event context tuple (event_ind, resource_ind, time_interval_ind, line_ind_associated)
                         if an event was successfully created and linked, otherwise None.
        - Optional[Tuple]: Equipment class info (name, individual, position) if relevant,
                         otherwise None.
    """
    proc_logger.debug(f"--- Processing Row {row_num} ---")
    try:
        # 1. Process Asset Hierarchy -> plant, area, pcell, line individuals
        plant_ind, area_ind, pcell_ind, line_ind = process_asset_hierarchy(row, context, property_mappings)
        if not plant_ind:  # Plant is essential
            raise ValueError("Failed to establish Plant individual, cannot proceed with row.")

        # 2. Determine Resource (Line or Equipment) for the Event
        eq_type = row.get('EQUIPMENT_TYPE', '')
        resource_individual: Optional[Thing] = None
        resource_base_id: Optional[str] = None  # For naming related individuals
        equipment_ind: Optional[Thing] = None
        eq_class_ind: Optional[ThingClass] = None
        eq_class_name: Optional[str] = None
        eq_class_pos: Optional[int] = None
        eq_class_info_result: Optional[Tuple[str, Thing, Optional[int]]] = None

        if eq_type == 'Line' and line_ind:
            resource_individual = line_ind
            resource_base_id = line_ind.name
            proc_logger.debug(f"Row {row_num}: Identified as Line record for: {line_ind.name}")

        elif eq_type == 'Equipment':
            equipment_ind, eq_class_ind, eq_class_name = process_equipment(row, context, line_ind, property_mappings)
            if equipment_ind:
                resource_individual = equipment_ind
                resource_base_id = f"Eq_{equipment_ind.name}"
                if eq_class_ind and eq_class_name:
                    # Attempt to get position; getattr returns None if attribute doesn't exist
                    pos_val = getattr(eq_class_ind, "defaultSequencePosition", None)
                    # Ensure position is an integer if found
                    eq_class_pos = int(pos_val) if isinstance(pos_val, (int, float, str)) and str(pos_val).isdigit() else None
                    eq_class_info_result = (eq_class_name, eq_class_ind, eq_class_pos)
                    proc_logger.debug(f"Row {row_num}: Processed Equipment {equipment_ind.name} of class {eq_class_name} (Pos: {eq_class_pos})")
            else:
                 proc_logger.warning(f"Row {row_num}: Identified as Equipment record, but failed to process Equipment individual. Event linkages might be incomplete.")

        else:
            proc_logger.warning(f"Row {row_num}: Could not determine resource. EQUIPMENT_TYPE='{eq_type}', EQUIPMENT_ID='{row.get('EQUIPMENT_ID')}', LINE_NAME='{row.get('LINE_NAME')}'. Event linkages might be incomplete.")

        if not resource_individual:
            proc_logger.error(f"Row {row_num}: No valid resource (Line or Equipment) identified. Cannot link event record correctly.")
            resource_base_id = f"UnknownResource_Row{row_num}" # Fallback for naming
            # Continue processing other parts, but event linking will fail later

        # 3. Process Material
        material_ind = process_material(row, context, property_mappings)

        # 4. Process Production Request
        request_ind = process_production_request(row, context, property_mappings)

        # 5. Process Shift
        shift_ind = process_shift(row, context, property_mappings)

        # 6. Process State & Reason
        state_ind, reason_ind = process_state_reason(row, context, property_mappings)

        # 7. Process Time Interval
        time_interval_ind = process_time_interval(row, context, resource_base_id, row_num, property_mappings)

        # 8. Process Event Record and Links
        event_ind: Optional[Thing] = None
        event_context_result: Optional[Tuple[Thing, Thing, Thing, Thing]] = None
        if resource_individual and time_interval_ind: # Need resource and interval for meaningful event
            event_ind = process_event_record(row, context, resource_individual, resource_base_id, row_num,
                                             request_ind, material_ind, time_interval_ind,
                                             shift_ind, state_ind, reason_ind, property_mappings)
            if not event_ind:
                raise ValueError("Failed to create EventRecord individual.")
            else:
                # Determine associated line for linking context
                associated_line_ind: Optional[Thing] = None
                prod_line_class = context.get_class("ProductionLine")
                equipment_class = context.get_class("Equipment")
                part_of_prop = context.get_prop("isPartOfProductionLine")

                if prod_line_class and isinstance(resource_individual, prod_line_class):
                    associated_line_ind = resource_individual
                elif equipment_class and part_of_prop and isinstance(resource_individual, equipment_class):
                    # Safely access potentially multi-valued property
                    line_val = getattr(resource_individual, part_of_prop.python_name, None)
                    if isinstance(line_val, list) and line_val:
                        associated_line_ind = line_val[0] # Take first if multiple
                    elif line_val and not isinstance(line_val, list):
                        associated_line_ind = line_val # Assign if single value

                # Check if associated_line_ind is indeed a ProductionLine instance
                if prod_line_class and isinstance(associated_line_ind, prod_line_class):
                    event_context_result = (event_ind, resource_individual, time_interval_ind, associated_line_ind)
                    proc_logger.debug(f"Row {row_num}: Stored context for Event {event_ind.name} (Resource: {resource_individual.name}, Line: {associated_line_ind.name})")
                else:
                    proc_logger.warning(f"Row {row_num}: Could not determine associated ProductionLine for Event {event_ind.name} (Resource: {resource_individual.name}). Skipping context for isPartOfLineEvent linking.")
        elif not resource_individual:
             proc_logger.warning(f"Row {row_num}: Skipping EventRecord creation as no valid resource individual was found.")
        elif not time_interval_ind:
             proc_logger.warning(f"Row {row_num}: Skipping EventRecord creation as no valid time interval individual was found or created.")

        # Return success and any gathered context/info
        return True, event_context_result, eq_class_info_result

    except (KeyError, ValueError, TypeError, AttributeError) as specific_err:
        # Log specific errors with traceback
        proc_logger.error(f"Specific error processing data row {row_num} (Type: {type(specific_err).__name__}): {row if len(str(row)) < 500 else str(row)[:500] + '...'}", exc_info=True)
        return False, None, None # Indicate failure
    except Exception as e:
        # Log unexpected errors with traceback
        proc_logger.error(f"An unexpected error processing data row {row_num}: {row if len(str(row)) < 500 else str(row)[:500] + '...'}", exc_info=True)
        return False, None, None # Indicate failure

===========================================
FILE: ontology_generator/population/row_processor.py
===========================================

import logging
from typing import Any, Dict, Optional, Tuple, List
from owlready2 import Thing

# Assuming imports for processing functions from other modules
from .asset import process_asset_hierarchy, process_material, process_production_request
from .equipment import process_equipment_and_class
from .events import process_event_related
# Need Person processing if added?
# from .person import process_person

# Import core components needed
from .core import PopulationContext, apply_object_property_mappings

# Logger setup
row_proc_logger = logging.getLogger(__name__)

# Type Alias for the central registry
IndividualRegistry = Dict[Tuple[str, str], Thing] # Key: (entity_type_str, unique_id_str), Value: Individual Object
RowIndividuals = Dict[str, Thing] # Key: entity_type_str, Value: Individual Object for this row


def process_single_data_row_pass1(
    row: Dict[str, Any],
    row_num: int,
    context: PopulationContext,
    property_mappings: Dict[str, Dict[str, Dict[str, Any]]],
    all_created_individuals_by_uid: IndividualRegistry
) -> Tuple[bool, RowIndividuals, Optional[Tuple], Optional[Tuple]]:
    """
    Processes a single data row during Pass 1: Creates individuals and applies data properties.

    Args:
        row: The data row dictionary.
        row_num: The original row number (for logging).
        context: The PopulationContext.
        property_mappings: The parsed property mappings.
        all_created_individuals_by_uid: The central registry to populate and use for get_or_create.

    Returns:
        Tuple: (success_flag, created_individuals_in_row, event_context_tuple, eq_class_info_tuple)
               - success_flag (bool): True if the essential parts of the row were processed.
               - created_individuals_in_row (Dict[str, Thing]): Individuals created/retrieved for this row.
               - event_context_tuple (Optional[Tuple]): Context for linking events later.
               - eq_class_info_tuple (Optional[Tuple]): Info for equipment class tracking.
    """
    row_proc_logger.debug(f"Row {row_num} - Pass 1 Start")
    created_inds_this_row: RowIndividuals = {}
    event_context = None
    eq_class_info = None
    success = True # Assume success unless critical failure

    try:
        # Add row_num to the row dictionary for use by downstream processors
        row['row_num'] = row_num
        
        # --- 1. Process Asset Hierarchy (Plant, Area, ProcessCell, ProductionLine) ---
        plant_ind, area_ind, pcell_ind, line_ind = process_asset_hierarchy(
            row, context, property_mappings, all_created_individuals_by_uid, pass_num=1
        )
        if plant_ind: created_inds_this_row["Plant"] = plant_ind
        if area_ind: created_inds_this_row["Area"] = area_ind
        if pcell_ind: created_inds_this_row["ProcessCell"] = pcell_ind
        if line_ind: created_inds_this_row["ProductionLine"] = line_ind
        if not plant_ind:
             row_proc_logger.error(f"Row {row_num} - Pass 1: Failed to process mandatory Plant. Aborting row.")
             return False, {}, None, None

        # --- 2. Process Equipment & Equipment Class ---
        equipment_ind, eq_class_ind, eq_class_info_out = None, None, None
        
        # Check EQUIPMENT_TYPE to determine if we should process equipment
        equipment_type = row.get('EQUIPMENT_TYPE', '').strip() if 'EQUIPMENT_TYPE' in row else 'Equipment'
        
        if equipment_type == 'Equipment':
            # Only process equipment and class if it's actually an equipment type
            equipment_ind, eq_class_ind, eq_class_info_out = process_equipment_and_class(
                row, context, property_mappings, all_created_individuals_by_uid, line_ind, pass_num=1
            )
            if equipment_ind: created_inds_this_row["Equipment"] = equipment_ind
            if eq_class_ind: created_inds_this_row["EquipmentClass"] = eq_class_ind
            if eq_class_info_out: eq_class_info = eq_class_info_out
        else:
            row_proc_logger.debug(f"Row {row_num} - Not processing equipment for EQUIPMENT_TYPE='{equipment_type}'")

        # --- 3. Process Material ---
        material_ind = process_material(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        if material_ind: created_inds_this_row["Material"] = material_ind

        # --- 4. Process Production Request ---
        request_ind = process_production_request(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        if request_ind: created_inds_this_row["ProductionRequest"] = request_ind

        # --- 5. Process Events (EventRecord, TimeInterval, Shift, State, Reason) ---
        event_related_inds, event_context_out = process_event_related(
            row, context, property_mappings, all_created_individuals_by_uid,
            equipment_ind=equipment_ind,
            line_ind=line_ind,
            material_ind=material_ind, # Pass context
            request_ind=request_ind, # Pass context
            pass_num=1,
            row_num=row_num  # Pass the actual row number explicitly
        )
        created_inds_this_row.update(event_related_inds)
        if event_context_out: event_context = event_context_out

        # --- 6. Process Person (Example) ---
        # person_ind = process_person(row, context, property_mappings, all_created_individuals_by_uid, pass_num=1)
        # if person_ind: created_inds_this_row["Person"] = person_ind

        row_proc_logger.debug(f"Row {row_num} - Pass 1 End. Created/found {len(created_inds_this_row)} individuals.")

    except Exception as e:
        row_proc_logger.error(f"Row {row_num} - Pass 1: Critical error processing row: {e}", exc_info=True)
        success = False
        created_inds_this_row = {} # Clear partial results on error
    finally:
        # Clean up row dictionary by removing temporary row_num 
        if 'row_num' in row:
            del row['row_num']

    return success, created_inds_this_row, event_context, eq_class_info


def process_single_data_row_pass2(
    row: Dict[str, Any],
    row_num: int,
    context: PopulationContext,
    property_mappings: Dict[str, Dict[str, Dict[str, Any]]],
    individuals_in_row: RowIndividuals,
    linking_context: IndividualRegistry
) -> bool:
    """
    Processes a single data row during Pass 2: Applies object property mappings.

    Args:
        row: The data row dictionary.
        row_num: The original row number (for logging).
        context: The PopulationContext.
        property_mappings: The parsed property mappings.
        individuals_in_row: Dictionary of individuals created/retrieved for THIS row in Pass 1.
        linking_context: The central registry of ALL created individuals from Pass 1.

    Returns:
        bool: True if linking was attempted successfully (even if some links failed safely), False on critical error.
    """
    row_proc_logger.debug(f"Row {row_num} - Pass 2 Start")
    success = True # Assume success unless critical error

    # Add row_num to row dict temporarily for potential use in logging within apply funcs
    row['row_num'] = row_num 

    try:
        # Iterate through the individuals created for this row in Pass 1
        for entity_type, individual in individuals_in_row.items():
            if not individual:
                continue

            if entity_type in property_mappings:
                # Apply object property mappings for this entity type using the full context
                # But exclude structural properties which will be handled in a separate post-processing step
                apply_object_property_mappings(
                    individual,
                    property_mappings[entity_type],
                    row,
                    context,
                    entity_type,
                    row_proc_logger,
                    linking_context,
                    individuals_in_row,
                    exclude_structural=True
                )

        row_proc_logger.debug(f"Row {row_num} - Pass 2 End.")

    except Exception as e:
        row_proc_logger.error(f"Row {row_num} - Pass 2: Critical error during linking: {e}", exc_info=True)
        success = False
    finally:
        # Clean up temporary key
        if 'row_num' in row: del row['row_num']

    return success 

def process_structural_relationships(
    context: PopulationContext,
    property_mappings: Dict[str, Dict[str, Dict[str, Any]]],
    all_created_individuals_by_uid: IndividualRegistry,
    logger=None
) -> int:
    """
    Post-processing function that establishes structural relationships between individuals that were created
    from different rows. This addresses the limitation of row-based Pass 2 linking for structural properties.

    Args:
        context: The PopulationContext.
        property_mappings: The parsed property mappings.
        all_created_individuals_by_uid: The central registry of ALL created individuals.
        logger: Logger to use (defaults to row_proc_logger if None)

    Returns:
        int: The number of structural links created
    """
    # Use the provided logger or fall back to the module logger
    log = logger or row_proc_logger
    log.info("Starting post-processing of structural relationships...")
    
    # Structure to track links by type
    links_created = 0
    links_by_type = {}
    
    # Define known structural properties
    structural_properties = [
        "isPartOfProductionLine",      # Equipment -> ProductionLine
        "hasEquipmentPart",            # ProductionLine -> Equipment
        "memberOfClass"                # Equipment -> EquipmentClass
    ]
    
    # Process Equipment -> ProductionLine relationships
    if "Equipment" in property_mappings and "object_properties" in property_mappings["Equipment"]:
        log.info("Processing Equipment.isPartOfProductionLine structural relationships...")
        
        # Get all Equipment individuals
        equipment_individuals = [
            ind for uid, ind in all_created_individuals_by_uid.items() 
            if uid[0] == "Equipment"
        ]
        
        # Get all ProductionLine individuals
        line_individuals = [
            ind for uid, ind in all_created_individuals_by_uid.items() 
            if uid[0] == "ProductionLine"
        ]
        
        # Get the isPartOfProductionLine property mapping
        eq_line_mapping = property_mappings["Equipment"]["object_properties"].get("isPartOfProductionLine")
        
        if eq_line_mapping and equipment_individuals and line_individuals:
            # The column that contains the Line ID for Equipment
            line_id_column = eq_line_mapping.get("column")
            
            if line_id_column:
                # First, build a lookup map for lines by their ID
                lines_by_id = {}
                line_id_prop = "lineId" # Property name expected to hold the ID value
                
                # Check if the lineId property exists
                line_id_obj_prop = context.get_prop(line_id_prop)
                if not line_id_obj_prop:
                    log.warning(f"Required property mapping '{line_id_prop}' not found. Cannot create line lookup map for structural relationships.")
                else:
                    for line_ind in line_individuals:
                        # Get the line's ID value from its properties
                        if hasattr(line_ind, line_id_prop) and getattr(line_ind, line_id_prop):
                            line_id = getattr(line_ind, line_id_prop)
                            lines_by_id[str(line_id)] = line_ind
                
                # Check for the isPartOfProductionLine and hasEquipmentPart properties
                is_part_of_line_prop = context.get_prop("isPartOfProductionLine")
                has_equipment_part_prop = context.get_prop("hasEquipmentPart")
                
                if not is_part_of_line_prop:
                    log.warning(f"Required property mapping 'isPartOfProductionLine' not found. Cannot link equipment to lines.")
                elif not has_equipment_part_prop:
                    log.warning(f"Required property mapping 'hasEquipmentPart' not found. Cannot link lines to equipment.")
                else:
                    # Link equipment to lines
                    for eq_ind in equipment_individuals:
                        # Get data stored in the equipment individual
                        eq_data = context.get_individual_data(eq_ind) or {}
                        
                        # Get the line ID this equipment is part of
                        line_id_value = eq_data.get(line_id_column)
                        
                        if line_id_value and str(line_id_value) in lines_by_id:
                            # Get the corresponding line individual
                            line_ind = lines_by_id[str(line_id_value)]
                            
                            # Create bidirectional links if they don't exist
                            # Check if the equipment is already linked to this line
                            current_line = getattr(eq_ind, is_part_of_line_prop.python_name, None)
                            if not current_line or line_ind not in current_line:
                                context.set_prop(eq_ind, "isPartOfProductionLine", line_ind)
                                context.set_prop(line_ind, "hasEquipmentPart", eq_ind)
                                links_created += 1
                                log.debug(f"Linked Equipment {eq_ind.name} to Line {line_ind.name} via isPartOfProductionLine/hasEquipmentPart")
                                
                                # Record the link type for statistics
                                links_by_type["Equipment->Line"] = links_by_type.get("Equipment->Line", 0) + 1
    
    # Process Equipment -> EquipmentClass relationships
    if "Equipment" in property_mappings and "object_properties" in property_mappings["Equipment"]:
        log.info("Processing Equipment.memberOfClass structural relationships...")
        
        # Get all Equipment individuals (if not already fetched)
        if 'equipment_individuals' not in locals():
            equipment_individuals = [
                ind for uid, ind in all_created_individuals_by_uid.items() 
                if uid[0] == "Equipment"
            ]
        
        # Get all EquipmentClass individuals
        class_individuals = [
            ind for uid, ind in all_created_individuals_by_uid.items() 
            if uid[0] == "EquipmentClass"
        ]
        
        # Get the memberOfClass property mapping
        eq_class_mapping = property_mappings["Equipment"]["object_properties"].get("memberOfClass")
        
        if eq_class_mapping and equipment_individuals and class_individuals:
            # Check if the memberOfClass property exists
            member_of_class_prop = context.get_prop("memberOfClass")
            if not member_of_class_prop:
                log.warning(f"Required property mapping 'memberOfClass' not found. Cannot link equipment to classes.")
            else:
                # The column that contains the Class name/ID for Equipment
                class_id_column = eq_class_mapping.get("column")
                if class_id_column:
                    # Build a lookup map for equipment classes
                    classes_by_name = {}
                    class_id_prop = "equipmentClassId" # Property expected to hold the class ID value
                    class_name_prop = "equipmentClassName" # Alternative property
                    
                    # Check if the class identifier properties exist
                    class_id_obj_prop = context.get_prop(class_id_prop)
                    class_name_obj_prop = context.get_prop(class_name_prop)
                    
                    if not class_id_obj_prop and not class_name_obj_prop:
                        log.warning(f"Required property mappings '{class_id_prop}' and '{class_name_prop}' not found. Cannot create class lookup map.")
                    else:
                        primary_class_prop = class_id_obj_prop if class_id_obj_prop else class_name_obj_prop
                        primary_prop_name = class_id_prop if class_id_obj_prop else class_name_prop
                        
                        # Build the class lookup map
                        for class_ind in class_individuals:
                            # Get the class ID/name from its properties
                            if hasattr(class_ind, primary_prop_name) and getattr(class_ind, primary_prop_name):
                                class_id = getattr(class_ind, primary_prop_name)
                                if isinstance(class_id, list) and class_id:
                                    class_id = class_id[0] # Take first item if it's a list
                                classes_by_name[str(class_id)] = class_ind
                            # Also try extracting from the individual name as fallback
                            else:
                                class_name = getattr(class_ind, "name", "").replace("EquipmentClass_", "")
                                if class_name:
                                    classes_by_name[class_name] = class_ind
                        
                        # Link equipment to classes
                        for eq_ind in equipment_individuals:
                            # Get data stored in the equipment individual
                            eq_data = context.get_individual_data(eq_ind) or {}
                            
                            # Get the class this equipment belongs to
                            class_value = eq_data.get(class_id_column)
                            
                            # Try different approaches to match with class
                            matching_class_ind = None
                            
                            if class_value and str(class_value) in classes_by_name:
                                # Direct match by ID/name
                                matching_class_ind = classes_by_name[str(class_value)]
                            else:
                                # Try to extract class from equipment name if column value not found
                                from .equipment import parse_equipment_class
                                eq_name = getattr(eq_ind, "equipmentName", None) or getattr(eq_ind, "name", "")
                                if eq_name:
                                    parsed_class = parse_equipment_class(equipment_name=eq_name)
                                    if parsed_class and str(parsed_class) in classes_by_name:
                                        matching_class_ind = classes_by_name[str(parsed_class)]
                                        log.debug(f"Parsed class '{parsed_class}' from equipment name '{eq_name}'")
                            
                            if matching_class_ind:
                                # Create the link if it doesn't exist
                                current_class = getattr(eq_ind, member_of_class_prop.python_name, None)
                                if not current_class or matching_class_ind not in current_class:
                                    context.set_prop(eq_ind, "memberOfClass", matching_class_ind)
                                    links_created += 1
                                    log.debug(f"Linked Equipment {eq_ind.name} to EquipmentClass {matching_class_ind.name} via memberOfClass")
                                    
                                    # Record the link type for statistics
                                    links_by_type["Equipment->Class"] = links_by_type.get("Equipment->Class", 0) + 1
    
    # Log summary of links created
    for link_type, count in links_by_type.items():
        log.info(f"Created {count} {link_type} structural links")
    
    log.info(f"Post-processing complete: Created {links_created} structural links in total")
    return links_created 

===========================================
FILE: ontology_generator/population/sequence.py
===========================================

"""
Sequence relationship module for the ontology generator.

This module provides functions for setting up equipment sequence relationships.
"""
from typing import Dict, Any, List, Optional, Tuple

from owlready2 import Thing, Ontology, ThingClass, PropertyClass

from ontology_generator.utils.logging import pop_logger
from ontology_generator.population.core import PopulationContext, _set_property_value
from ontology_generator.config import DEFAULT_EQUIPMENT_SEQUENCE, LINE_SPECIFIC_EQUIPMENT_SEQUENCE

def _safe_sort_by_position(items, default_position=999999):
    """
    Safely sorts items by position value, handling None values gracefully.
    
    Args:
        items: Dictionary items (key, value) where value might be None
        default_position: Default value to use for None positions
        
    Returns:
        Sorted list of (key, value) tuples
    """
    def get_safe_position(item):
        key, position = item
        if position is None:
            pop_logger.warning(f"Found None position for {key}, using default position {default_position} for sorting")
            return default_position
        return position
        
    return sorted(items, key=get_safe_position)

def setup_equipment_sequence_relationships(onto: Ontology,
                                           equipment_class_positions: Dict[str, int],
                                           defined_classes: Dict[str, ThingClass],
                                           defined_properties: Dict[str, PropertyClass],
                                           created_equipment_class_inds: Dict[str, Thing]):
    """
    Establish upstream/downstream relationships between equipment *classes* based on sequence positions.
    """
    pop_logger.info("Setting up CLASS-LEVEL equipment sequence relationships based on position...")

    # Get context for properties/classes
    context = PopulationContext(onto, defined_classes, defined_properties, {}) # is_functional map not needed here

    # Get the CLASS-LEVEL properties
    prop_classIsUpstreamOf = context.get_prop("classIsUpstreamOf")
    prop_classIsDownstreamOf = context.get_prop("classIsDownstreamOf") # Optional for inverse

    if not prop_classIsUpstreamOf:
        pop_logger.error("Cannot establish CLASS-LEVEL sequence relationships: 'classIsUpstreamOf' property not defined.")
        return
    if not prop_classIsDownstreamOf:
        pop_logger.warning("'classIsDownstreamOf' inverse property not found. Only forward class relationships will be set.")

    cls_EquipmentClass = context.get_class("EquipmentClass")
    if not cls_EquipmentClass: return # Should have been caught earlier, but safe check

    # Verify domain/range compatibility (optional but good practice)
    if cls_EquipmentClass not in prop_classIsUpstreamOf.domain:
        pop_logger.warning(f"Property 'classIsUpstreamOf' ({prop_classIsUpstreamOf}) does not have EquipmentClass in its domain {prop_classIsUpstreamOf.domain}.")
    # Range check assumes list of classes is expected
    if not any(issubclass(cls_EquipmentClass, r_cls) for r_cls in (prop_classIsUpstreamOf.range if isinstance(prop_classIsUpstreamOf.range, list) else [prop_classIsUpstreamOf.range])):
        pop_logger.warning(f"Property 'classIsUpstreamOf' ({prop_classIsUpstreamOf}) does not have EquipmentClass in its range {prop_classIsUpstreamOf.range}.")


    if not created_equipment_class_inds:
        pop_logger.warning("No created EquipmentClass individuals provided. Cannot establish class relationships.")
        return
    if not equipment_class_positions:
        pop_logger.warning("Equipment class positions dictionary is empty. Cannot establish class relationships.")
        return

    # Sort classes by their position number
    sorted_classes = sorted(equipment_class_positions.items(), key=lambda item: item[1])

    if len(sorted_classes) < 2:
        pop_logger.warning("Not enough equipment classes with sequence positions (< 2) to establish relationships.")
        return

    # Create relationships based on sequence order
    relationships_created = 0
    with onto:
        for i in range(len(sorted_classes) - 1):
            upstream_class_name, up_pos = sorted_classes[i]
            downstream_class_name, down_pos = sorted_classes[i + 1]

            upstream_ind = created_equipment_class_inds.get(upstream_class_name)
            downstream_ind = created_equipment_class_inds.get(downstream_class_name)

            if not upstream_ind:
                pop_logger.warning(f"Sequence setup: Upstream class individual '{upstream_class_name}' not found in provided dict.")
                continue
            if not downstream_ind:
                pop_logger.warning(f"Sequence setup: Downstream class individual '{downstream_class_name}' not found in provided dict.")
                continue

            pop_logger.debug(f"Evaluating CLASS relationship: {upstream_ind.name} (Pos {up_pos}) -> {downstream_ind.name} (Pos {down_pos})")

            # Set relationships (classIsUpstreamOf is NON-functional per spec)
            try:
                # Use helper to check if relationship already exists before appending
                _set_property_value(upstream_ind, prop_classIsUpstreamOf, downstream_ind, is_functional=False)

                # Explicitly set the inverse relationship if available and needed
                if prop_classIsDownstreamOf:
                    _set_property_value(downstream_ind, prop_classIsDownstreamOf, upstream_ind, is_functional=False)

                # Check if the forward relationship was actually added (or already existed)
                if downstream_ind in getattr(upstream_ind, prop_classIsUpstreamOf.python_name, []):
                    relationships_created += 1 # Count successful links (new or existing is fine)
                    pop_logger.debug(f"Confirmed CLASS relationship: {upstream_class_name} classIsUpstreamOf {downstream_class_name}")

            except Exception as e:
                pop_logger.error(f"Error setting class relationship {upstream_class_name} -> {downstream_class_name}: {e}")

    pop_logger.info(f"Established/verified {relationships_created} CLASS-LEVEL upstream relationships.")

    # Print relationship summary to stdout
    print("\n=== EQUIPMENT CLASS SEQUENCE RELATIONSHIP REPORT ===")
    if relationships_created > 0:
        print(f"Established/verified {relationships_created} upstream relationships between Equipment Classes:")
        # Re-iterate to print the established sequence
        for i in range(len(sorted_classes) - 1):
            upstream_class_name, _ = sorted_classes[i]
            downstream_class_name, _ = sorted_classes[i + 1]
            # Check if both individuals exist to avoid errors if one was missing during linking
            if created_equipment_class_inds.get(upstream_class_name) and created_equipment_class_inds.get(downstream_class_name):
                print(f"  {upstream_class_name} → {downstream_class_name}")
    else:
        print("No class-level sequence relationships were created or verified.")
    print(f"Total classes with positions considered: {len(sorted_classes)}")

def setup_equipment_instance_relationships(onto: Ontology,
                                          defined_classes: Dict[str, ThingClass],
                                          defined_properties: Dict[str, PropertyClass],
                                          property_is_functional: Dict[str, bool],
                                          equipment_class_positions: Dict[str, int]):
    """
    Establish upstream/downstream relationships between equipment *instances* within the same production line.
    
    The approach:
    1. Group equipment instances by production line
    2. For each line:
        a. Determine equipment class sequence positions using line-specific or default configuration
        b. Assign sequencePosition to each Equipment instance based on its class's position
        c. Sort instances on the line by sequencePosition and then by equipmentId (for same position)
        d. Link sorted instances with isImmediatelyUpstreamOf/isImmediatelyDownstreamOf relationships
    
    Args:
        onto: The ontology
        defined_classes: Dictionary of defined classes
        defined_properties: Dictionary of defined properties
        property_is_functional: Dictionary indicating whether properties are functional
        equipment_class_positions: Dictionary mapping equipment class names to sequence positions
    """
    pop_logger.info("Setting up INSTANCE-LEVEL equipment relationships within production lines...")

    # Get context for properties/classes
    context = PopulationContext(onto, defined_classes, defined_properties, property_is_functional)

    # Get the required classes and properties
    cls_Equipment = context.get_class("Equipment")
    cls_ProductionLine = context.get_class("ProductionLine")
    cls_EquipmentClass = context.get_class("EquipmentClass")
    
    # Get instance-level properties
    prop_isPartOfProductionLine = context.get_prop("isPartOfProductionLine")
    prop_memberOfClass = context.get_prop("memberOfClass")
    prop_equipmentClassId = context.get_prop("equipmentClassId")
    prop_equipmentId = context.get_prop("equipmentId")
    prop_sequencePosition = context.get_prop("sequencePosition")
    prop_isImmediatelyUpstreamOf = context.get_prop("isImmediatelyUpstreamOf")
    prop_isImmediatelyDownstreamOf = context.get_prop("isImmediatelyDownstreamOf")

    # Check essentials
    required_components = [
        cls_Equipment, cls_ProductionLine, cls_EquipmentClass,
        prop_isPartOfProductionLine, prop_memberOfClass, prop_equipmentClassId,
        prop_equipmentId, prop_sequencePosition, prop_isImmediatelyUpstreamOf
    ]
    
    missing_components = [name for i, name in enumerate([
        "Equipment", "ProductionLine", "EquipmentClass",
        "isPartOfProductionLine", "memberOfClass", "equipmentClassId",
        "equipmentId", "sequencePosition", "isImmediatelyUpstreamOf"
    ]) if not required_components[i]]
    
    if missing_components:
        pop_logger.error(f"Missing required components for equipment sequencing: {', '.join(missing_components)}")
        return

    if not prop_isImmediatelyDownstreamOf:
        pop_logger.warning("'isImmediatelyDownstreamOf' inverse property not found. Only forward instance relationships will be set.")

    if not equipment_class_positions:
        pop_logger.warning("Equipment class positions dictionary is empty. Cannot establish instance relationships.")
        return
        
    # Log the equipment class positions for better diagnostics
    pop_logger.info(f"Equipment class positions from configuration (total: {len(equipment_class_positions)}):")
    for class_name, position in sorted(equipment_class_positions.items(), key=lambda x: (x[1] if x[1] is not None else 999999)):
        pop_logger.info(f"  • Class '{class_name}': Position {position}")

    # Group equipment instances by line
    pop_logger.debug("Grouping equipment instances by production line...")
    line_equipment_map: Dict[Thing, List[Thing]] = {}  # {line_individual: [equipment_instances]}
    
    # Track lines with equipment but no sequence
    lines_without_sequence: List[str] = []

    # Count totals for diagnostic tracking
    total_equipment_processed = 0
    total_equipment_with_class = 0
    total_equipment_with_line = 0
    
    # Step 1: Group all Equipment instances by ProductionLine
    for equipment_inst in onto.search(type=cls_Equipment):
        total_equipment_processed += 1
        
        # Get the line(s) this equipment belongs to
        equipment_lines = getattr(equipment_inst, prop_isPartOfProductionLine.python_name, [])
        if not equipment_lines:
            pop_logger.debug(f"Equipment {equipment_inst.name} is not linked to any ProductionLine. Skipping.")
            continue
        
        total_equipment_with_line += 1
        
        # Get the EquipmentClass this equipment belongs to
        equipment_class_ind = getattr(equipment_inst, prop_memberOfClass.python_name, None)
        
        # More detailed logging when missing EquipmentClass link
        if not equipment_class_ind:
            eq_id = getattr(equipment_inst, prop_equipmentId.python_name, equipment_inst.name)
            pop_logger.warning(f"Equipment {eq_id} has no memberOfClass relationship. Skipping for sequence setup.")
            continue
        
        if not isinstance(equipment_class_ind, cls_EquipmentClass):
            eq_id = getattr(equipment_inst, prop_equipmentId.python_name, equipment_inst.name)
            pop_logger.warning(f"Equipment {eq_id} linked to non-EquipmentClass '{equipment_class_ind}'. Skipping for sequence setup.")
            continue
        
        total_equipment_with_class += 1
        
        # Add equipment to each of its production lines
        for line in equipment_lines:
            if not isinstance(line, cls_ProductionLine):
                pop_logger.warning(f"Equipment {equipment_inst.name} linked to non-ProductionLine '{line}'. Skipping this link.")
                continue
            
            if line not in line_equipment_map:
                line_equipment_map[line] = []
            
            line_equipment_map[line].append(equipment_inst)
    
    # Log summary of equipment distribution for diagnosis
    pop_logger.info(f"Equipment distribution summary:")
    pop_logger.info(f"  • Total equipment found: {total_equipment_processed}")
    pop_logger.info(f"  • Equipment linked to lines: {total_equipment_with_line}")
    pop_logger.info(f"  • Equipment linked to equipment classes: {total_equipment_with_class}")
    pop_logger.info(f"  • Production lines with equipment: {len(line_equipment_map)}")
    
    # Process each line to establish equipment instance relationships
    total_relationships = 0
    line_relationship_counts: Dict[str, int] = {}
    
    def safe_get_equipment_id(equipment: Thing) -> str:
        """Helper to safely get equipmentId or fallback to name for sorting."""
        equipment_id = getattr(equipment, prop_equipmentId.python_name, None)
        if equipment_id:
            return str(equipment_id)
        return equipment.name
    
    with onto:
        for line_ind, equipment_instances in line_equipment_map.items():
            line_id = getattr(line_ind, "lineId", line_ind.name)
            pop_logger.info(f"Processing equipment instance relationships for line: {line_id}")
            
            if not equipment_instances:
                pop_logger.debug(f"No equipment instances found for line: {line_id}")
                continue
            
            # Step 2: Get sequence configuration for this line
            # Check for line-specific sequence configuration first
            line_specific_sequence = LINE_SPECIFIC_EQUIPMENT_SEQUENCE.get(line_id)
            
            if line_specific_sequence:
                pop_logger.info(f"Using line-specific sequence configuration for line {line_id}")
                sequence_config = line_specific_sequence
                # Log the line-specific sequence
                pos_str = ", ".join([f"{cls}:{pos}" for cls, pos in sorted(sequence_config.items(), key=lambda x: x[1])])
                pop_logger.info(f"Line-specific sequence for {line_id}: {pos_str}")
            else:
                # Use the default global sequence
                pop_logger.info(f"Using default sequence configuration for line {line_id}")
                sequence_config = DEFAULT_EQUIPMENT_SEQUENCE
            
            # Step 3: Assign sequencePosition to each Equipment instance based on its class position
            equipment_with_positions = []
            equipment_without_positions = []
            
            for equipment_inst in equipment_instances:
                eq_id = safe_get_equipment_id(equipment_inst)
                
                # Get the EquipmentClass this equipment belongs to
                equipment_class_ind = getattr(equipment_inst, prop_memberOfClass.python_name, None)
                if not equipment_class_ind:
                    pop_logger.warning(f"Equipment {eq_id} has no memberOfClass. Skipping sequence assignment.")
                    equipment_without_positions.append((equipment_inst, "no_class_link"))
                    continue
                
                # Get the class name string from the EquipmentClass individual
                class_name = getattr(equipment_class_ind, prop_equipmentClassId.python_name, None)
                if not class_name:
                    # Try to get the name directly from the individual
                    class_name = equipment_class_ind.name
                    if class_name.startswith("EquipmentClass_"):
                        class_name = class_name[len("EquipmentClass_"):]
                    pop_logger.warning(f"EquipmentClass {equipment_class_ind.name} has no equipmentClassId. Using name {class_name} as fallback.")
                
                # If still no class name, skip this equipment
                if not class_name:
                    pop_logger.warning(f"Cannot determine class name for {eq_id}. Skipping sequence assignment.")
                    equipment_without_positions.append((equipment_inst, "no_class_name"))
                    continue
                
                # Find position from sequence configuration
                position = sequence_config.get(class_name)
                
                if position is None:
                    # Try fallback to global sequence if using line-specific but not found
                    if line_specific_sequence and class_name in DEFAULT_EQUIPMENT_SEQUENCE:
                        position = DEFAULT_EQUIPMENT_SEQUENCE.get(class_name)
                        pop_logger.info(f"Using fallback global position {position} for class {class_name} on line {line_id}")
                
                if position is not None:
                    # Set the sequencePosition on the Equipment instance
                    try:
                        context.set_prop(equipment_inst, "sequencePosition", position)
                        pop_logger.debug(f"Set sequencePosition={position} on Equipment {eq_id} (class: {class_name})")
                        equipment_with_positions.append((equipment_inst, position, eq_id))
                    except Exception as e:
                        pop_logger.error(f"Error setting sequencePosition={position} on Equipment {eq_id}: {e}")
                        equipment_without_positions.append((equipment_inst, "set_position_error"))
                else:
                    pop_logger.warning(f"No sequence position found for class {class_name} on line {line_id}")
                    equipment_without_positions.append((equipment_inst, f"no_position_for_class_{class_name}"))
            
            # Step 4: Sort equipment instances by position, then by equipmentId
            sorted_equipment = sorted(equipment_with_positions, key=lambda x: (x[1], x[2]))
            
            if not sorted_equipment:
                pop_logger.warning(f"No equipment with sequence positions found on line {line_id}. Skipping relationship setup.")
                lines_without_sequence.append(line_id)
                continue
            
            # Log the sorted equipment for verification
            pop_logger.info(f"Sorted equipment on line {line_id} (format: id [position]):")
            for i, (eq, pos, eq_id) in enumerate(sorted_equipment):
                pop_logger.info(f"  {i+1}. {eq_id} [{pos}]")
            
            # Step 5: Link equipment instances with isImmediatelyUpstreamOf/isImmediatelyDownstreamOf
            relationships_created = 0
            for i in range(len(sorted_equipment) - 1):
                upstream_eq, _, up_id = sorted_equipment[i]
                downstream_eq, _, down_id = sorted_equipment[i + 1]
                
                # Validate to ensure we're not creating self-references
                if upstream_eq is downstream_eq:
                    pop_logger.error(f"Detected self-reference attempt for equipment {up_id} on line {line_id}. Skipping this link.")
                    continue
                
                try:
                    # Check if the relationship already exists to avoid duplicates
                    downstream_list = getattr(upstream_eq, prop_isImmediatelyUpstreamOf.python_name, [])
                    if not isinstance(downstream_list, list):
                        downstream_list = [downstream_list] if downstream_list else []
                    
                    if downstream_eq in downstream_list:
                        pop_logger.debug(f"Relationship already exists: {up_id} isImmediatelyUpstreamOf {down_id}")
                    else:
                        # Create forward relationship
                        _set_property_value(upstream_eq, prop_isImmediatelyUpstreamOf, downstream_eq, is_functional=False)
                        pop_logger.debug(f"Created forward relationship: {up_id} isImmediatelyUpstreamOf {down_id}")
                    
                    # Create inverse relationship if property exists
                    if prop_isImmediatelyDownstreamOf:
                        upstream_list = getattr(downstream_eq, prop_isImmediatelyDownstreamOf.python_name, [])
                        if not isinstance(upstream_list, list):
                            upstream_list = [upstream_list] if upstream_list else []
                        
                        if upstream_eq in upstream_list:
                            pop_logger.debug(f"Inverse relationship already exists: {down_id} isImmediatelyDownstreamOf {up_id}")
                        else:
                            _set_property_value(downstream_eq, prop_isImmediatelyDownstreamOf, upstream_eq, is_functional=False)
                            pop_logger.debug(f"Created inverse relationship: {down_id} isImmediatelyDownstreamOf {up_id}")
                    
                    relationships_created += 1
                except Exception as e:
                    pop_logger.error(f"Error creating relationship between {up_id} and {down_id}: {e}")
            
            # Record relationships for this line
            if relationships_created > 0:
                line_relationship_counts[line_id] = relationships_created
                total_relationships += relationships_created
                pop_logger.info(f"Established {relationships_created} instance relationships for line {line_id}.")
            
            # Log info about equipment without positions
            if equipment_without_positions:
                # Group by reason for diagnostic purposes
                reason_groups = {}
                for eq, reason in equipment_without_positions:
                    if reason not in reason_groups:
                        reason_groups[reason] = []
                    reason_groups[reason].append(safe_get_equipment_id(eq))
                
                # Log the grouped reasons
                pop_logger.warning(f"Line {line_id} has {len(equipment_without_positions)} equipment without positions:")
                for reason, ids in reason_groups.items():
                    pop_logger.warning(f"  • Reason '{reason}': {len(ids)} equipment - {', '.join(ids[:5])}" + 
                                       (f" and {len(ids)-5} more" if len(ids) > 5 else ""))

    # Print summary report
    print("\n=== EQUIPMENT INSTANCE RELATIONSHIP REPORT ===")
    if total_relationships > 0:
        pop_logger.info(f"Established {total_relationships} equipment instance relationships across {len(line_relationship_counts)} production lines.")
        print(f"Established {total_relationships} equipment instance relationships on {len(line_relationship_counts)} lines:")
        for line_id, count in sorted(line_relationship_counts.items()):
            print(f"  Line {line_id}: {count} relationships")

        # Print info about the sequencing approach
        print("\nInstance sequencing approach:")
        print("  • Each Equipment instance is assigned a sequencePosition based on its EquipmentClass")
        print("  • Equipment instances are sorted by sequencePosition, then by equipmentId")
        print("  • Sorted instances are linked via isImmediatelyUpstreamOf/isImmediatelyDownstreamOf")
        print("  • Sequence configuration is determined by:")
        print("    - Line-specific configuration (if available in LINE_SPECIFIC_EQUIPMENT_SEQUENCE)")
        print("    - Global DEFAULT_EQUIPMENT_SEQUENCE configuration (fallback)")
    else:
        pop_logger.warning("No equipment instance relationships were established.")
        print("No equipment instance relationships could be established.")
        print("Possible reasons:")
        print("  • Equipment not linked to lines/classes")
        print("  • Missing sequence positions for equipment classes")
        print("  • No equipment found on the same line")
        
    # Log lines without sequence
    if lines_without_sequence:
        print("\nProduction lines without sequence relationships:")
        for line_id in sorted(lines_without_sequence):
            print(f"  • {line_id}")

    return total_relationships  # Return count of created relationships for tracking


===========================================
FILE: ontology_generator/utils/__init__.py
===========================================

from .types import safe_cast
from .logging import main_logger, configure_logging, analysis_logger # Also expose logging functions here if desired, or keep them module-specific


===========================================
FILE: ontology_generator/utils/logging.py
===========================================

"""
Logging utilities for the ontology generator.

This module provides functions for setting up and configuring logging.
"""
import logging
import sys
from typing import Optional, List

from ontology_generator.config import LOG_FORMAT, SUPPRESSED_WARNINGS, MessageFilter, setup_logging_filters

# --- Logger Instances ---
# Main module loggers that will be configured at initialization
logger = logging.getLogger("ontology_definition")  # Logger for definition module
pop_logger = logging.getLogger("ontology_population")  # Logger for population module
link_logger = logging.getLogger("event_linking")  # Logger for event linking pass
main_logger = logging.getLogger("create_ontology")  # Logger for main script
analysis_logger = logging.getLogger("ontology_analysis")  # Logger for analysis module

class WarningSuppressionFilter(logging.Filter):
    """
    A logging filter that suppresses specific warning messages based on configured substrings.
    """
    def __init__(self, suppressed_warnings=None):
        super().__init__()
        self.suppressed_warnings = suppressed_warnings or []
        self.suppressed_count = 0
        
    def filter(self, record):
        if record.levelno == logging.WARNING:
            # Get the formatted message
            message = record.getMessage()
            # Check if any suppressed warning substring is in the message
            for suppressed in self.suppressed_warnings:
                if suppressed in message:
                    self.suppressed_count += 1
                    return False  # Suppress this warning
        return True  # Let other messages through

class InfoSuppressionFilter(logging.Filter):
    """
    A logging filter that suppresses specific INFO messages based on configured substrings.
    """
    def __init__(self, suppressed_info=None):
        super().__init__()
        self.suppressed_info = suppressed_info or []
        self.suppressed_count = 0
        
    def filter(self, record):
        if record.levelno == logging.INFO:
            # Get the formatted message
            message = record.getMessage()
            # Check if any suppressed info substring is in the message
            for suppressed in self.suppressed_info:
                if suppressed in message:
                    self.suppressed_count += 1
                    return False  # Suppress this info message
        return True  # Let other messages through

def configure_logging(
    log_level: int = logging.INFO,
    log_file: Optional[str] = None,
    handlers: Optional[List[logging.Handler]] = None
) -> None:
    """
    Configure the root logger with the specified settings.
    
    Args:
        log_level: The logging level to use (default: INFO)
        log_file: Optional path to a log file
        handlers: Optional list of handlers to add to the root logger
    """
    # Reset root logger configuration
    root_logger = logging.getLogger()
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # Set up basic configuration
    logging.basicConfig(level=log_level, format=LOG_FORMAT, stream=sys.stdout)
    root_logger.setLevel(log_level)
    
    # Set all handler levels
    for handler in root_logger.handlers:
        handler.setLevel(log_level)
    
    # Add file handler if specified
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(logging.Formatter(LOG_FORMAT))
        file_handler.setLevel(log_level)
        root_logger.addHandler(file_handler)
    
    # Add additional handlers if provided
    if handlers:
        for handler in handlers:
            root_logger.addHandler(handler)
    
    # Set up warning suppression filter for specific loggers
    warning_filter = WarningSuppressionFilter(SUPPRESSED_WARNINGS)
    for logger_name in [
        "ontology_generator.population.row_processor",
        "ontology_population",
        "ontology_generator.population.equipment",
        "ontology_generator.population.events",
        "ontology_generator.population.core"
    ]:
        logging.getLogger(logger_name).addFilter(warning_filter)
    
    # Set up info suppression filter for individual creation messages
    info_filter = InfoSuppressionFilter(["Created new individual"])
    pop_logger.addFilter(info_filter)
    
    # Store filters for later access
    global _warning_filter, _info_filter
    _warning_filter = warning_filter
    _info_filter = info_filter
    
    # Set up message filter for SUPPRESSED_WARNINGS at all log levels
    setup_logging_filters()
    
    # Log confirmation
    main_logger.info("Logging configured.")
    if log_level == logging.DEBUG:
        main_logger.info("Verbose logging enabled (DEBUG level).")
    elif log_level == logging.WARNING:
        main_logger.info("Quiet logging enabled (WARNING level).")
    else:
        main_logger.info("Standard logging enabled (INFO level).")
    main_logger.info(f"Warning suppression filter applied for {len(SUPPRESSED_WARNINGS)} message patterns.")

# Global variables to store filter instances
_warning_filter = None
_info_filter = None

def get_suppressed_message_counts():
    """
    Get counts of suppressed messages.
    
    Returns:
        A tuple of (warning_count, info_count)
    """
    warning_count = _warning_filter.suppressed_count if _warning_filter else 0
    info_count = _info_filter.suppressed_count if _info_filter else 0
    return warning_count, info_count

def log_suppressed_message_counts():
    """
    Log the counts of suppressed messages.
    """
    warning_count, info_count = get_suppressed_message_counts()
    total_count = warning_count + info_count
    
    if total_count > 0:
        main_logger.info(f"Message suppression summary: {total_count} messages suppressed "
                         f"({warning_count} warnings, {info_count} info messages)")
    else:
        main_logger.info("No messages have been suppressed by filters")

def get_module_logger(name: str) -> logging.Logger:
    """
    Get a logger for a specific module with the specified name.
    
    Args:
        name: The name of the logger
        
    Returns:
        A configured logger instance
    """
    return logging.getLogger(name)


===========================================
FILE: ontology_generator/utils/types.py
===========================================

"""
Type conversion utilities for the ontology generator.

This module provides functions for safe type conversion and handling.
"""
import re
from datetime import datetime, date, time
from decimal import Decimal, InvalidOperation
from typing import Any, Optional, Type, List, Dict, TypeVar, Union

from dateutil import parser as dateutil_parser
from dateutil.parser import ParserError

from ontology_generator.utils.logging import pop_logger

T = TypeVar('T')

def safe_cast(value: Any, target_type: Type[T], default: Optional[T] = None) -> Optional[T]:
    """
    Safely casts a value to a target type, returning default on failure.
    
    Args:
        value: The value to cast
        target_type: The target type to cast to
        default: The default value to return on failure
        
    Returns:
        The cast value, or the default if casting fails
    """
    if value is None or value == '':
        return default
    try:
        original_value_repr = repr(value)  # For logging
        value_str = str(value).strip()

        if target_type is str:
            return value_str
        if target_type is int:
            # Handle potential floats in data like '224.0' -> 224
            # Also handle direct integers or strings representing integers
            try:
                # TKT-006: Handle empty strings, whitespace, etc.
                if not value_str or value_str.isspace():
                    return default
                
                # Special case for numeric metrics: treat "" or "0" as 0
                if value_str == "" or value_str == "0":
                    return 0
                    
                return int(float(value_str))
            except ValueError:
                # Maybe it was already an int disguised as string?
                return int(value_str)
        if target_type is float:
            # TKT-006: Handle empty strings, whitespace, etc.
            if not value_str or value_str.isspace():
                return default
                
            # Special case for numeric metrics: treat "" or "0" as 0.0
            if value_str == "" or value_str == "0":
                return 0.0
                
            # Handles standard float conversion
            return float(value_str)
        # Note: xsd:decimal maps to float based on XSD_TYPE_MAP
        if target_type is bool:
            val_lower = value_str.lower()
            if val_lower in ['true', '1', 't', 'y', 'yes']:
                return True
            elif val_lower in ['false', '0', 'f', 'n', 'no']:
                return False
            else:
                pop_logger.warning(f"Could not interpret {original_value_repr} as boolean.")
                return None  # Explicitly return None for uninterpretable bools
        if target_type is datetime:
            # --- Use dateutil.parser for robust parsing ---
            try:
                # Check for common problematic patterns first
                if value_str.lower() in ['', 'null', 'none', 'na', 'n/a', '?']:
                    pop_logger.warning(f"Empty or null datetime value: {original_value_repr}")
                    return default
                
                # Check for common malformed date patterns
                if re.match(r'^\d{1,2}/\d{1,2}$', value_str):  # Just MM/DD with no year
                    pop_logger.warning(f"Incomplete date without year: {original_value_repr}")
                    return default
                
                # Additional cleanup for common issues that dateutil might misinterpret
                cleaned_value = value_str
                # Remove any double spaces that might confuse the parser
                cleaned_value = re.sub(r'\s+', ' ', cleaned_value).strip()
                
                # No need for extensive pre-cleaning or format list with dateutil
                # It handles various formats, including spaces and timezones
                parsed_dt = dateutil_parser.parse(cleaned_value)

                # Capture parsing details for diagnostic logging
                has_timezone = parsed_dt.tzinfo is not None
                timezone_name = str(parsed_dt.tzinfo) if has_timezone else "None"

                # dateutil returns an AWARE datetime if offset is present.
                # owlready2 stores naive datetimes.
                # Maintain existing behavior: make it naive (loses original offset info).
                if has_timezone:
                    pop_logger.debug(f"Parsed datetime {original_value_repr} with timezone {timezone_name}, storing as naive datetime.")
                    parsed_dt = parsed_dt.replace(tzinfo=None)
                else:
                    pop_logger.debug(f"Parsed datetime {original_value_repr} without timezone, storing as naive datetime.")

                pop_logger.debug(f"Successfully parsed datetime '{original_value_repr}' → {parsed_dt}")
                return parsed_dt

            except (ParserError, ValueError, TypeError) as e:  # Catch errors from dateutil and potential downstream issues
                # Provide more detailed diagnostic information about the failed parse
                pop_logger.warning(f"Could not parse datetime '{original_value_repr}': {e}")
                
                # Try some common patterns explicitly as a fallback
                try:
                    # Try to identify the pattern for better error reporting
                    if '/' in value_str:
                        pattern = "MM/DD/YYYY or DD/MM/YYYY"
                    elif '-' in value_str:
                        pattern = "YYYY-MM-DD"
                    elif '.' in value_str:
                        pattern = "DD.MM.YYYY"
                    else:
                        pattern = "unknown"
                    
                    pop_logger.warning(f"Original datetime string appears to use {pattern} format. Check data source for consistency.")
                except:
                    pass
                    
                return default
            except Exception as e:  # Catch any other unexpected errors
                pop_logger.error(f"Unexpected error parsing datetime '{original_value_repr}': {e}", exc_info=False)
                return default
            # --- End of dateutil parsing block ---

        if target_type is date:
            try:
                # Try ISO first
                return date.fromisoformat(value_str)  # Assumes YYYY-MM-DD
            except ValueError:
                # Try other common formats if needed
                try:
                    dt_obj = datetime.strptime(value_str, "%m/%d/%Y")  # Example alternative
                    return dt_obj.date()
                except ValueError:
                    pop_logger.warning(f"Could not parse date string {original_value_repr} as ISO or m/d/Y date.")
                    return default
        if target_type is time:
            try:
                # Try ISO first
                return time.fromisoformat(value_str)  # Assumes HH:MM:SS[.ffffff][+/-HH:MM]
            except ValueError:
                # Try other common formats if needed
                try:
                    dt_obj = datetime.strptime(value_str, "%H:%M:%S")  # Just H:M:S
                    return dt_obj.time()
                except ValueError:
                    pop_logger.warning(f"Could not parse time string {original_value_repr} as ISO or H:M:S time.")
                    return default

        # Final fallback cast attempt
        return target_type(value_str)

    except (ValueError, TypeError, InvalidOperation) as e:
        target_type_name = target_type.__name__ if target_type else "None"
        original_value_repr = repr(value)[:50] + ('...' if len(repr(value)) > 50 else '') # Added for clarity
        pop_logger.warning(f"Failed to cast {original_value_repr} to {target_type_name}: {e}. Returning default: {default}")
        return default
    except Exception as e:
        target_type_name = target_type.__name__ if target_type else "None"
        original_value_repr = repr(value)[:50] + ('...' if len(repr(value)) > 50 else '') # Added for clarity
        pop_logger.error(f"Unexpected error casting {original_value_repr} to {target_type_name}: {e}", exc_info=False)
        return default

def sanitize_name(name: Any) -> str:
    """
    Sanitizes a name to be valid for use in identifiers.
    
    Args:
        name: The name to sanitize
        
    Returns:
        A sanitized version of the name
    """
    if name is None or str(name).strip() == '':
        return "unnamed"
        
    # Convert to string and strip whitespace
    name_str = str(name).strip()
    
    # Replace spaces and common problematic chars with underscore
    safe_name = re.sub(r'\s+|[<>:"/\\|?*#%\']', '_', name_str)
    
    # Remove any remaining non-alphanumeric, non-hyphen, non-underscore chars (allows periods)
    safe_name = re.sub(r'[^\w\-._]', '', safe_name)
    
    # Ensure it doesn't start with a number or hyphen (common restriction)
    if safe_name and (safe_name[0].isdigit() or safe_name[0] == '-'):
        safe_name = "_" + safe_name
        
    # Check if empty after sanitization
    if not safe_name:
        fallback_hash = abs(hash(name_str))  # Hash the original string
        safe_name = f"UnnamedData_{fallback_hash}"
        
    return safe_name


